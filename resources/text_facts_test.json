{
    "P361": [],
    "P1889": [],
    "P279": [],
    "P1552": [],
    "P301": [],
    "P366": [],
    "P31": [],
    "P138": [],
    "P373": [],
    "P1382": [],
    "P1001": [],
    "P1269": [],
    "P2670": [],
    "P527": [],
    "P156": [],
    "P2184": [],
    "P6104": [],
    "P2317": [],
    "P2541": [],
    "P2808": [],
    "P3301": [],
    "P106": [],
    "P101": [],
    "P1056": [],
    "P1629": [],
    "P1659": [],
    "P1855": [],
    "P2302": [],
    "P3734": [],
    "P1647": [],
    "P1479": [],
    "P1478": [],
    "P2283": [],
    "P180": [],
    "P1537": [],
    "P641": [],
    "P2579": [],
    "P910": [],
    "P460": [],
    "P1535": [],
    "P461": [],
    "P5137": [],
    "P155": [],
    "P61": [],
    "P1448": [],
    "P277": [],
    "P348": [],
    "P800": [],
    "P2548": [],
    "P645": [],
    "P644": [],
    "P688": [],
    "P703": [],
    "P702": [],
    "P681": [],
    "P680": [],
    "P682": [],
    "P684": [],
    "P828": [],
    "P2578": [],
    "P1568": [],
    "P1851": [],
    "P920": [],
    "P1687": [],
    "P4934": [],
    "P144": [],
    "P2159": [],
    "P793": [],
    "P2271": [],
    "P1963": [],
    "P6477": [],
    "P2591": [],
    "P425": [],
    "P487": [],
    "P913": [],
    "P2354": [],
    "P360": [],
    "P1762": [],
    "P17": [],
    "P131": [],
    "P186": [],
    "P1303": [],
    "P870": [],
    "P8864": [],
    "P1542": [],
    "P547": [],
    "P518": [],
    "P1142": [],
    "P4878": [],
    "P1813": [],
    "P1987": [],
    "P2179": [],
    "P935": [],
    "P2650": [],
    "P2263": [],
    "P452": [],
    "P4969": [],
    "P3712": [],
    "P1268": [],
    "P416": [],
    "P1748": [],
    "P111": [],
    "P1557": [],
    "P1843": [],
    "P1582": [],
    "P171": [],
    "P3095": [],
    "P2384": [],
    "P1114": [],
    "P1423": [],
    "P1329": [],
    "P159": [],
    "P2043": [],
    "P5008": [],
    "P4428": [],
    "probability distribution*****multiple": [
        "$ARG1 over $ARG2"
    ],
    "function*****support": [
        "$ARG1 , but $ARG2"
    ],
    "support*****distribution": [
        "$ARG1 operations that implicitly require knowledge of it , such as drawing samples from the $ARG2"
    ],
    "structured probabilistic model*****graphical model": [
        "$ARG1 described in terms of graphs and factors , using the language of $ARG2",
        "$ARG1 or $ARG2"
    ],
    "boltzmann machine*****learning": [
        "$ARG1 were originally introduced as a general \u201c connectionist \u201d approach to $ARG2",
        "$ARG1 when trained with $ARG2"
    ],
    "learning*****probability distribution": [
        "$ARG1 arbitrary $ARG2",
        "$ARG1 a full $ARG2"
    ],
    "boltzmann machine*****other": [
        "$ARG1 that include $ARG2",
        "$ARG1 ( with every unit connected to every $ARG2"
    ],
    "boltzmann machine*****inference": [
        "$ARG1 and discuss the issues that come up when trying to train and perform $ARG2"
    ],
    "inference*****model": [
        "$ARG1 in the $ARG2",
        "$ARG1 in the $ARG2",
        "$ARG1 in the $ARG2",
        "$ARG1 network or recognition $ARG2",
        "$ARG1 in a di\ufb00erent $ARG2",
        "$ARG1 in the $ARG2",
        "$ARG1 network are adapted to increase L. This $ARG2",
        "$ARG1 in the trained $ARG2"
    ],
    "boltzmann machine*****vector": [
        "$ARG1 over a d-dimensional binary random $ARG2",
        "$ARG1 introduces a $ARG2"
    ],
    "vector*****1": [
        "$ARG1 x \u2208 { 0 , $ARG2",
        "$ARG1 may be raised to di\ufb00erent powers , so that a moment may be any quantity of the form Ex \u03a0i xni i where n = [ n $ARG2",
        "$ARG1 c , to obtain \uf8ef $ARG2",
        "$ARG1 sums to $ARG2",
        "$ARG1 , to guarantee that these outputs are positive and sum to $ARG2",
        "$ARG1 with ei = $ARG2",
        "$ARG1 or sequence of vectors that summarize the input sequence X = ( x ( $ARG2",
        "$ARG1 ( just like in \ufb01gure 10.9 ) to generate the output sequence Y = ( y ( $ARG2",
        "$ARG1 h\u0302 of probabilities , with q ( hi = $ARG2",
        "$ARG1 p \u2208 [ 0 , $ARG2"
    ],
    "boltzmann machine*****energy-based model": [
        "$ARG1 is an $ARG2"
    ],
    "joint probability distribution*****energy function": [
        "$ARG1 using an $ARG2"
    ],
    "energy function*****partition function": [
        "$ARG1 and Z is the $ARG2",
        "$ARG1 that prevents the $ARG2"
    ],
    "partition function*****1": [
        "$ARG1 that ensures that x P ( x ) = $ARG2",
        "$ARG1 We can now write the ratio \u00b7 \u00b7 \u00b7 n\u22121 = $ARG2",
        "$ARG1 is given by : $ARG2",
        "$ARG1 \ue059 p\u0303 \u03b7 ( x \u03b7 ) p\u0303\u03b7 n\u22121 ( x \u03b7n\u22121 ) = p\u0303 $ARG2",
        "$ARG1 Bridge sampling estimates the ratio Z $ARG2"
    ],
    "energy function*****boltzmann machine": [
        "$ARG1 of the $ARG2",
        "$ARG1 that maintains tractability of all of the di\ufb00erent conditional distributions needed to use the $ARG2"
    ],
    "boltzmann machine*****matrix": [
        "$ARG1 is given by E ( x ) = \u2212x\ue03eU x \u2212 b\ue03ex , where U is the \u201c weight \u201d $ARG2"
    ],
    "matrix*****model": [
        "$ARG1 of $ARG2",
        "$ARG1 R that was absent from the $ARG2",
        "$ARG1 , the connectivity is relatively dense , the $ARG2",
        "$ARG1 in a fully connected $ARG2"
    ],
    "model*****vector": [
        "$ARG1 parameters and b is the $ARG2",
        "$ARG1 takes inputs x and parameters \u03b8 , both encapsulated in the $ARG2",
        "$ARG1 family , in which we assume that an observed data $ARG2",
        "$ARG1 , we used a $ARG2",
        "$ARG1 \u2019 s output $ARG2",
        "$ARG1 assigns an input described by $ARG2",
        "$ARG1 parameter $ARG2"
    ],
    "vector*****bias": [
        "$ARG1 of $ARG2"
    ],
    "bias*****boltzmann machine": [
        "$ARG1 In the general setting of the $ARG2"
    ],
    "boltzmann machine*****set": [
        "$ARG1 , we are given a $ARG2",
        "$ARG1 as consisting of a $ARG2"
    ],
    "probability*****linear model": [
        "$ARG1 of one unit being on is given by a $ARG2"
    ],
    "linear model*****logistic regression": [
        "$ARG1 ( $ARG2",
        "$ARG1 , such as $ARG2",
        "$ARG1 , like $ARG2"
    ],
    "logistic regression*****other": [
        "$ARG1 ) from the values of the $ARG2"
    ],
    "perceptron*****model": [
        "$ARG1 and $ARG2",
        "$ARG1 ( Rosenblatt , 1958 , 1962 ) became the \ufb01rst $ARG2"
    ],
    "logistic regression*****universal approximator": [
        "$ARG1 into an MLP results in the MLP being a $ARG2"
    ],
    "universal approximator*****boltzmann machine": [
        "$ARG1 of functions , a $ARG2"
    ],
    "boltzmann machine*****universal approximator": [
        "$ARG1 becomes a $ARG2"
    ],
    "universal approximator*****probability mass function": [
        "$ARG1 of $ARG2"
    ],
    "learning*****boltzmann machine": [
        "$ARG1 algorithms for $ARG2"
    ],
    "boltzmann machine*****maximum likelihood": [
        "$ARG1 are usually based on $ARG2",
        "$ARG1 , in that it converges to di\ufb00erent points than the $ARG2"
    ],
    "boltzmann machine*****partition function": [
        "$ARG1 have an intractable $ARG2"
    ],
    "partition function*****maximum likelihood": [
        "$ARG1 , so the $ARG2"
    ],
    "property*****boltzmann machine": [
        "$ARG1 of $ARG2"
    ],
    "learning*****maximum likelihood": [
        "$ARG1 rules based on $ARG2",
        "$ARG1 Conditional Distributions with $ARG2",
        "$ARG1 these means with $ARG2",
        "$ARG1 the means of the mixture , $ARG2"
    ],
    "maximum likelihood*****statistics": [
        "$ARG1 is that the update for a particular weight connecting two units depends only the $ARG2",
        "$ARG1 as ideas spread between the $ARG2"
    ],
    "rule*****boltzmann machine": [
        "$ARG1 is \u201c local , \u201d which makes $ARG2"
    ],
    "random variable*****boltzmann machine": [
        "$ARG1 in a $ARG2"
    ],
    "boltzmann machine*****random variable": [
        "$ARG1 , then the axons and dendrites connecting two $ARG2"
    ],
    "positive phase*****connection": [
        "$ARG1 , two units that frequently activate together have their $ARG2"
    ],
    "example*****learning": [
        "$ARG1 of a Hebbian $ARG2",
        "$ARG1 of the importance of $ARG2",
        "$ARG1 of the broader class of non-distributed representations , which are representations that may contain many entries but without signi\ufb01cant meaningful separate control over Examples of $ARG2",
        "$ARG1 of a $ARG2",
        "$ARG1 : $ARG2",
        "$ARG1 , the motivation for $ARG2",
        "$ARG1 , by $ARG2",
        "$ARG1 of an encoder-decoder or sequence-to-sequence RNN architecture , for $ARG2",
        "$ARG1 , we see that we can mostly resolve the di\ufb03culties in $ARG2",
        "$ARG1 , the ease of $ARG2",
        "$ARG1 , in semisupervised $ARG2",
        "$ARG1 of multi-instance $ARG2",
        "$ARG1 , if we want to use a $ARG2",
        "$ARG1 of how a $ARG2",
        "$ARG1 of how overcomplete , high-capacity models may be used as autoencoders so long as care is taken to prevent them from $ARG2"
    ],
    "rule*****learning": [
        "$ARG1 ( Hebb , 1949 ) often summarized with the mnemonic \u201c \ufb01re together , wire together. \u201d Hebbian $ARG2"
    ],
    "learning*****information": [
        "$ARG1 algorithms that use more $ARG2",
        "$ARG1 a useful representation of h , we would like h to encode enough $ARG2",
        "$ARG1 algorithms share statistical strength across di\ufb00erent tasks , including using $ARG2"
    ],
    "information*****statistics": [
        "$ARG1 than local $ARG2"
    ],
    "example*****back-propagation": [
        "$ARG1 , for the brain to implement $ARG2",
        "$ARG1 , we do not explain how the $ARG2",
        "$ARG1 : $ARG2",
        "$ARG1 , we walk through the $ARG2"
    ],
    "back-propagation*****multilayer perceptron": [
        "$ARG1 in a $ARG2"
    ],
    "multilayer perceptron*****gradient": [
        "$ARG1 , it seems necessary for the brain to maintain a secondary communication network for transmitting $ARG2",
        "$ARG1 and a means of computing the $ARG2"
    ],
    "back-propagation*****inference": [
        "$ARG1 of gradients to $ARG2",
        "$ARG1 through the $ARG2"
    ],
    "inference*****energy-based model": [
        "$ARG1 in $ARG2"
    ],
    "energy-based model*****boltzmann machine": [
        "$ARG1 similar to the $ARG2",
        "$ARG1 are called $ARG2",
        "$ARG1 and when to call it a $ARG2"
    ],
    "negative phase*****boltzmann machine": [
        "$ARG1 of $ARG2"
    ],
    "learning*****view": [
        "$ARG1 is somewhat harder to explain from a biological point of $ARG2",
        "$ARG1 point of $ARG2"
    ],
    "dream sleep*****negative phase": [
        "$ARG1 may be a form of $ARG2"
    ],
    "harmonium*****restricted boltzmann machine": [
        "$ARG1 ( Smolensky , 1986 ) , $ARG2"
    ],
    "1*****restricted boltzmann machine": [
        "$ARG1 ) Figure 20.1 : Examples of models that may be built with $ARG2"
    ],
    "restricted boltzmann machine*****undirected graphical model": [
        "$ARG1 itself is an $ARG2"
    ],
    "undirected graphical model*****graph": [
        "$ARG1 based on a bipartite $ARG2"
    ],
    "graph*****other": [
        "$ARG1 and hidden units in the $ARG2",
        "$ARG1 , with odd layers on one side and even layers on the $ARG2",
        "$ARG1 have been ordered in such a way that we can compute their output one after the $ARG2",
        "$ARG1 describing how concepts are related to each $ARG2",
        "$ARG1 encodes only simplifying assumptions about which variables are conditionally independent from each $ARG2",
        "$ARG1 representing how your roommate \u2019 s health h r , your health hy , and your work colleague \u2019 s health hc a\ufb00ect each $ARG2",
        "$ARG1 must be designed to connect those variables that are tightly coupled and omit edges between $ARG2"
    ],
    "deep belief network*****graphical model": [
        "$ARG1 is a hybrid $ARG2"
    ],
    "local conditional probability distribution*****deep belief network": [
        "$ARG1 needed by the $ARG2"
    ],
    "deep belief network*****local conditional probability distribution": [
        "$ARG1 are copied directly from the $ARG2"
    ],
    "deep belief network*****graph": [
        "$ARG1 with a completely undirected $ARG2"
    ],
    "deep boltzmann machine*****undirected graphical model": [
        "$ARG1 is an $ARG2"
    ],
    "restricted boltzmann machine*****other": [
        "$ARG1 , but as we see later there are extensions to $ARG2"
    ],
    "set*****random variable": [
        "$ARG1 of n v binary $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2"
    ],
    "random variable*****vector": [
        "$ARG1 which we refer to collectively with the $ARG2"
    ],
    "vector*****hidden layer": [
        "$ARG1 v. We refer to the latent or $ARG2",
        "$ARG1 and h ( t ) is the current $ARG2"
    ],
    "hidden layer*****random variable": [
        "$ARG1 of nh binary $ARG2"
    ],
    "boltzmann machine*****restricted boltzmann machine": [
        "$ARG1 , the $ARG2"
    ],
    "restricted boltzmann machine*****energy-based model": [
        "$ARG1 is an $ARG2"
    ],
    "energy-based model*****joint probability distribution": [
        "$ARG1 with the $ARG2",
        "$ARG1 , meaning that the the $ARG2"
    ],
    "energy function*****constant": [
        "$ARG1 for an RBM is given by E ( v , h ) = \u2212b\ue03e v \u2212 c\ue03e h \u2212 v\ue03e W h , and Z is the normalizing $ARG2"
    ],
    "constant*****partition function": [
        "$ARG1 known as the $ARG2",
        "$ARG1 Z is known as the $ARG2"
    ],
    "partition function*****method": [
        "$ARG1 Z that the naive $ARG2",
        "$ARG1 is to use a Monte Carlo $ARG2"
    ],
    "method*****algorithm": [
        "$ARG1 of computing Z ( exhaustively summing over all states ) could be computationally intractable , unless a cleverly designed $ARG2",
        "$ARG1 , given in $ARG2",
        "$ARG1 , such as the Levenberg\u2013Marquardt $ARG2",
        "$ARG1 of conjugate gradients , the BFGS $ARG2",
        "$ARG1 as the BFGS $ARG2",
        "$ARG1 is exact , and the $ARG2"
    ],
    "algorithm*****probability distribution": [
        "$ARG1 could exploit regularities in the $ARG2",
        "$ARG1 to at least implicitly capture the structure of the $ARG2",
        "$ARG1 that searches over all possible $ARG2"
    ],
    "restricted boltzmann machine*****partition function": [
        "$ARG1 , Long and Servedio ( 2010 ) formally proved that the $ARG2",
        "$ARG1 as components , the hardness results for computing the $ARG2"
    ],
    "partition function*****joint probability distribution": [
        "$ARG1 Z implies that the normalized $ARG2"
    ],
    "graph*****property": [
        "$ARG1 structure of the RBM has the very special $ARG2"
    ],
    "constant*****distribution": [
        "$ARG1 with respect to the $ARG2",
        "$ARG1 ) while the marginal $ARG2"
    ],
    "joint probability*****vector": [
        "$ARG1 over the $ARG2"
    ],
    "1*****\u03c3": [
        "$ARG1 | v ) exp cj + v\ue03e W : ,j exp { 0 } + exp { cj + v\ue03e W : ,j } = $ARG2",
        "$ARG1 | h ) = $ARG2",
        "$ARG1 ) = $ARG2",
        "$ARG1 ) ) = $ARG2",
        "$ARG1 | v , h ( 2 ) ) = $ARG2",
        "$ARG1 ) \u2190 $ARG2",
        "$ARG1 ) = $ARG2",
        "$ARG1 ) = $ARG2",
        "$ARG1 ) = $ARG2",
        "$ARG1 + exp log ppmodel pnoise ( x ) = $ARG2",
        "$ARG1 ) = $ARG2",
        "$ARG1 | x ) = $ARG2",
        "$ARG1 ) , but with its own parameters : g ( t ) g ( t\u22121 ) \uf8f8 gi = $ARG2",
        "$ARG1 \u2212 u ( i t\u22121 ) ) $ARG2",
        "$ARG1 | x ; \u03b8 ) = $ARG2",
        "$ARG1 | v ) = $ARG2",
        "$ARG1 \u2212 $ARG2",
        "$ARG1 ) = $ARG2",
        "$ARG1 \u2212 h\u02c6i ) ( log $ARG2",
        "$ARG1 \u2212 h\u0302 i ) ) exp \u2212 ( vi \u2212 Wi , :h ) 2 h\u0302 i ( log $ARG2",
        "$ARG1 \u2212 h\u02c6i ) ( log $ARG2",
        "$ARG1 \u2212 h\u02c6j ) ( log $ARG2",
        "$ARG1 \u2212 h\u0302j ) ) ( 19.39 ) \uf8f0 Wj , k \u2212 \u03b2 j \uf8edv 2j \u2212 2v j Wj , : h\u0302 + W j , k W j , l h\u0302k h\u0302l \uf8fb\uf8f8 \uf8fb \uf8fb = log $ARG2",
        "$ARG1 \u2212 log $ARG2",
        "$ARG1 \u2212 log $ARG2",
        "$ARG1 \u2212 $ARG2",
        "$ARG1 \u2212 $ARG2",
        "$ARG1 ) , $ARG2",
        "$ARG1 ) \u03b6 ( x ) = $ARG2"
    ],
    "1*****hidden layer": [
        "$ARG1 | v ) = We can now express the full conditional over the $ARG2"
    ],
    "hidden layer*****\u03c3": [
        "$ARG1 as the factorial P ( h | v ) = $ARG2"
    ],
    "\u03c3*****1": [
        "$ARG1 ( 2h \u2212 $ARG2",
        "$ARG1 ( ( 2v \u2212 $ARG2",
        "$ARG1 bi + W : ,i ( $ARG2",
        "$ARG1 bi + W : ,i h In the case of real-valued visible units , substitute v \u223c N v ; b ( 0 ) + W ( $ARG2",
        "$ARG1 b ( $ARG2",
        "$ARG1 Wi , : h ( $ARG2",
        "$ARG1 v\ue03eW : ,i + Wi , : h ( 2 ) ( $ARG2",
        "$ARG1 \uf8ed ( 2 ) ( $ARG2",
        "$ARG1 V W ( $ARG2",
        "$ARG1 H\u0302 ( $ARG2",
        "$ARG1 Wj , : H\u0303i , : \u2200i , j , H\u0303i , j sampled from P ( H\u0303 i , j = $ARG2",
        "$ARG1 V\u0303i , : W : ,j + H\u0303i , : W j , : \u2206 W ( $ARG2",
        "$ARG1 ( ( 2y \u2212 $ARG2",
        "$ARG1 ( ( 2y \u2212 $ARG2",
        "$ARG1 ( z ) is equivalent to de\ufb01ning P ( y = $ARG2",
        "$ARG1 v W : ,i + bi , P ( hi = 0 | v ) = $ARG2",
        "$ARG1 ( bi ) \u2212 log h\u0302 i ) + ( $ARG2",
        "$ARG1 ( \u2212b i ) \u2212 log ( $ARG2",
        "$ARG1 ( bi ) \u2212 log h\u0302 i ) + ( $ARG2",
        "$ARG1 ( \u2212b i ) \u2212 log ( $ARG2",
        "$ARG1 ( bj ) \u2212 log h\u02c6j ) + ( $ARG2",
        "$ARG1 ( \u2212bj ) \u2212 log ( $ARG2",
        "$ARG1 ( bi ) \u2212 log h\u0302i \u2212 $ARG2",
        "$ARG1 ( \u2212b i ) \uf8f0\u03b2 j \uf8ed vj Wj , i \u2212 $ARG2",
        "$ARG1 = $ARG2",
        "$ARG1 ( x ) = $ARG2",
        "$ARG1 ( x ) ( $ARG2",
        "$ARG1 ( x ) ) $ARG2",
        "$ARG1 ( x ) \u2200x \u2208 ( 0 , $ARG2",
        "$ARG1 ( x ) = log \u2200x > 0 , \u03b6 \u22121 ( x ) = log ( exp ( x ) \u2212 $ARG2"
    ],
    "derivation*****other": [
        "$ARG1 will show that the $ARG2"
    ],
    "other*****distribution": [
        "$ARG1 condition of interest to us , P ( v | h ) , is also a factorial $ARG2",
        "$ARG1 cases , the DBM may represent the $ARG2",
        "$ARG1 conditional $ARG2",
        "$ARG1 , and the relationship ( in general a joint $ARG2",
        "$ARG1 values , denoted x\u2212i , are given , then we know the $ARG2",
        "$ARG1 samples from the same $ARG2",
        "$ARG1 words , the $ARG2"
    ],
    "distribution*****\u03c3": [
        "$ARG1 : P ( v | h ) = $ARG2",
        "$ARG1 x \u223c N ( x ; b , W W \ue03e + $ARG2",
        "$ARG1 is given by $ARG2",
        "$ARG1 , since we need to invert $ARG2"
    ],
    "restricted boltzmann machine*****gibbs sampling": [
        "$ARG1 Because the RBM admits e\ufb03cient evaluation and di\ufb00erentiation of P\u0303 ( v ) and e\ufb03cient MCMC sampling in the form of block $ARG2"
    ],
    "gibbs sampling*****partition function": [
        "$ARG1 , it can readily be trained with any of the techniques described in chapter 18 for training models that have intractable $ARG2"
    ],
    "undirected model*****deep learning": [
        "$ARG1 used in $ARG2"
    ],
    "other*****deep boltzmann machine": [
        "$ARG1 deep models , such as the $ARG2",
        "$ARG1 approach to jointly training $ARG2"
    ],
    "deep boltzmann machine*****partition function": [
        "$ARG1 , combine both the di\ufb03culty of an intractable $ARG2"
    ],
    "partition function*****inference": [
        "$ARG1 and the di\ufb03culty of intractable $ARG2"
    ],
    "deep belief network*****deep learning": [
        "$ARG1 in 2006 began the current $ARG2"
    ],
    "kernel*****objective function": [
        "$ARG1 machines with convex $ARG2"
    ],
    "deep belief network*****support vector machine": [
        "$ARG1 demonstrated that deep architectures can be successful , by outperforming kernelized $ARG2"
    ],
    "support vector machine*****dataset": [
        "$ARG1 on the MNIST $ARG2"
    ],
    "deep belief network*****other": [
        "$ARG1 have mostly fallen out of favor and are rarely used , even compared to $ARG2",
        "$ARG1 \u201d should refer speci\ufb01cally to models with undirected connections in the deepest layer and directed connections pointing downward between all $ARG2"
    ],
    "other*****learning": [
        "$ARG1 unsupervised or generative $ARG2",
        "$ARG1 cases , our $ARG2",
        "$ARG1 idea , that a $ARG2",
        "$ARG1 models presented later will apply these principles to $ARG2",
        "$ARG1 layer types , so the $ARG2",
        "$ARG1 derivatives , either as part of the $ARG2",
        "$ARG1 techniques , gradient-based $ARG2",
        "$ARG1 sequence-to-sequence $ARG2",
        "$ARG1 variants of the $ARG2",
        "$ARG1 regularized autoencoders , the motivation for DAEs was to allow the $ARG2",
        "$ARG1 convolutional models in benchmarks of statistical $ARG2"
    ],
    "learning*****deep learning": [
        "$ARG1 algorithms , but they are still deservedly recognized for their important role in $ARG2",
        "$ARG1 the right representation for the data provides one perspective on $ARG2",
        "$ARG1 with continuous variables in the context of $ARG2"
    ],
    "contains*****1": [
        "$ARG1 l weight matrices : W ( $ARG2",
        "$ARG1 l + $ARG2",
        "$ARG1 two functions chained together : h = f ( $ARG2",
        "$ARG1 vectors x ( t ) with the time step index t ranging from $ARG2",
        "$ARG1 the classes ( 0 , 0 ) and ( 0 , $ARG2",
        "$ARG1 the classes ( $ARG2"
    ],
    "probability distribution*****1": [
        "$ARG1 represented by the DBN is given by ( l\u22121 ) \ue03e ( l\u22121 ) ( l\u22121 ) \ue03e ( l ) ( l ) ) \u221d exp b P ( h , h ( k+1 ) \ue03e ( k+1 ) \u2200i , \u2200k \u2208 $ARG2",
        "$ARG1 over a sequence of variables , p ( x ( $ARG2",
        "$ARG1 over all words in V as follows : P ( y = i | C ) =1i\u2208L P ( y = i | C , i \u2208 L ) ( $ARG2",
        "$ARG1 summing or integrating to $ARG2",
        "$ARG1 that can be factored as p ( a , b , c , d , e ) = $ARG2"
    ],
    "gibbs sampling*****hidden layer": [
        "$ARG1 on the top two $ARG2"
    ],
    "pass*****ancestral sampling": [
        "$ARG1 of $ARG2"
    ],
    "ancestral sampling*****model": [
        "$ARG1 through the rest of the $ARG2"
    ],
    "model*****deep belief network": [
        "$ARG1 to draw a sample from the visible $ARG2",
        "$ARG1 as a \u201c $ARG2",
        "$ARG1 is the convolutional $ARG2"
    ],
    "deep belief network*****undirected model": [
        "$ARG1 incur many of the problems associated with both directed models and $ARG2",
        "$ARG1 ( DBN ) , it is an entirely $ARG2",
        "$ARG1 \u201d but because it can be described as a purely $ARG2"
    ],
    "inference*****deep belief network": [
        "$ARG1 in a $ARG2"
    ],
    "deep belief network*****explaining away": [
        "$ARG1 is intractable due to the $ARG2"
    ],
    "explaining away*****interaction": [
        "$ARG1 e\ufb00ect within each directed layer , and due to the $ARG2"
    ],
    "interaction*****hidden layer": [
        "$ARG1 between the two $ARG2"
    ],
    "evidence lower bound*****expectation": [
        "$ARG1 takes the $ARG2"
    ],
    "expectation*****problem": [
        "$ARG1 of cliques whose size is equal to the network Evaluating or maximizing the log-likelihood requires not just confronting the $ARG2",
        "$ARG1 for a given and an and the fact that the $ARG2"
    ],
    "problem*****inference": [
        "$ARG1 of intractable $ARG2",
        "$ARG1 to be removed from the $ARG2",
        "$ARG1 of intractable $ARG2",
        "$ARG1 of di\ufb03cult $ARG2"
    ],
    "inference*****problem": [
        "$ARG1 to marginalize out the latent variables , but also the $ARG2",
        "$ARG1 network for only one $ARG2",
        "$ARG1 usually refers to the di\ufb03cult $ARG2"
    ],
    "problem*****partition function": [
        "$ARG1 of an intractable $ARG2",
        "$ARG1 of intractable $ARG2"
    ],
    "partition function*****undirected model": [
        "$ARG1 within the $ARG2",
        "$ARG1 for many $ARG2",
        "$ARG1 of an $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2"
    ],
    "deep belief network*****contrastive divergence": [
        "$ARG1 , one begins by training an RBM to maximize Ev\u223cpdata log p ( v ) using $ARG2"
    ],
    "contrastive divergence*****stochastic maximum likelihood": [
        "$ARG1 or $ARG2"
    ],
    "1*****probability distribution": [
        "$ARG1 ) is the $ARG2",
        "$ARG1 so that it represents a valid $ARG2"
    ],
    "other*****model": [
        "$ARG1 words , the second RBM is trained to $ARG2",
        "$ARG1 pieces of the $ARG2",
        "$ARG1 possible approaches to generative modeling await Researchers studying generative models often need to compare one generative $ARG2",
        "$ARG1 animals ( Crick and Mitchison , 1983 ) , the idea being that the brain maintains a probabilistic $ARG2",
        "$ARG1 approaches sidestep the issue , by training the $ARG2",
        "$ARG1 cases , sampling is actually our goal , in the sense that we want to train a $ARG2",
        "$ARG1 generative models we have described here is a variant ( Pham et al. , 1992 ) that trains a fully parametric generative $ARG2",
        "$ARG1 increases to the $ARG2",
        "$ARG1 variables b , with respect to a $ARG2",
        "$ARG1 features of the $ARG2",
        "$ARG1 part of the $ARG2",
        "$ARG1 times , these constraints and penalties are designed to express a generic preference for a simpler $ARG2",
        "$ARG1 ways to penalize the size of the $ARG2",
        "$ARG1 words , it pushes the $ARG2",
        "$ARG1 ways to express our prior knowledge about suitable values of the $ARG2",
        "$ARG1 hidden units are in the $ARG2"
    ],
    "model*****distribution": [
        "$ARG1 the $ARG2",
        "$ARG1 fails to learn to represent the $ARG2",
        "$ARG1 samples from the previous RBM \u2019 s posterior $ARG2",
        "$ARG1 the $ARG2",
        "$ARG1 , the VAE \ufb01rst draws a sample z from the code $ARG2",
        "$ARG1 that gradually restores the structure to an unstructured $ARG2",
        "$ARG1 is better at capturing some $ARG2",
        "$ARG1 has a better internal representation of the $ARG2",
        "$ARG1 \u2019 s $ARG2",
        "$ARG1 \u2019 s $ARG2",
        "$ARG1 \u2019 s $ARG2",
        "$ARG1 has learned to represent the sparsity in the data $ARG2",
        "$ARG1 or from the noise $ARG2",
        "$ARG1 and the noise $ARG2",
        "$ARG1 that can sample from the training $ARG2",
        "$ARG1 is trained to estimate the conditional $ARG2",
        "$ARG1 more robust when the $ARG2",
        "$ARG1 family is capable of representing the training $ARG2",
        "$ARG1 that captures the joint $ARG2",
        "$ARG1 representing a conditional $ARG2",
        "$ARG1 to represent a $ARG2",
        "$ARG1 representing a $ARG2",
        "$ARG1 an arbitrary $ARG2",
        "$ARG1 , it is best to sample the new x points from a $ARG2",
        "$ARG1 , such as its posterior $ARG2",
        "$ARG1 , one can sample from another $ARG2",
        "$ARG1 that de\ufb01nes a marginal $ARG2",
        "$ARG1 their joint $ARG2",
        "$ARG1 will still incur some error on many problems , because there may still be some noise in the $ARG2",
        "$ARG1 as producing a conditional $ARG2",
        "$ARG1 , in practice we typically assume a fairly broad $ARG2",
        "$ARG1 generates new samples from the $ARG2",
        "$ARG1 a $ARG2",
        "$ARG1 of the joint $ARG2",
        "$ARG1 a $ARG2",
        "$ARG1 needs to accurately capture the $ARG2",
        "$ARG1 with visible variables x and latent variables h , with an explicit joint $ARG2",
        "$ARG1 \u2019 s prior $ARG2",
        "$ARG1 \u2019 s $ARG2",
        "$ARG1 to have the same score as the data $ARG2",
        "$ARG1 and draw samples from this $ARG2",
        "$ARG1 , trained in an unsupervised paradigm ( to capture the $ARG2"
    ],
    "model*****classi\ufb01cation": [
        "$ARG1 , but most of the interest in DBNs arose from their ability to improve $ARG2",
        "$ARG1 can do that well , then it can pick the classes that yield the least $ARG2",
        "$ARG1 for $ARG2",
        "$ARG1 can be used to do $ARG2",
        "$ARG1 has in its $ARG2"
    ],
    "weights*****1": [
        "$ARG1 from the DBN and use them to de\ufb01ne an MLP : h ( $ARG2",
        "$ARG1 W ( 2 ) between h ( $ARG2",
        "$ARG1 for the jumps between the intermediate distributions given in p\u0303\u03b71 ( x\u03b71 ) p\u0303\u03b7 2 ( x \u03b72 ) p\u03031 ( x1 ) p\u0303 0 ( x \u03b71 ) p\u0303\u03b7 $ARG2",
        "$ARG1 on layers 3 through l are greater than $ARG2",
        "$ARG1 are normalized to sum to $ARG2",
        "$ARG1 between 0 and $ARG2",
        "$ARG1 into and from h ( $ARG2",
        "$ARG1 by \u00b5 \u223c N ( $ARG2"
    ],
    "weights*****classi\ufb01cation": [
        "$ARG1 and biases learned via generative training of the DBN , we may train the MLP to perform a $ARG2"
    ],
    "example*****discriminative \ufb01ne-tuning": [
        "$ARG1 of $ARG2"
    ],
    "approximate inference*****set": [
        "$ARG1 techniques are motivated by their ability to \ufb01nd a maximally tight variational lower bound on the log-likelihood under some $ARG2"
    ],
    "graphical model*****explaining away": [
        "$ARG1 has $ARG2"
    ],
    "quality*****deep belief network": [
        "$ARG1 as a The term \u201c $ARG2"
    ],
    "deep belief network*****neural network": [
        "$ARG1 \u201d is commonly used incorrectly to refer to any kind of deep $ARG2"
    ],
    "neural network*****variable": [
        "$ARG1 , even networks without latent $ARG2"
    ],
    "deep belief network*****belief network": [
        "$ARG1 \u201d may also cause some confusion because the term \u201c $ARG2"
    ],
    "belief network*****deep belief network": [
        "$ARG1 \u201d is sometimes used to refer to purely directed models , while $ARG2"
    ],
    "deep belief network*****bayesian network": [
        "$ARG1 also share the acronym DBN with dynamic $ARG2"
    ],
    "bayesian network*****markov chain": [
        "$ARG1 for representing $ARG2"
    ],
    "graphical model*****deep boltzmann machine": [
        "$ARG1 for a $ARG2",
        "$ARG1 , such as $ARG2"
    ],
    "deep boltzmann machine*****visible layer": [
        "$ARG1 with one $ARG2",
        "$ARG1 with one $ARG2"
    ],
    "visible layer*****hidden layer": [
        "$ARG1 ( bottom ) and two $ARG2",
        "$ARG1 , v , and three $ARG2"
    ],
    "deep boltzmann machine*****model": [
        "$ARG1 or DBM ( Salakhutdinov and Hinton , 2009a ) is another kind of deep , generative $ARG2",
        "$ARG1 ( Montavon and Muller , 2012 ) , which reparametrizes the $ARG2",
        "$ARG1 This $ARG2"
    ],
    "joint probability distribution*****model": [
        "$ARG1 over the $ARG2"
    ],
    "model*****energy function": [
        "$ARG1 variables is parametrized by an $ARG2",
        "$ARG1 is de\ufb01ned as the combination of two $ARG2",
        "$ARG1 is de\ufb01ned via its $ARG2",
        "$ARG1 are shown in Since the $ARG2"
    ],
    "hidden layer*****1": [
        "$ARG1 , h ( $ARG2",
        "$ARG1 , the activation probabilities are given by : P ( v i = $ARG2",
        "$ARG1 , these distributions are P ( v | h ( $ARG2",
        "$ARG1 , P ( h ( $ARG2",
        "$ARG1 , L is given by \ue058 \ue058 ( $ARG2"
    ],
    "1*****joint probability": [
        "$ARG1 ) , h ( 2 ) and h ( 3 ) , the $ARG2"
    ],
    "energy function*****1": [
        "$ARG1 is then de\ufb01ned as follows : E ( v , h ( $ARG2",
        "$ARG1 is : EmPoT ( x , h ( m ) , h ( c ) ) \ue058 ( c ) = E m ( x , h ( m ) ) + r ( j ) \ue03e x + ( $ARG2"
    ],
    "1*****deep boltzmann machine": [
        "$ARG1 ) Figure 20.3 : A $ARG2"
    ],
    "deep boltzmann machine*****graph": [
        "$ARG1 , re-arranged to reveal its bipartite $ARG2"
    ],
    "model*****inference": [
        "$ARG1 behavior as well as how we go about performing $ARG2",
        "$ARG1 , such as being able to perform $ARG2",
        "$ARG1 ( z ) approach Traditional approaches to variational $ARG2",
        "$ARG1 can provide answers to many $ARG2",
        "$ARG1 rather than just perform $ARG2",
        "$ARG1 , but does not force us to pay the cost of full $ARG2",
        "$ARG1 , performing $ARG2",
        "$ARG1 structure is tightly linked with the choice of $ARG2",
        "$ARG1 description , but it is also possible to train the $ARG2",
        "$ARG1 with exact $ARG2",
        "$ARG1 with latent variables and equipped with an $ARG2"
    ],
    "other*****probability": [
        "$ARG1 given the values of the neighboring layers , so the distributions over binary variables can be fully described by the Bernoulli parameters giving the $ARG2",
        "$ARG1 generative models are better at rarely assigning high $ARG2",
        "$ARG1 and would visit many di\ufb00erent regions in x space proportional to their $ARG2",
        "$ARG1 By de\ufb01ning p ( x ) to be \u03b4 shifted by \u2212\u00b5 we obtain an in\ufb01nitely narrow and in\ufb01nitely high peak of $ARG2",
        "$ARG1 applications require an approximation that rarely places high $ARG2"
    ],
    "example*****hidden layer": [
        "$ARG1 with two $ARG2",
        "$ARG1 of the DBM with two $ARG2",
        "$ARG1 with two $ARG2",
        "$ARG1 with two $ARG2",
        "$ARG1 , classes that were not linearly separable in the input features may become linearly separable in the last $ARG2",
        "$ARG1 , if an MLP with a single $ARG2",
        "$ARG1 , a $ARG2",
        "$ARG1 , suppose there are l fully connected $ARG2",
        "$ARG1 , we use a feedforward network with two input units , one $ARG2"
    ],
    "gibbs sampling*****deep boltzmann machine": [
        "$ARG1 in a $ARG2",
        "$ARG1 applied to a $ARG2"
    ],
    "gibbs sampling*****variable": [
        "$ARG1 is to update only one $ARG2",
        "$ARG1 , in which sampling from T ( x \ue030 | x ) is accomplished by selecting one $ARG2"
    ],
    "gibbs sampling*****visible layer": [
        "$ARG1 can be divided into two blocks of updates , one including all even layers ( including the $ARG2"
    ],
    "visible layer*****other": [
        "$ARG1 ) and the $ARG2"
    ],
    "connection*****distribution": [
        "$ARG1 pattern , given the even layers , the $ARG2"
    ],
    "classi\ufb01cation*****approximate inference": [
        "$ARG1 using a heuristically motivated $ARG2"
    ],
    "approximate inference*****mean \ufb01eld": [
        "$ARG1 procedure , in which we guess that a reasonable value for the $ARG2"
    ],
    "expectation*****pass": [
        "$ARG1 of the hidden units can be provided by an upward $ARG2"
    ],
    "pass*****sigmoid": [
        "$ARG1 through the network in an MLP that uses $ARG2"
    ],
    "activation function*****weights": [
        "$ARG1 and the same $ARG2"
    ],
    "interaction*****mean \ufb01eld": [
        "$ARG1 makes it possible to use \ufb01xed point equations to actually optimize the variational lower bound and \ufb01nd the true optimal $ARG2"
    ],
    "mean \ufb01eld*****approximate inference": [
        "$ARG1 allows the $ARG2"
    ],
    "view*****neuroscience": [
        "$ARG1 of $ARG2"
    ],
    "neuroscience*****human": [
        "$ARG1 , because the $ARG2"
    ],
    "property*****series": [
        "$ARG1 , DBMs have been used as computational models of real neuroscienti\ufb01c phenomena ( $ARG2"
    ],
    "other*****process": [
        "$ARG1 layers are used only at the end of the sampling $ARG2",
        "$ARG1 variables in the $ARG2",
        "$ARG1 words , we can think of the $ARG2"
    ],
    "process*****ancestral sampling": [
        "$ARG1 , in one e\ufb03cient $ARG2"
    ],
    "model*****markov chain": [
        "$ARG1 participating in every $ARG2",
        "$ARG1 for having a $ARG2",
        "$ARG1 faster than the $ARG2"
    ],
    "inference*****distribution": [
        "$ARG1 The conditional $ARG2",
        "$ARG1 , where we restrict the approximating $ARG2",
        "$ARG1 allows the construction of a $ARG2",
        "$ARG1 in the DBM to compute the $ARG2",
        "$ARG1 or simply from the need to normalize the $ARG2",
        "$ARG1 problems ( to determine the marginal $ARG2",
        "$ARG1 is to compute the single most likely value of the missing variables , rather than to infer the entire $ARG2",
        "$ARG1 , this justi\ufb01cation is rather vacuous , because the bound is in\ufb01nitely loose , due to the Dirac $ARG2",
        "$ARG1 and learn the parameters by maximizing the ELBO de\ufb01ned by the Dirac $ARG2",
        "$ARG1 allow us to learn using a point estimate of p ( h | v ) rather than inferring the entire $ARG2"
    ],
    "distribution*****hidden layer": [
        "$ARG1 over all $ARG2"
    ],
    "1*****interaction": [
        "$ARG1 ) , h ( 2 ) | v ) does not factorize due due to the $ARG2"
    ],
    "1*****distribution": [
        "$ARG1 ) and h ( 2 ) which render these variables As was the case with the DBN , we are left to seek out methods to approximate the DBM posterior $ARG2",
        "$ARG1 ) , h ( 2 ) | v ) In general , we do not have to provide a parametric form of the approximating $ARG2",
        "$ARG1 ) to obtain a sample from the desired $ARG2",
        "$ARG1 ) can represent a conditional $ARG2",
        "$ARG1 , are su\ufb03ciently close , we can reliably estimate each of the factors sampling and then use these to obtain an Where do these intermediate distributions come from ? Just as the original proposal $ARG2",
        "$ARG1 ) as the proposal $ARG2",
        "$ARG1 , w\ue03eh + b This would indeed de\ufb01ne a valid conditional $ARG2",
        "$ARG1 , so the $ARG2",
        "$ARG1 , the $ARG2"
    ],
    "distribution*****mean \ufb01eld": [
        "$ARG1 over their hidden units\u2014while complicated\u2014is easy to approximate with a variational approximation ( as discussed in section 19.4 ) , speci\ufb01cally a $ARG2",
        "$ARG1 should be obtained by running the $ARG2"
    ],
    "mean \ufb01eld*****inference": [
        "$ARG1 approximation is a simple form of variational $ARG2",
        "$ARG1 recurrent $ARG2",
        "$ARG1 to improve its estimates , and training the $ARG2"
    ],
    "inference*****task": [
        "$ARG1 , we approach the $ARG2"
    ],
    "mean \ufb01eld*****set": [
        "$ARG1 approximation , the approximating family is the $ARG2"
    ],
    "mean \ufb01eld*****example": [
        "$ARG1 approach for the $ARG2"
    ],
    "assumption*****1": [
        "$ARG1 implies that Q ( h ( $ARG2",
        "$ARG1 , we can add connections from the output at time t to the hidden unit at time t + $ARG2"
    ],
    "mean \ufb01eld*****1": [
        "$ARG1 approximation attempts to \ufb01nd a member of this family of distributions that best \ufb01ts the true posterior P ( h ( $ARG2",
        "$ARG1 approach is to minimize ( 2 ) | v ) Q ( h , h KL ( Q\ue06bP ) = Q ( h ( $ARG2"
    ],
    "process*****distribution": [
        "$ARG1 must be run again to \ufb01nd a di\ufb00erent $ARG2",
        "$ARG1 whose output changes every time we query it , it may seem counterintuitive to take the derivatives of y with respect to the parameters of its $ARG2",
        "$ARG1 itself rather than the mathematical speci\ufb01cation of the joint $ARG2",
        "$ARG1 that brings a $ARG2",
        "$ARG1 thus converges to a stationary $ARG2",
        "$ARG1 converges to sampling from the correct $ARG2",
        "$ARG1 C ( x\u0303 | x ) which represents a conditional $ARG2",
        "$ARG1 to make use of all of the knowledge about the input $ARG2"
    ],
    "distribution*****independence": [
        "$ARG1 beyond enforcing the $ARG2"
    ],
    "assumption*****loss": [
        "$ARG1 on binary hidden units ( the case we are developing here ) there is no $ARG2"
    ],
    "loss*****model": [
        "$ARG1 of generality resulting from \ufb01xing a parametrization of the $ARG2",
        "$ARG1 , with respect to a given $ARG2"
    ],
    "bernoulli distribution*****probability": [
        "$ARG1 , that is we associate the $ARG2"
    ],
    "probability*****element": [
        "$ARG1 of each $ARG2",
        "$ARG1 event for an $ARG2"
    ],
    "element*****1": [
        "$ARG1 of h ( $ARG2",
        "$ARG1 of y\u02c6i be between 0 and $ARG2",
        "$ARG1 table indexed by t0 and t $ARG2",
        "$ARG1 of x moves away from 0 by \ue00f , the L $ARG2"
    ],
    "bias*****\u03c3": [
        "$ARG1 terms ) : ( 2 ) ( 2 ) v i Wi , j + Wj , k\ue030 h\u0302 k\ue030 , \u2200j h\u0302k = $ARG2"
    ],
    "gradient*****learning": [
        "$ARG1 for $ARG2",
        "$ARG1 of 0 is typically problematic because the $ARG2",
        "$ARG1 can shrink too small to be useful for $ARG2",
        "$ARG1 direction is far more useful for $ARG2",
        "$ARG1 explosion prevents $ARG2"
    ],
    "learning*****quality": [
        "$ARG1 , and \ufb01fty usually su\ufb03ce to obtain a high $ARG2"
    ],
    "quality*****example": [
        "$ARG1 representation of a single speci\ufb01c $ARG2"
    ],
    "example*****classi\ufb01cation": [
        "$ARG1 to be used for high-accuracy $ARG2",
        "$ARG1 of a $ARG2",
        "$ARG1 , suppose we have a multiclass $ARG2",
        "$ARG1 , a $ARG2"
    ],
    "learning*****challenge": [
        "$ARG1 in the DBM must confront both the $ARG2"
    ],
    "challenge*****partition function": [
        "$ARG1 of an intractable $ARG2",
        "$ARG1 of intractable $ARG2"
    ],
    "partition function*****challenge": [
        "$ARG1 , using the techniques from chapter 18 , and the $ARG2"
    ],
    "challenge*****distribution": [
        "$ARG1 of an intractable posterior $ARG2"
    ],
    "deep boltzmann machine*****hidden layer": [
        "$ARG1 with two $ARG2"
    ],
    "contains*****partition function": [
        "$ARG1 the log $ARG2"
    ],
    "partition function*****restricted boltzmann machine": [
        "$ARG1 and sampling that apply to $ARG2",
        "$ARG1 of $ARG2"
    ],
    "restricted boltzmann machine*****deep boltzmann machine": [
        "$ARG1 also apply to $ARG2"
    ],
    "probability mass function*****boltzmann machine": [
        "$ARG1 of a $ARG2"
    ],
    "boltzmann machine*****annealed importance sampling": [
        "$ARG1 requires approximate methods such as $ARG2"
    ],
    "model*****gradient": [
        "$ARG1 requires approximations to the $ARG2",
        "$ARG1 , we need to be able to back-propagate the $ARG2",
        "$ARG1 each player as making in\ufb01nitesimally small $ARG2",
        "$ARG1 of the world and follows the $ARG2",
        "$ARG1 often requires computing the $ARG2",
        "$ARG1 is able to represent long term dependencies , the $ARG2",
        "$ARG1 \u2019 s output is very far from correct , it is clear simply from computing the $ARG2",
        "$ARG1 if we can e\ufb03ciently obtain an estimate of the $ARG2"
    ],
    "gradient*****partition function": [
        "$ARG1 of the log $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 directly confront the $ARG2",
        "$ARG1 of the log $ARG2",
        "$ARG1 of the log $ARG2",
        "$ARG1 of the logarithm of the $ARG2",
        "$ARG1 of the log $ARG2"
    ],
    "contrastive divergence*****deep boltzmann machine": [
        "$ARG1 is slow for $ARG2"
    ],
    "deep boltzmann machine*****contrastive divergence": [
        "$ARG1 because they do not allow e\ufb03cient sampling of the hidden units given the visible units\u2014instead , $ARG2"
    ],
    "contrastive divergence*****markov chain": [
        "$ARG1 would require burning in a $ARG2",
        "$ARG1 initializes its $ARG2",
        "$ARG1 using a speci\ufb01c kind of $ARG2"
    ],
    "markov chain*****negative phase": [
        "$ARG1 every time a new $ARG2"
    ],
    "stochastic maximum likelihood*****algorithm": [
        "$ARG1 as applied to the DBM is given in $ARG2"
    ],
    "recall*****bias parameter": [
        "$ARG1 that we describe a simpli\ufb01ed varient of the DBM that lacks $ARG2"
    ],
    "stochastic maximum likelihood*****initialization": [
        "$ARG1 ( as described above ) from a random $ARG2"
    ],
    "weights*****distribution": [
        "$ARG1 in all but the \ufb01rst layer represents approximately the same $ARG2",
        "$ARG1 between p\u03030 and p\u0303 \u2217 and between p\u03031 and p\u0303\u2217 : \ue058 p\u0303\u2217 ( x ( k ) ) p\u0303\u2217 ( x0 ) p\u03030 ( x ) p\u03031 ( x ) If the bridge $ARG2"
    ],
    "method*****problem": [
        "$ARG1 for overcoming the joint training $ARG2",
        "$ARG1 of conjugate gradients seeks to address this $ARG2",
        "$ARG1 can solve the $ARG2"
    ],
    "problem*****pretraining": [
        "$ARG1 of DBMs is greedy layer-wise $ARG2"
    ],
    "algorithm*****stochastic maximum likelihood": [
        "$ARG1 20.1 The variational $ARG2",
        "$ARG1 18.3 The $ARG2"
    ],
    "algorithm*****hidden layer": [
        "$ARG1 for training a DBM with two $ARG2",
        "$ARG1 is that we need to store the input to the nonlinearity of the $ARG2"
    ],
    "set*****number": [
        "$ARG1 \ue00f , the step size , to a small positive $ARG2",
        "$ARG1 k , the $ARG2",
        "$ARG1 \ue00f , the step size , to a small positive $ARG2",
        "$ARG1 k , the $ARG2",
        "$ARG1 \ue00f , the step size , to a small positive $ARG2",
        "$ARG1 k , the $ARG2",
        "$ARG1 \ue00f , the step size , to a small positive $ARG2",
        "$ARG1 k , the $ARG2",
        "$ARG1 is large and the $ARG2",
        "$ARG1 to the $ARG2",
        "$ARG1 of variables increases exponentially as the $ARG2",
        "$ARG1 of points that can be approximated well by considering only a small $ARG2",
        "$ARG1 { .1 , .01 , 10\u22123 , 10\u22124 , 10\u22125 } , or a $ARG2"
    ],
    "number*****markov chain": [
        "$ARG1 of Gibbs steps , high enough to allow a $ARG2",
        "$ARG1 of Gibbs steps , high enough to allow a $ARG2",
        "$ARG1 line in R. This corresponds to a $ARG2",
        "$ARG1 of Gibbs steps , high enough to allow a $ARG2",
        "$ARG1 of $ARG2"
    ],
    "markov chain*****1": [
        "$ARG1 of p ( v , h ( $ARG2"
    ],
    "1*****set": [
        "$ARG1 ) and H\u0303 ( 2 ) each with m rows $ARG2",
        "$ARG1 ) , illustrating examples of some $ARG2",
        "$ARG1 , 2,3 } by de\ufb01ning the $ARG2"
    ],
    "set*****bernoulli distribution": [
        "$ARG1 to random values ( e.g. , from $ARG2"
    ],
    "bernoulli distribution*****model": [
        "$ARG1 , possibly with marginals matched to the $ARG2"
    ],
    "loop*****minibatch": [
        "$ARG1 ) do Sample a $ARG2"
    ],
    "minibatch*****design matrix": [
        "$ARG1 of m examples from the training data and arrange them as the rows of a $ARG2",
        "$ARG1 of activations of the layer to normalize , arranged as a $ARG2"
    ],
    "1*****model": [
        "$ARG1 ) and H\u0302 ( 2 ) , possibly to the $ARG2",
        "$ARG1 states , or equivalently as a $ARG2",
        "$ARG1 \ue058 p $ARG2",
        "$ARG1 , provide accurate samples from the $ARG2",
        "$ARG1 ) ( x ; W , c ) and y = f ( 2 ) ( h ; w , b ) , with the complete $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 ) to u ( ni ) correspond to the parameters of the $ARG2"
    ],
    "inference*****1": [
        "$ARG1 H\u0302 ( $ARG2",
        "$ARG1 equations never assign 0 or $ARG2"
    ],
    "1*****gibbs sampling": [
        "$ARG1 to k ( $ARG2"
    ],
    "gibbs sampling*****1": [
        "$ARG1 ) do Gibbs block $ARG2",
        "$ARG1 step with a = $ARG2"
    ],
    "1*****illustration": [
        "$ARG1 ) ( this is a cartoon $ARG2",
        "$ARG1 ] \ue021 Figure 15.7 : $ARG2",
        "$ARG1 ) Figure 8.7 : $ARG2"
    ],
    "illustration*****algorithm": [
        "$ARG1 , in practice use a more e\ufb00ective $ARG2",
        "$ARG1 of how the nearest neighbor $ARG2",
        "$ARG1 of how the nearest neighbor $ARG2"
    ],
    "algorithm*****learning rate": [
        "$ARG1 , such as momentum with a decaying $ARG2",
        "$ARG1 is the $ARG2",
        "$ARG1 ( Jacobs , 1988 ) is an early heuristic approach to adapting individual $ARG2",
        "$ARG1 8.4 , individually adapts the $ARG2",
        "$ARG1 Require : Global $ARG2",
        "$ARG1 Require : Global $ARG2"
    ],
    "change*****model": [
        "$ARG1 in the $ARG2",
        "$ARG1 its output , even when the $ARG2",
        "$ARG1 based on the input sequence , because the time constants are output by the $ARG2",
        "$ARG1 the $ARG2"
    ],
    "weights*****pass": [
        "$ARG1 are e\ufb00ectively doubled during the upward $ARG2"
    ],
    "deep boltzmann machine*****standard": [
        "$ARG1 requires a modi\ufb01cation of the $ARG2"
    ],
    "standard*****algorithm": [
        "$ARG1 SML $ARG2",
        "$ARG1 form in $ARG2"
    ],
    "algorithm*****mean \ufb01eld": [
        "$ARG1 , which is to use a small amount of $ARG2"
    ],
    "mean \ufb01eld*****negative phase": [
        "$ARG1 during the $ARG2",
        "$ARG1 in the $ARG2"
    ],
    "expectation*****gradient": [
        "$ARG1 of the energy $ARG2"
    ],
    "gradient*****mean \ufb01eld": [
        "$ARG1 should be computed with respect to the $ARG2"
    ],
    "distribution*****other": [
        "$ARG1 in which all of the units are independent from each $ARG2",
        "$ARG1 for $ARG2",
        "$ARG1 in order to solve the $ARG2",
        "$ARG1 P ( x ) to compute some $ARG2"
    ],
    "deep boltzmann machine*****unsupervised pretraining": [
        "$ARG1 Classic DBMs require greedy $ARG2"
    ],
    "unsupervised pretraining*****classi\ufb01cation": [
        "$ARG1 , and to perform $ARG2"
    ],
    "deep boltzmann machine*****dataset": [
        "$ARG1 training procedure used to classify the MNIST $ARG2",
        "$ARG1 trained on the MNIST $ARG2"
    ],
    "1*****learning": [
        "$ARG1 to 20 during $ARG2",
        "$ARG1 ) , the $ARG2"
    ],
    "set*****1": [
        "$ARG1 of features h ( $ARG2",
        "$ARG1 of m samples { x\u0303 ( $ARG2",
        "$ARG1 of m samples { x\u0303 ( $ARG2",
        "$ARG1 of size n \u2212 $ARG2",
        "$ARG1 consists of m examples { x ( $ARG2",
        "$ARG1 to $ARG2",
        "$ARG1 of input points for which hi = $ARG2",
        "$ARG1 into the \ufb01rst ni nodes u ( $ARG2",
        "$ARG1 of values { y ( $ARG2",
        "$ARG1 { x ( $ARG2",
        "$ARG1 to roughly $ARG2",
        "$ARG1 { x ( $ARG2",
        "$ARG1 { x ( $ARG2",
        "$ARG1 { x ( $ARG2",
        "$ARG1 { x ( $ARG2",
        "$ARG1 { x ( $ARG2",
        "$ARG1 { x ( $ARG2",
        "$ARG1 of m examples Initialize \u03c10 = 0 Initialize g0 = 0 Initialize t = $ARG2",
        "$ARG1 to $ARG2",
        "$ARG1 of entities { $ARG2",
        "$ARG1 containing m elements : { x ( $ARG2",
        "$ARG1 w $ARG2",
        "$ARG1 to $ARG2",
        "$ARG1 of samples { x ( $ARG2",
        "$ARG1 of samples { x ( $ARG2",
        "$ARG1 of samples { x ( $ARG2",
        "$ARG1 of m examples X = { x ( $ARG2",
        "$ARG1 of data samples { x ( $ARG2",
        "$ARG1 of elementary basis vectors ( { [ $ARG2",
        "$ARG1 \u03bb $ARG2",
        "$ARG1 S = { $ARG2",
        "$ARG1 of vectors { v ( $ARG2",
        "$ARG1 consists of m labeled examples { ( x ( $ARG2"
    ],
    "1*****mean \ufb01eld": [
        "$ARG1 ) and h ( 2 ) that are obtained by running $ARG2"
    ],
    "pass*****mean \ufb01eld": [
        "$ARG1 of $ARG2"
    ],
    "mean \ufb01eld*****output layer": [
        "$ARG1 , with an additional $ARG2"
    ],
    "output layer*****weights": [
        "$ARG1 for the estimate of y. Initialize the MLP \u2019 s $ARG2"
    ],
    "stochastic gradient descent*****dropout": [
        "$ARG1 and $ARG2"
    ],
    "boltzmann machine*****model": [
        "$ARG1 probabilistic $ARG2",
        "$ARG1 , extended to $ARG2",
        "$ARG1 framework is a rich space of models permitting many more $ARG2",
        "$ARG1 , which require extremely careful $ARG2",
        "$ARG1 was \ufb01rst introduced to describe a $ARG2"
    ],
    "problem*****deep boltzmann machine": [
        "$ARG1 of the $ARG2"
    ],
    "model*****cost function": [
        "$ARG1 in order to make the Hessian of the $ARG2",
        "$ARG1 and the $ARG2",
        "$ARG1 p ( y | x ) automatically determines a $ARG2",
        "$ARG1 that represents certain beliefs , designing a $ARG2",
        "$ARG1 to be nonlinear , then most $ARG2",
        "$ARG1 for which the true $ARG2",
        "$ARG1 the $ARG2",
        "$ARG1 , and the degree to which the $ARG2"
    ],
    "cost function*****learning": [
        "$ARG1 better-conditioned at the beginning of the $ARG2",
        "$ARG1 must be large and predictable enough to serve as a good guide for the $ARG2",
        "$ARG1 , while exploding gradients can make $ARG2",
        "$ARG1 value even in the absence of any local minima or saddle Future research will need to develop further understanding of the factors that in\ufb02uence the length of the $ARG2",
        "$ARG1 typically includes at least one term that causes the $ARG2"
    ],
    "model*****pretraining": [
        "$ARG1 that can be trained without a greedy layer-wise $ARG2",
        "$ARG1 , it has not been applied as a $ARG2"
    ],
    "model*****set": [
        "$ARG1 obtains excellent test $ARG2",
        "$ARG1 may assign a log-likelihood of negative in\ufb01nity to the test $ARG2",
        "$ARG1 over\ufb01ts the training $ARG2",
        "$ARG1 has clearly over\ufb01t , because it does not produces images that were not in the training $ARG2",
        "$ARG1 of the test $ARG2",
        "$ARG1 as being fake , and all samples from the training $ARG2",
        "$ARG1 capacity greater so that it can \ufb01t the training $ARG2",
        "$ARG1 ( the $ARG2",
        "$ARG1 is said to be identi\ufb01able if a su\ufb03ciently large training $ARG2",
        "$ARG1 of real values p ( y | x ) = N ( y ; f ( \u03b8 ) , \u03b2 \u22121 ) can have negative log-likelihood that asymptotes to negative in\ufb01nity\u2014if f ( \u03b8 ) is able to correctly predict the value of all training $ARG2",
        "$ARG1 on the validation $ARG2",
        "$ARG1 trained on the original training $ARG2",
        "$ARG1 needed to \ufb01t the training $ARG2",
        "$ARG1 ranks a held-out of $ARG2",
        "$ARG1 on the test $ARG2",
        "$ARG1 on the test $ARG2",
        "$ARG1 , we have access to a training $ARG2",
        "$ARG1 by measuring its performance on a test $ARG2",
        "$ARG1 is not able to obtain a su\ufb03ciently low error value on the training $ARG2",
        "$ARG1 simply stores the X and y from the training $ARG2",
        "$ARG1 looks up the nearest entry in the training $ARG2",
        "$ARG1 , the training error increases as the size of the training $ARG2",
        "$ARG1 size , the cost per SGD update does not depend on the training $ARG2",
        "$ARG1 as the training $ARG2",
        "$ARG1 will over\ufb01t the training $ARG2",
        "$ARG1 , and a $ARG2",
        "$ARG1 induces a representation h of the data v. We can often use Eh\u223cp ( h|v ) [ h ] as a $ARG2",
        "$ARG1 to infer h from v is that we do not have a supervised training $ARG2",
        "$ARG1 with better validation $ARG2",
        "$ARG1 capacity have such a U-shaped validation $ARG2",
        "$ARG1 by determining how many steps it can take to \ufb01t the training $ARG2",
        "$ARG1 components as sharing a unique $ARG2",
        "$ARG1 capacity and training $ARG2",
        "$ARG1 of the validation $ARG2",
        "$ARG1 after training then reloading it for test $ARG2"
    ],
    "set*****quality": [
        "$ARG1 log-likelihood and produces high $ARG2"
    ],
    "model*****back-propagation": [
        "$ARG1 uses an alternative training criterion that allows the use of the $ARG2",
        "$ARG1 could no longer be trained using $ARG2"
    ],
    "algorithm*****gradient": [
        "$ARG1 in order to avoid the problems with MCMC estimates of the $ARG2",
        "$ARG1 , using $ARG2",
        "$ARG1 using $ARG2",
        "$ARG1 is almost always based on using the $ARG2",
        "$ARG1 no longer has a guide for how to improve the corresponding Instead , it is better to use a di\ufb00erent approach that ensures there is always a strong $ARG2",
        "$ARG1 can be applied to these tasks as well , and is not restricted to computing the $ARG2",
        "$ARG1 that speci\ufb01es the actual $ARG2",
        "$ARG1 is used to compute the $ARG2",
        "$ARG1 is not capable of simplifying the $ARG2",
        "$ARG1 that follows the $ARG2",
        "$ARG1 8.1 shows how to follow this estimate of the $ARG2",
        "$ARG1 always observes $ARG2",
        "$ARG1 would simply take a single step based on each $ARG2",
        "$ARG1 that was inspired by Nesterov \u2019 s accelerated $ARG2",
        "$ARG1 8.9 The conjugate $ARG2",
        "$ARG1 can then compute a $ARG2",
        "$ARG1 : the $ARG2",
        "$ARG1 then follows the estimated $ARG2",
        "$ARG1 where the M step consists of taking a single $ARG2",
        "$ARG1 , that the $ARG2",
        "$ARG1 , and that the $ARG2"
    ],
    "classi\ufb01cation*****missing inputs": [
        "$ARG1 performance and ability to reason well about $ARG2",
        "$ARG1 despite the presence of $ARG2",
        "$ARG1 with $ARG2",
        "$ARG1 with $ARG2",
        "$ARG1 with $ARG2"
    ],
    "boltzmann machine*****view": [
        "$ARG1 is easiest to describe if we return to the general $ARG2"
    ],
    "view*****boltzmann machine": [
        "$ARG1 of a $ARG2"
    ],
    "set*****matrix": [
        "$ARG1 of units x with a weight $ARG2",
        "$ARG1 of iterative techniques for performing various operations like approximately inverting a $ARG2",
        "$ARG1 into a $ARG2",
        "$ARG1 of biases , W is a learnable weight $ARG2",
        "$ARG1 of m-dimensional vectors can have more than m mutually linearly independent columns , but a $ARG2",
        "$ARG1 the weight $ARG2"
    ],
    "recall*****energy function": [
        "$ARG1 from equation 20.2 that he $ARG2"
    ],
    "matrix*****boltzmann machine": [
        "$ARG1 U , we can implement structures of $ARG2"
    ],
    "change*****set": [
        "$ARG1 the $ARG2",
        "$ARG1 between these two values does not marginally make much di\ufb00erence in terms of validation $ARG2"
    ],
    "set*****probability distribution": [
        "$ARG1 of $ARG2",
        "$ARG1 c arbitrarily high , rather than setting c to create a valid $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of functions is to learn a $ARG2",
        "$ARG1 are identically distributed , drawn from the same $ARG2",
        "$ARG1 and the $ARG2",
        "$ARG1 of variables and we want to know the $ARG2"
    ],
    "probability distribution*****model": [
        "$ARG1 that the $ARG2",
        "$ARG1 estimated by the $ARG2",
        "$ARG1 p A ( x ; \u03b8 A ) = Z1A p\u0303A ( x ; \u03b8A ) and $ARG2",
        "$ARG1 for which a minimum depth of SPN is required to avoid needing an exponentially large $ARG2",
        "$ARG1 de\ufb01ned by $ARG2",
        "$ARG1 over parameters of the $ARG2",
        "$ARG1 can be represented by either a directed $ARG2",
        "$ARG1 based on encouraging the $ARG2",
        "$ARG1 over the parameters of a $ARG2"
    ],
    "model*****change": [
        "$ARG1 can represent , but it does $ARG2",
        "$ARG1 p ( h , x ) , we have seen that if p ( h | x ) encodes x too well , then sampling from p ( x | h ) will not $ARG2",
        "$ARG1 to learn features that $ARG2",
        "$ARG1 to $ARG2"
    ],
    "change*****stochastic gradient descent": [
        "$ARG1 the dynamics of $ARG2"
    ],
    "hessian matrix*****boltzmann machine": [
        "$ARG1 improves , and observed that the centering trick is equivalent to another $ARG2"
    ],
    "technique*****gradient": [
        "$ARG1 , the enhanced $ARG2",
        "$ARG1 for approximating the $ARG2"
    ],
    "hessian matrix*****learning": [
        "$ARG1 allows $ARG2"
    ],
    "learning*****deep boltzmann machine": [
        "$ARG1 to succeed , even in di\ufb03cult cases like training a $ARG2"
    ],
    "deep boltzmann machine*****multiple": [
        "$ARG1 with $ARG2"
    ],
    "deep boltzmann machine*****mean \ufb01eld": [
        "$ARG1 ( MP-DBM ) which works by viewing the $ARG2"
    ],
    "mean \ufb01eld*****recurrent network": [
        "$ARG1 equations as de\ufb01ning a family of $ARG2"
    ],
    "recurrent network*****inference": [
        "$ARG1 for approximately solving every possible $ARG2",
        "$ARG1 obtain an accurate answer to the corresponding $ARG2"
    ],
    "model*****recurrent network": [
        "$ARG1 is trained to make each $ARG2"
    ],
    "example*****subset": [
        "$ARG1 , randomly sampling a $ARG2",
        "$ARG1 , we sample a $ARG2"
    ],
    "subset*****inference": [
        "$ARG1 of inputs to the $ARG2",
        "$ARG1 of the data variables to serve as inputs to the $ARG2"
    ],
    "inference*****principle": [
        "$ARG1 network to predict the values of the remaining This general $ARG2"
    ],
    "principle*****computational graph": [
        "$ARG1 of back-propagating through the $ARG2"
    ],
    "computational graph*****approximate inference": [
        "$ARG1 for $ARG2"
    ],
    "approximate inference*****other": [
        "$ARG1 has been applied to $ARG2",
        "$ARG1 approximations of p ( h | v ) , the lower bound L will be tighter , in $ARG2",
        "$ARG1 error in v given the code of the $ARG2",
        "$ARG1 has also been applied to $ARG2"
    ],
    "loss*****distribution": [
        "$ARG1 is typically based on the approximate conditional $ARG2",
        "$ARG1 associated with an output $ARG2"
    ],
    "distribution*****approximate inference": [
        "$ARG1 that the $ARG2"
    ],
    "boltzmann machine*****gibbs sampling": [
        "$ARG1 learned by the MP-DBM , it tends to be somewhat defective , in the sense that $ARG2"
    ],
    "model*****approximate inference": [
        "$ARG1 as it is really used\u2014with $ARG2",
        "$ARG1 that uses learned $ARG2",
        "$ARG1 in a way that makes the approximating assumptions underlying the $ARG2",
        "$ARG1 families that allow a fast $ARG2",
        "$ARG1 , improve $ARG2"
    ],
    "approximate inference*****example": [
        "$ARG1 , for $ARG2"
    ],
    "example*****missing inputs": [
        "$ARG1 , to \ufb01ll in $ARG2"
    ],
    "missing inputs*****classi\ufb01cation": [
        "$ARG1 , or to perform $ARG2",
        "$ARG1 : $ARG2",
        "$ARG1 ; $ARG2"
    ],
    "classi\ufb01cation*****inference": [
        "$ARG1 results with the original DBM were based on training a separate classi\ufb01er to use features extracted by the DBM , rather than by using $ARG2"
    ],
    "other*****approximate inference": [
        "$ARG1 advantage of back-propagating through $ARG2",
        "$ARG1 algorithms we describe in this chapter , EM is not an approach to $ARG2"
    ],
    "approximate inference*****back-propagation": [
        "$ARG1 is that $ARG2"
    ],
    "back-propagation*****gradient": [
        "$ARG1 computes the exact $ARG2",
        "$ARG1 , computing a vector-valued $ARG2",
        "$ARG1 only for the computation of a $ARG2",
        "$ARG1 is therefore not the only way or the optimal way of computing the $ARG2",
        "$ARG1 at every $ARG2"
    ],
    "gradient*****loss": [
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "optimization*****bias": [
        "$ARG1 than the approximate gradients of SML training , which su\ufb00er from both $ARG2"
    ],
    "bias*****variance": [
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 or $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 , we can compute its $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 and one with more $ARG2",
        "$ARG1 and one that su\ufb00ers from large $ARG2",
        "$ARG1 and the $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 ( dotted ) tends to decrease and $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 , $ARG2",
        "$ARG1 for reduced $ARG2"
    ],
    "illustration*****process": [
        "$ARG1 of the multi-prediction training $ARG2"
    ],
    "process*****deep boltzmann machine": [
        "$ARG1 for a $ARG2"
    ],
    "row*****example": [
        "$ARG1 indicates a di\ufb00erent $ARG2",
        "$ARG1 per $ARG2"
    ],
    "example*****minibatch": [
        "$ARG1 within a $ARG2",
        "$ARG1 or a $ARG2",
        "$ARG1 , it is very common to use the term \u201c batch size \u201d to describe the size of a $ARG2",
        "$ARG1 datasets containing billions of examples in a data center , it can be impractical to sample examples truly uniformly at random every time we want to construct a $ARG2",
        "$ARG1 into a $ARG2"
    ],
    "process*****other": [
        "$ARG1 , with arrows indicating which variables in\ufb02uence which $ARG2",
        "$ARG1 are independent from each $ARG2",
        "$ARG1 but also many $ARG2"
    ],
    "view*****inference": [
        "$ARG1 the $ARG2"
    ],
    "process*****example": [
        "$ARG1 for each $ARG2",
        "$ARG1 an $ARG2",
        "$ARG1 an $ARG2",
        "$ARG1 from a clean $ARG2",
        "$ARG1 any $ARG2"
    ],
    "example*****recurrent network": [
        "$ARG1 as a $ARG2",
        "$ARG1 of a $ARG2",
        "$ARG1 assumed that the Jacobian was the same at every time step , corresponding to a $ARG2",
        "$ARG1 of a $ARG2"
    ],
    "gradient descent*****back-propagation": [
        "$ARG1 and $ARG2",
        "$ARG1 following gradients computed by $ARG2"
    ],
    "back-propagation*****recurrent network": [
        "$ARG1 to train these $ARG2"
    ],
    "graph*****pseudolikelihood": [
        "$ARG1 is that it does not provide a way to optimize the log-likelihood , but rather a heuristic approximation of the generalized $ARG2"
    ],
    "dropout*****computational graph": [
        "$ARG1 shares the same parameters among many di\ufb00erent $ARG2"
    ],
    "computational graph*****graph": [
        "$ARG1 , with the di\ufb00erence between each $ARG2",
        "$ARG1 and add additional nodes to the $ARG2",
        "$ARG1 is a directed acyclic $ARG2",
        "$ARG1 , it is important to specify that the automatic di\ufb00erentiation software should not di\ufb00erentiate through the $ARG2",
        "$ARG1 but the depth of the $ARG2"
    ],
    "boltzmann machine*****image": [
        "$ARG1 were originally developed for use with binary data , many applications such as $ARG2"
    ],
    "image*****audio": [
        "$ARG1 and $ARG2"
    ],
    "audio*****probability distribution": [
        "$ARG1 modeling seem to require the ability to represent $ARG2"
    ],
    "1*****expectation": [
        "$ARG1 ] as representing the $ARG2"
    ],
    "expectation*****variable": [
        "$ARG1 of a binary $ARG2"
    ],
    "example*****set": [
        "$ARG1 , Hinton ( 2000 ) treats grayscale images in the training $ARG2",
        "$ARG1 , we may learn about one $ARG2",
        "$ARG1 , we typically do not want to partition the data into the $ARG2",
        "$ARG1 , the test $ARG2",
        "$ARG1 in the training $ARG2",
        "$ARG1 , we may $ARG2",
        "$ARG1 training $ARG2",
        "$ARG1 training $ARG2",
        "$ARG1 , we can always \ufb01t the training $ARG2",
        "$ARG1 from the test $ARG2",
        "$ARG1 in the training $ARG2",
        "$ARG1 , to access x1 , x3 and x6 , we de\ufb01ne the $ARG2",
        "$ARG1 , there is not any guarantee that the objective on the validation $ARG2",
        "$ARG1 a $ARG2",
        "$ARG1 , the training $ARG2",
        "$ARG1 by making fractal-shaped sets or sets that are de\ufb01ned by transforming the $ARG2"
    ],
    "set*****probability": [
        "$ARG1 as de\ufb01ning [ 0,1 ] $ARG2",
        "$ARG1 , but may also assign high $ARG2",
        "$ARG1 , but it has also under\ufb01t , because it assigns no $ARG2",
        "$ARG1 label y is correct with $ARG2",
        "$ARG1 of cards , we use exactly the same formulas as when we compute the $ARG2",
        "$ARG1 of common sense assumptions implies that the same axioms must control both kinds of $ARG2",
        "$ARG1 of sets that we can compute the $ARG2"
    ],
    "probability*****1": [
        "$ARG1 of a binary value being $ARG2",
        "$ARG1 of being $ARG2",
        "$ARG1 under p $ARG2",
        "$ARG1 of also assigning b to be $ARG2",
        "$ARG1 of assigning b to be \u22121 is close to $ARG2",
        "$ARG1 , it must lie in the interval [ 0 , $ARG2",
        "$ARG1 : P ( y = $ARG2",
        "$ARG1 of the n -th value may be obtained by subtracting the \ufb01rst n \u2212 $ARG2",
        "$ARG1 of w 4 can be decomposed as follows : P ( y = w4 ) = P ( b0 = $ARG2",
        "$ARG1 of class $ARG2",
        "$ARG1 of class 0 , because these two values must add up to $ARG2",
        "$ARG1 : p ( y = $ARG2",
        "$ARG1 of exactly 0 or exactly $ARG2",
        "$ARG1 of each entry being $ARG2",
        "$ARG1 to represent a degree of belief , with $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 12 , we choose the value of s to be $ARG2",
        "$ARG1 is given by $ARG2",
        "$ARG1 mass 1m on each of the m points x ( $ARG2"
    ],
    "1*****other": [
        "$ARG1 , and the binary pixels are all sampled independently from each $ARG2",
        "$ARG1 when the corresponding input is maximal ( z i = maxi zi ) and zi is much greater than all of the $ARG2",
        "$ARG1 and all $ARG2",
        "$ARG1 and ci = 0 for all $ARG2",
        "$ARG1 and all $ARG2",
        "$ARG1 , while all of the $ARG2",
        "$ARG1 \u2212 \ue00f , and otherwise any of the $ARG2"
    ],
    "boltzmann machine*****probability": [
        "$ARG1 that de\ufb01ne a $ARG2"
    ],
    "probability*****restricted boltzmann machine": [
        "$ARG1 density over $ARG2"
    ],
    "restricted boltzmann machine*****exponential family": [
        "$ARG1 may be developed for many $ARG2"
    ],
    "distribution*****gaussian distribution": [
        "$ARG1 over the visible units being a $ARG2",
        "$ARG1 over the observations given h ( m ) and h ( c ) as a multivariate $ARG2",
        "$ARG1 , such as a unit uniform or unit $ARG2",
        "$ARG1 , a typical choice is to use p ( hi ) = dh Typical choices of these non-Gaussian distributions have larger peaks near 0 than does the $ARG2",
        "$ARG1 with high entropy , such as a $ARG2"
    ],
    "gaussian distribution*****function": [
        "$ARG1 whose mean is a $ARG2"
    ],
    "covariance matrix*****precision": [
        "$ARG1 or a $ARG2",
        "$ARG1 , this is equivalent to ensuring that the $ARG2"
    ],
    "matrix*****gaussian distribution": [
        "$ARG1 for the $ARG2",
        "$ARG1 to be non-diagonal in this context , because some operations on the $ARG2"
    ],
    "energy function*****distribution": [
        "$ARG1 by expanding the unnormalized log conditional $ARG2",
        "$ARG1 de\ufb01nes a joint $ARG2",
        "$ARG1 speci\ufb01es a multivariate Gaussian , with a conditional $ARG2"
    ],
    "function*****random variable": [
        "$ARG1 only of the parameters and not the $ARG2",
        "$ARG1 of a $ARG2"
    ],
    "random variable*****model": [
        "$ARG1 in the $ARG2",
        "$ARG1 in the $ARG2"
    ],
    "distribution*****partition function": [
        "$ARG1 , and the $ARG2",
        "$ARG1 p0 ( x ) = Z10 p\u03030 ( x ) which supports tractable sampling and tractable evaluation of both the $ARG2",
        "$ARG1 p0 is close to p1 , equation 18.44 can be an e\ufb00ective way of estimating the $ARG2",
        "$ARG1 ( for which the $ARG2",
        "$ARG1 with known $ARG2",
        "$ARG1 p1 for which we are trying to estimate the $ARG2"
    ],
    "partition function*****energy function": [
        "$ARG1 of whatever $ARG2"
    ],
    "energy function*****other": [
        "$ARG1 and do not add any $ARG2"
    ],
    "other*****energy function": [
        "$ARG1 terms involving v , then our $ARG2"
    ],
    "model*****restricted boltzmann machine": [
        "$ARG1 instead of a $ARG2",
        "$ARG1 with exclusively binary variables , but today many models such as the mean-covariance $ARG2"
    ],
    "matrix*****1": [
        "$ARG1 , we \ufb01nd that for each hidden unit hi we have a term \u03b2j W j , i In the above , we used the fact that h2i = hi because hi \u2208 { 0 , $ARG2",
        "$ARG1 : \uf8ef $ARG2",
        "$ARG1 A has n linearly independent eigenvectors , { v ( $ARG2",
        "$ARG1 A with two orthonormal eigenvectors , v ( $ARG2"
    ],
    "energy function*****bias": [
        "$ARG1 , then it will naturally $ARG2"
    ],
    "bias*****weights": [
        "$ARG1 h i to be turned o\ufb00 when the $ARG2",
        "$ARG1 introduced by this heuristic oversampling can then be corrected using importance $ARG2"
    ],
    "weights*****precision": [
        "$ARG1 for that unit are large and connected to visible units with high $ARG2",
        "$ARG1 with rational numbers of unbounded $ARG2"
    ],
    "bias*****model": [
        "$ARG1 term does not a\ufb00ect the family of distributions the $ARG2",
        "$ARG1 is small , CD could be used as an inexpensive way to initialize a $ARG2"
    ],
    "bias parameter*****learning": [
        "$ARG1 for the hidden units ) but it does a\ufb00ect the $ARG2"
    ],
    "learning*****model": [
        "$ARG1 dynamics of the $ARG2",
        "$ARG1 performs well when the $ARG2",
        "$ARG1 a generative $ARG2",
        "$ARG1 a $ARG2",
        "$ARG1 relies not on changing the Monte Carlo sampling technology but rather on changing the parametrization of the $ARG2",
        "$ARG1 a probabilistic binary classi\ufb01er in which one of the categories corresponds to the data generated by the $ARG2",
        "$ARG1 a $ARG2",
        "$ARG1 the $ARG2",
        "$ARG1 the $ARG2",
        "$ARG1 which underlying causes are important and relevant enough to $ARG2",
        "$ARG1 a generative $ARG2",
        "$ARG1 whenever this happens , whether the $ARG2",
        "$ARG1 or using a trained $ARG2",
        "$ARG1 such a $ARG2",
        "$ARG1 a single , shared $ARG2",
        "$ARG1 this $ARG2",
        "$ARG1 in this $ARG2",
        "$ARG1 over the past 30 years have been obtained by changing the $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 techniques we can learn a $ARG2",
        "$ARG1 to \ufb01t a $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 would consider the $ARG2"
    ],
    "weights*****magnitude": [
        "$ARG1 rapidly increase in $ARG2"
    ],
    "energy function*****variance": [
        "$ARG1 on a Gaussian-Bernoulli RBM is thus E ( v , h ) = v ( \u03b2 \ue00c v ) \u2212 ( v \ue00c \u03b2 ) \ue03e W h \u2212 b\ue03eh but we may also add extra terms or parametrize the energy in terms of the $ARG2"
    ],
    "variance*****precision": [
        "$ARG1 rather than $ARG2",
        "$ARG1 , or $ARG2",
        "$ARG1 or $ARG2",
        "$ARG1 estimate p ( y | x ) = N ( y | wT x + b , 1/\u03b2 ) where \u03b2 is a $ARG2",
        "$ARG1 or $ARG2"
    ],
    "derivation*****bias": [
        "$ARG1 , we have not included a $ARG2"
    ],
    "constant*****precision": [
        "$ARG1 ( perhaps estimated based on the marginal $ARG2"
    ],
    "scalar*****identity matrix": [
        "$ARG1 times the $ARG2",
        "$ARG1 times the $ARG2"
    ],
    "identity matrix*****diagonal matrix": [
        "$ARG1 , or it may be a $ARG2"
    ],
    "gaussian distribution*****matrix": [
        "$ARG1 require inverting the $ARG2"
    ],
    "matrix*****diagonal matrix": [
        "$ARG1 , and a $ARG2",
        "$ARG1 D is de\ufb01ned to be a $ARG2"
    ],
    "other*****boltzmann machine": [
        "$ARG1 forms of $ARG2",
        "$ARG1 variants of $ARG2"
    ],
    "boltzmann machine*****covariance": [
        "$ARG1 permit modeling the $ARG2"
    ],
    "covariance*****undirected model": [
        "$ARG1 structure , using various techniques to avoid inverting the $ARG2"
    ],
    "undirected model*****covariance": [
        "$ARG1 of Conditional $ARG2"
    ],
    "covariance*****model": [
        "$ARG1 While the Gaussian RBM has been the canonical energy $ARG2",
        "$ARG1 RBM ( mcRBM1 ) , the mean-product of t-distribution ( mPoT ) $ARG2",
        "$ARG1 RBM ( Ranzato et al. , 2010a ) , also called a cRBM , whose components $ARG2",
        "$ARG1 units h ( c ) , the mcRBM $ARG2",
        "$ARG1 of the observation in a signi\ufb01cantly di\ufb00erent way.\ue010 The mcRBM and mPoT \ue011\u22121 both $ARG2",
        "$ARG1 is very similar to that given by a di\ufb00erent $ARG2"
    ],
    "bias*****natural image": [
        "$ARG1 is not well suited to the statistical variations present in some types of real-valued data , especially $ARG2"
    ],
    "problem*****information": [
        "$ARG1 is that much of the $ARG2",
        "$ARG1 : given some $ARG2",
        "$ARG1 is to introduce extra $ARG2"
    ],
    "content*****natural image": [
        "$ARG1 present in $ARG2"
    ],
    "natural image*****covariance": [
        "$ARG1 is embedded in the $ARG2"
    ],
    "other*****absolute value": [
        "$ARG1 words , it is the relationships between pixels and not their $ARG2"
    ],
    "absolute value*****information": [
        "$ARG1 where most of the useful $ARG2"
    ],
    "model*****ssrbm": [
        "$ARG1 and the spike and slab RBM ( $ARG2",
        "$ARG1 , the $ARG2"
    ],
    "hidden layer*****covariance": [
        "$ARG1 is divided into two groups of units : mean units and $ARG2"
    ],
    "other*****covariance": [
        "$ARG1 half is a $ARG2",
        "$ARG1 output units are used to parametrize the $ARG2"
    ],
    "model*****covariance": [
        "$ARG1 the conditional $ARG2",
        "$ARG1 the $ARG2",
        "$ARG1 the $ARG2"
    ],
    "energy function*****standard": [
        "$ARG1 : Emc ( x , h ( m ) , h ( c ) ) = Em ( x , h ( m ) ) + Ec ( x , h ( c ) ) , where Em is the $ARG2"
    ],
    "standard*****energy function": [
        "$ARG1 Gaussian-Bernoulli RBM $ARG2"
    ],
    "energy function*****covariance": [
        "$ARG1 that models the conditional $ARG2"
    ],
    "1*****covariance": [
        "$ARG1 \ue058 ( c ) \ue010 \ue03e ( j ) \ue0112 \ue058 ( c ) ( c ) E c ( x , h ( c ) ) = The parameter r ( j ) corresponds to the $ARG2",
        "$ARG1 \u2212 \u03b3j ) log h j where r ( j ) is the $ARG2"
    ],
    "covariance*****vector": [
        "$ARG1 weight $ARG2",
        "$ARG1 weight $ARG2"
    ],
    "vector*****covariance": [
        "$ARG1 of $ARG2"
    ],
    "gaussian distribution*****covariance matrix": [
        "$ARG1 : pmc ( x | h ( m ) , h ( c ) ) = N \uf8edx ; C xmc W : ,j h j \uf8f8 , Cxmc ( c ) ( j ) ( j ) \ue03e Note that the $ARG2",
        "$ARG1 , with $ARG2",
        "$ARG1 , whose $ARG2",
        "$ARG1 , the mixture of Gaussians might constrain the $ARG2"
    ],
    "covariance matrix*****matrix": [
        "$ARG1 and that W is the weight $ARG2"
    ],
    "matrix*****energy function": [
        "$ARG1 associated with the Gaussian RBM modeling the This version of the Gaussian-Bernoulli RBM $ARG2"
    ],
    "energy function*****image": [
        "$ARG1 assumes the $ARG2"
    ],
    "contrastive divergence*****persistent contrastive divergence": [
        "$ARG1 or $ARG2",
        "$ARG1 and $ARG2"
    ],
    "persistent contrastive divergence*****covariance": [
        "$ARG1 because of its non-diagonal conditional $ARG2"
    ],
    "distribution*****standard": [
        "$ARG1 of x , h ( m ) , h ( c ) which , in a $ARG2"
    ],
    "standard*****gibbs sampling": [
        "$ARG1 RBM , is accomplished by $ARG2"
    ],
    "distribution*****covariance": [
        "$ARG1 over the observation is a multivariate Gaussian ( with non-diagonal $ARG2",
        "$ARG1 over x that has non-diagonal $ARG2",
        "$ARG1 over the parameter \u03b8 , it is still often desirable to have a Unless there is a reason to assume a particular $ARG2"
    ],
    "covariance*****distribution": [
        "$ARG1 ) $ARG2",
        "$ARG1 is not a computationally e\ufb03cient way to parametrize the $ARG2"
    ],
    "distribution*****probability distribution": [
        "$ARG1 G ( k , \u03b8 ) is a $ARG2"
    ],
    "distribution*****model": [
        "$ARG1 to understand the basic ideas underlying the mPoT $ARG2",
        "$ARG1 q ( z | x ) and the $ARG2",
        "$ARG1 , we actually can not tell , unless we have some way of determining how loose the bound for $ARG2",
        "$ARG1 is one of the goals of a generative $ARG2",
        "$ARG1 with marginals matched to the $ARG2",
        "$ARG1 where the data occurs , and another pushing down on the $ARG2",
        "$ARG1 where the $ARG2",
        "$ARG1 , we can think of it as \ufb01nding points that the $ARG2",
        "$ARG1 and the $ARG2",
        "$ARG1 that is very close to the $ARG2",
        "$ARG1 is not close to the $ARG2",
        "$ARG1 will be very close to being fair samples from the current $ARG2",
        "$ARG1 with marginals matched to the $ARG2",
        "$ARG1 : pmodel ( x ) p $ARG2",
        "$ARG1 pmodel ( x , h ) , we often draw samples of x by alternating between sampling from pmodel ( x | h ) and sampling from p $ARG2",
        "$ARG1 and the $ARG2",
        "$ARG1 over the underlying causes changes or when we use the $ARG2",
        "$ARG1 and the $ARG2",
        "$ARG1 in the initial parameters of the $ARG2",
        "$ARG1 resembling the actual test inputs that will be supplied to the $ARG2",
        "$ARG1 pdata must lie within the $ARG2",
        "$ARG1 over the $ARG2",
        "$ARG1 over the $ARG2",
        "$ARG1 by using a \u201c complete graph. \u201d In the case of a directed $ARG2",
        "$ARG1 represented by the $ARG2",
        "$ARG1 p ( xi | P a G ( xi ) ) is easy to sample from , then the whole $ARG2",
        "$ARG1 even if the $ARG2",
        "$ARG1 pmodel ( x , h ) = p $ARG2",
        "$ARG1 over the latent variables , representing the $ARG2",
        "$ARG1 p ( \u03b8 ) encoding our beliefs about the $ARG2",
        "$ARG1 that the $ARG2",
        "$ARG1 over their domain not so much because they are particularly powerful but rather because their domain is simple ; they $ARG2",
        "$ARG1 that we sample from when we train a $ARG2"
    ],
    "restricted boltzmann machine*****covariance": [
        "$ARG1 ( Courville et al. , 2011 ) or ssRBMs provide another means of modeling the $ARG2"
    ],
    "ssrbm*****covariance": [
        "$ARG1 \u2019 s binary hidden units encode the conditional $ARG2",
        "$ARG1 parametrizes the conditional $ARG2",
        "$ARG1 speci\ufb01es the conditional $ARG2",
        "$ARG1 conditional $ARG2"
    ],
    "other*****1": [
        "$ARG1 words , each column W : ,i de\ufb01nes a component that can appear in the input when hi = $ARG2",
        "$ARG1 temperatures , particularly those where \u03b2 < $ARG2",
        "$ARG1 words , the nonlinear features have mapped both x = [ $ARG2",
        "$ARG1 words we wish to compute \u2202u for all i \u2208 { $ARG2"
    ],
    "variable*****variance": [
        "$ARG1 adds $ARG2",
        "$ARG1 prior is just the unit $ARG2"
    ],
    "persistent contrastive divergence*****gibbs sampling": [
        "$ARG1 with $ARG2"
    ],
    "energy function*****diagonal": [
        "$ARG1 : E ss ( x , s , h ) = \u2212 x\ue03e W : ,i s ihi + x\ue03e \u039b + \u03b1i \u00b5i s ih i \u2212 \u03b1i \u00b52i hi , where bi is the o\ufb00set of the spike hi and \u039b is a $ARG2"
    ],
    "precision*****variable": [
        "$ARG1 parameter for the real-valued slab $ARG2"
    ],
    "distribution*****energy function": [
        "$ARG1 de\ufb01ned via the $ARG2"
    ],
    "energy function*****ssrbm": [
        "$ARG1 , it is relatively straightforward to derive the $ARG2"
    ],
    "example*****distribution": [
        "$ARG1 , by marginalizing out the slab variables s , the conditional $ARG2",
        "$ARG1 , it has been successfully applied to modeling the conditional $ARG2",
        "$ARG1 , it is di\ufb03cult to predict missing The conditional $ARG2",
        "$ARG1 by sampling their values from some $ARG2",
        "$ARG1 , if the output is a $ARG2",
        "$ARG1 and maximizing a training objective that captures their joint $ARG2",
        "$ARG1 , after observing m examples , the predicted $ARG2",
        "$ARG1 for estimating the autoencoder reconstruction $ARG2"
    ],
    "covariance matrix*****positive de\ufb01nite": [
        "$ARG1 Cxss|h is $ARG2",
        "$ARG1 that is not $ARG2",
        "$ARG1 of the Gaussian is $ARG2"
    ],
    "sparse coding*****model": [
        "$ARG1 , where samples from the $ARG2",
        "$ARG1 ( Olshausen and Field , 1996 ) is a linear factor $ARG2",
        "$ARG1 is a linear factor $ARG2",
        "$ARG1 generative $ARG2",
        "$ARG1 probabilistic $ARG2"
    ],
    "model*****measure": [
        "$ARG1 \u201c almost never \u201d ( in the $ARG2",
        "$ARG1 capacity increases ( assuming the error $ARG2"
    ],
    "measure*****inference": [
        "$ARG1 theoretic sense ) contain zeros in the code , and MAP $ARG2"
    ],
    "contrast*****ssrbm": [
        "$ARG1 , the $ARG2"
    ],
    "covariance*****1": [
        "$ARG1 of the observations using the hidden spike activations hi = $ARG2"
    ],
    "1*****precision": [
        "$ARG1 to pinch the $ARG2"
    ],
    "matrix*****vector": [
        "$ARG1 along the direction speci\ufb01ed by the corresponding weight $ARG2",
        "$ARG1 A acting on a $ARG2",
        "$ARG1 by the $ARG2",
        "$ARG1 and a $ARG2",
        "$ARG1 , b \u2208 R m is a known $ARG2",
        "$ARG1 V of eigenvectors and a $ARG2"
    ],
    "model*****principal components analysis": [
        "$ARG1 : the product of probabilistic $ARG2"
    ],
    "ssrbm*****variance": [
        "$ARG1 parametrization permit signi\ufb01cant $ARG2"
    ],
    "restricted boltzmann machine*****covariance matrix": [
        "$ARG1 is that some settings of the parameters can correspond to a $ARG2"
    ],
    "covariance matrix*****probability": [
        "$ARG1 places more unnormalized $ARG2"
    ],
    "probability*****integral": [
        "$ARG1 on values that are farther from the mean , causing the $ARG2",
        "$ARG1 density ( for the $ARG2"
    ],
    "constrained optimization*****probability": [
        "$ARG1 to explicitly avoid the regions where the $ARG2",
        "$ARG1 to arbitrarily impose some speci\ufb01c minimal $ARG2"
    ],
    "probability*****model": [
        "$ARG1 is unde\ufb01ned is di\ufb03cult to do without being overly conservative and also preventing the $ARG2",
        "$ARG1 of the data under the $ARG2",
        "$ARG1 of those points , they are generally considered to represent the $ARG2",
        "$ARG1 under the $ARG2",
        "$ARG1 mass on these modes , the $ARG2",
        "$ARG1 that the $ARG2",
        "$ARG1 under the $ARG2",
        "$ARG1 according to the $ARG2",
        "$ARG1 under the $ARG2",
        "$ARG1 assigned to the most likely class thus gives an estimate of the con\ufb01dence the $ARG2"
    ],
    "ssrbm*****natural image": [
        "$ARG1 produce excellent samples of $ARG2"
    ],
    "partition function*****sparse coding": [
        "$ARG1 from becoming unde\ufb01ned results in a $ARG2"
    ],
    "model*****sparse coding": [
        "$ARG1 , spike and slab $ARG2",
        "$ARG1 , but the term \u201c $ARG2",
        "$ARG1 family , the spike and slab $ARG2",
        "$ARG1 that is a hybrid of $ARG2",
        "$ARG1 and then training f ( x ) to predict the values of the $ARG2"
    ],
    "memory*****machine learning": [
        "$ARG1 and statistical requirements of $ARG2"
    ],
    "matrix multiplication*****convolution": [
        "$ARG1 by discrete $ARG2",
        "$ARG1 or batch $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 ; the same $ARG2"
    ],
    "convolution*****kernel": [
        "$ARG1 with a small $ARG2",
        "$ARG1 arises because we have \ufb02ipped the $ARG2",
        "$ARG1 but without \ufb02ipping the $ARG2",
        "$ARG1 , and specify whether we mean to \ufb02ip the $ARG2",
        "$ARG1 with $ARG2",
        "$ARG1 ( without $ARG2",
        "$ARG1 with a $ARG2",
        "$ARG1 with a $ARG2",
        "$ARG1 with a single $ARG2",
        "$ARG1 with a small $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 is straightforward to apply ; the $ARG2",
        "$ARG1 is equivalent to converting both the input and the $ARG2"
    ],
    "kernel*****standard": [
        "$ARG1 is the $ARG2"
    ],
    "standard*****translation": [
        "$ARG1 way of solving these problems for inputs that have $ARG2"
    ],
    "convolutional network*****pooling": [
        "$ARG1 usually require a $ARG2",
        "$ARG1 often use a $ARG2",
        "$ARG1 Some theoretical work gives guidance as to which kinds of $ARG2",
        "$ARG1 architectures ( Szegedy et al. , 2014a ) are designed to use $ARG2"
    ],
    "pooling*****energy function": [
        "$ARG1 unit p over n binary detector units d and enforce p = maxi d i by setting the $ARG2",
        "$ARG1 region this requires 29 = 512 $ARG2"
    ],
    "energy function*****constraint": [
        "$ARG1 to be \u221e whenever that $ARG2",
        "$ARG1 can be thought of as an \u201c expert \u201d that determines whether a particular soft $ARG2"
    ],
    "energy function*****pooling": [
        "$ARG1 evaluations per $ARG2"
    ],
    "solution*****problem": [
        "$ARG1 to this $ARG2",
        "$ARG1 to the XOR $ARG2",
        "$ARG1 we described to the XOR $ARG2",
        "$ARG1 to this $ARG2",
        "$ARG1 to the full $ARG2",
        "$ARG1 to the true underlying $ARG2",
        "$ARG1 to this $ARG2",
        "$ARG1 to the original $ARG2",
        "$ARG1 to the $ARG2",
        "$ARG1 to the unconstrained least squares $ARG2",
        "$ARG1 to the constrained $ARG2",
        "$ARG1 to the constrained $ARG2",
        "$ARG1 to the noise robustness $ARG2"
    ],
    "problem*****probabilistic max pooling": [
        "$ARG1 called $ARG2"
    ],
    "probabilistic max pooling*****stochastic pooling": [
        "$ARG1 ( not to be confused with \u201c $ARG2"
    ],
    "stochastic pooling*****technique": [
        "$ARG1 , \u201d which is a $ARG2"
    ],
    "model*****variable": [
        "$ARG1 with a single $ARG2",
        "$ARG1 must use $ARG2",
        "$ARG1 emits a discrete $ARG2",
        "$ARG1 over both x and a new , binary class $ARG2",
        "$ARG1 to choose between one of n di\ufb00erent options for some internal $ARG2",
        "$ARG1 representing a $ARG2",
        "$ARG1 that can accommodate $ARG2",
        "$ARG1 learns a latent $ARG2",
        "$ARG1 , with latent variables h \u2208 R2 and just one visible $ARG2"
    ],
    "variable*****1": [
        "$ARG1 that has n + $ARG2",
        "$ARG1 from the i \u2212 $ARG2",
        "$ARG1 xi from the i \u2212 $ARG2",
        "$ARG1 y with y = $ARG2",
        "$ARG1 is $ARG2",
        "$ARG1 in time step t + $ARG2",
        "$ARG1 is slightly more complicated , because its mean must always be between 0 and $ARG2",
        "$ARG1 , v. Suppose that p ( h ) = N ( h ; 0 , I ) and p ( v | h ) = N ( v ; w \ue03e h ; $ARG2",
        "$ARG1 : P ( x ( $ARG2"
    ],
    "model*****1": [
        "$ARG1 that has n + $ARG2",
        "$ARG1 ( x , z ( i ) ) L k ( x , q ) = Ez ( $ARG2",
        "$ARG1 , we specify that pjoint ( y = $ARG2",
        "$ARG1 ( x ) + p noise ( x ) pjoint ( y = $ARG2",
        "$ARG1 is trained to be evaluated at \u03b2 = $ARG2",
        "$ARG1 being f ( x ; W , c , w , b ) = f ( 2 ) ( f ( $ARG2",
        "$ARG1 already has the right answer\u2014when y = $ARG2",
        "$ARG1 Require : b ( i ) , i \u2208 { $ARG2",
        "$ARG1 of visual processing CHAPTER $ARG2",
        "$ARG1 receives the ground truth output y ( t ) as input at time t + $ARG2",
        "$ARG1 includes connections from the output at one time step to the next log p ( y ( t ) | x ( $ARG2",
        "$ARG1 with SGD is O ( $ARG2",
        "$ARG1 ( y | x ) = N ( y ; x\ue03e w + b , $ARG2",
        "$ARG1 family being trained either ( $ARG2",
        "$ARG1 can generally be divided into two kinds of parts and associated $ARG2"
    ],
    "probabilistic max pooling*****constraint": [
        "$ARG1 does force the detector units to be mutually exclusive , which may be a useful regularizing $ARG2"
    ],
    "constraint*****model": [
        "$ARG1 in some contexts or a harmful limit on $ARG2"
    ],
    "model*****other": [
        "$ARG1 capacity in $ARG2",
        "$ARG1 that can map from some input x to some output y , and the di\ufb00erent entries of y are related to each $ARG2",
        "$ARG1 to use , the position of the chair , and $ARG2",
        "$ARG1 , making the conditional variances \u03c32i equal to each $ARG2",
        "$ARG1 relationships between sequences and $ARG2",
        "$ARG1 operate at \ufb01ne-grained time scales and can handle small details , while $ARG2",
        "$ARG1 only on the original training points , but train it to copy $ARG2",
        "$ARG1 must be able to share knowledge between one word and $ARG2",
        "$ARG1 ) are close to each $ARG2",
        "$ARG1 is to ask questions about how variables are related to each $ARG2",
        "$ARG1 , we might want to extract features E [ h | v ] describing the observed variables v. Sometimes we need to solve such problems in order to perform $ARG2",
        "$ARG1 to have $ARG2",
        "$ARG1 parameters should be close to each $ARG2",
        "$ARG1 in a more complicated way than $ARG2",
        "$ARG1 parameters to become larger Dropping units less often gives the units more opportunities to \u201c conspire \u201d with each $ARG2"
    ],
    "support*****pooling": [
        "$ARG1 overlapping $ARG2"
    ],
    "pooling*****convolutional network": [
        "$ARG1 regions are usually required to obtain the best performance from feedforward $ARG2",
        "$ARG1 units of $ARG2",
        "$ARG1 strategies in $ARG2",
        "$ARG1 ( see section 9.3 ) for building ensembles of $ARG2"
    ],
    "convolutional network*****constraint": [
        "$ARG1 , so this $ARG2"
    ],
    "probabilistic max pooling*****deep boltzmann machine": [
        "$ARG1 could be used to build convolutional $ARG2"
    ],
    "model*****convolutional network": [
        "$ARG1 is challenging to make work in practice , and usually does not perform as well as a classi\ufb01er as traditional $ARG2",
        "$ARG1 that reads the input to produce C may be an RNN ( Cho et al. , 2014a ; Sutskever et al. , 2014 ; Jean et al. , 2014 ) or a $ARG2",
        "$ARG1 architecture design elements of the modern $ARG2"
    ],
    "convolutional network*****supervised learning": [
        "$ARG1 trained with $ARG2"
    ],
    "boltzmann machine*****change": [
        "$ARG1 , it is di\ufb03cult to $ARG2"
    ],
    "convolutional network*****invariance": [
        "$ARG1 achieve size $ARG2"
    ],
    "invariance*****pooling": [
        "$ARG1 by scaling up the size of their $ARG2"
    ],
    "pooling*****boltzmann machine": [
        "$ARG1 regions proportional to the size of the input , but scaling $ARG2",
        "$ARG1 in convolutional $ARG2"
    ],
    "convolutional neural network*****number": [
        "$ARG1 can use a \ufb01xed $ARG2"
    ],
    "number*****pooling": [
        "$ARG1 of $ARG2",
        "$ARG1 of features provided as input to this layer after passing through several layers of $ARG2"
    ],
    "boltzmann machine*****pooling": [
        "$ARG1 , large $ARG2"
    ],
    "example*****model": [
        "$ARG1 , suppose we learn a $ARG2",
        "$ARG1 rather than a fake sample drawn from the $ARG2",
        "$ARG1 , suppose we use AIS to estimate log Z in order to compute log p\u0303 ( x ) \u2212 log Z for a new $ARG2",
        "$ARG1 of a $ARG2",
        "$ARG1 , a $ARG2",
        "$ARG1 , imagine we have two models : $ARG2",
        "$ARG1 , if we train a generative $ARG2",
        "$ARG1 , the predictive generative network has been trained to predict the appearance of a 3-D $ARG2",
        "$ARG1 of a generative $ARG2",
        "$ARG1 of such a $ARG2",
        "$ARG1 , we see that at time t = 2 , the $ARG2",
        "$ARG1 : \u03b8ML = arg max log p $ARG2",
        "$ARG1 into a more canonical form in order to reduce the amount of variation that the $ARG2",
        "$ARG1 , if the word dog and the word cat map to representations that share many attributes , then sentences that contain the word cat can inform the predictions that will be made by the $ARG2",
        "$ARG1 of a deep probabilistic $ARG2",
        "$ARG1 inputs that we will not use for training , only for evaluating how well the $ARG2",
        "$ARG1 , we trained the $ARG2",
        "$ARG1 , the quadratic $ARG2",
        "$ARG1 of how we can control a $ARG2",
        "$ARG1 of such a situation , suppose we want to $ARG2",
        "$ARG1 , suppose we want to $ARG2",
        "$ARG1 , consider a $ARG2",
        "$ARG1 , the directed $ARG2",
        "$ARG1 , as discussed in section 3.9.6 , the mixture of Gaussians $ARG2",
        "$ARG1 , consider a very simple probabilistic $ARG2",
        "$ARG1 , in a directed $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 , if the $ARG2",
        "$ARG1 with high \ufb01delity , or a generative $ARG2"
    ],
    "model*****probabilistic max pooling": [
        "$ARG1 with 2 \u00d7 2 $ARG2"
    ],
    "image*****number": [
        "$ARG1 by 50 % in each direction , we would expect the $ARG2",
        "$ARG1 implicitly speci\ufb01es that an underlying cause is only salient if it signi\ufb01cantly changes the brightness of a large $ARG2"
    ],
    "pooling*****constraint": [
        "$ARG1 regions by 50 % in each direction to 3 \u00d7 3 , then the mutual exclusivity $ARG2"
    ],
    "model*****image": [
        "$ARG1 \u2019 s input $ARG2",
        "$ARG1 consists of sequentially visiting di\ufb00erent small $ARG2",
        "$ARG1 p ( x | z ) , simply by changing the 2-D \u201c code \u201d z ( each $ARG2",
        "$ARG1 of images of cars and motorcycles , it will need to know about wheels , and about how many wheels should be in an $ARG2",
        "$ARG1 that then produces a sequence of words describing the $ARG2",
        "$ARG1 to correct for di\ufb00erences in the $ARG2",
        "$ARG1 to label every pixel in an $ARG2",
        "$ARG1 displayed superimposed on the $ARG2"
    ],
    "image*****model": [
        "$ARG1 in this way , the $ARG2",
        "$ARG1 are not modeled The publication describes the $ARG2",
        "$ARG1 patch , or 5-50 for a more complicated $ARG2",
        "$ARG1 produced by a $ARG2",
        "$ARG1 is used as input to a $ARG2",
        "$ARG1 cropped at slightly di\ufb00erent locations ) and have the di\ufb00erent instantiations of the $ARG2"
    ],
    "variable*****pooling": [
        "$ARG1 amounts of $ARG2"
    ],
    "pooling*****vector": [
        "$ARG1 in order to emit a \ufb01xed-size output $ARG2"
    ],
    "probabilistic max pooling*****model": [
        "$ARG1 may still accept variable-sized input images so long as the output of the $ARG2"
    ],
    "model*****feature": [
        "$ARG1 is a $ARG2",
        "$ARG1 itself is not especially sparse , only the $ARG2",
        "$ARG1 that learns a di\ufb00erent $ARG2",
        "$ARG1 of v and h , experimental observations show that E [ h | v ] or argmax h p ( h , v ) is a good $ARG2",
        "$ARG1 may be better for $ARG2",
        "$ARG1 must learn another h i , either that redundantly encodes the presence of a nose , or that detects the face by another $ARG2"
    ],
    "feature*****image": [
        "$ARG1 map that can scale in size proportional to the input $ARG2",
        "$ARG1 indicating whether a zebra is in the $ARG2",
        "$ARG1 that detects elephant trunks when representing an $ARG2"
    ],
    "boundary*****image": [
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "image*****boltzmann machine": [
        "$ARG1 also pose some di\ufb03culty , which is exacerbated by the fact that connections in a $ARG2"
    ],
    "undirected model*****mean \ufb01eld": [
        "$ARG1 with tractable layer-wise $ARG2"
    ],
    "mean \ufb01eld*****deep boltzmann machine": [
        "$ARG1 \ufb01xed point updates , it best \ufb01ts the de\ufb01nition of a $ARG2"
    ],
    "boltzmann machine*****structured output": [
        "$ARG1 for Structured or Sequential In the $ARG2",
        "$ARG1 can be used not just for $ARG2"
    ],
    "structured output*****model": [
        "$ARG1 scenario , we wish to train a $ARG2"
    ],
    "example*****speech": [
        "$ARG1 , in the $ARG2",
        "$ARG1 , in a $ARG2"
    ],
    "mapping*****model": [
        "$ARG1 an input x to an output y , the $ARG2",
        "$ARG1 from v to h depends on the choice of $ARG2"
    ],
    "model*****probability distribution": [
        "$ARG1 must estimate a $ARG2",
        "$ARG1 is de\ufb01ned to be the $ARG2",
        "$ARG1 ( x ) thus may not correspond exactly to a valid $ARG2",
        "$ARG1 MA de\ufb01ning a $ARG2",
        "$ARG1 MB de\ufb01ning a $ARG2",
        "$ARG1 \u2019 s predictions as the cost Sometimes , we take a simpler approach , where rather than predicting a complete $ARG2",
        "$ARG1 can then represent arbitrary $ARG2",
        "$ARG1 de\ufb01nes a $ARG2",
        "$ARG1 uses products of these conditional distributions to de\ufb01ne the $ARG2",
        "$ARG1 is an oracle that simply knows the true $ARG2",
        "$ARG1 is asked to return estimates of or a $ARG2",
        "$ARG1 \u2019 s role is to output a $ARG2",
        "$ARG1 i produces a $ARG2"
    ],
    "boltzmann machine*****1": [
        "$ARG1 can represent factors of the form p ( x ( t ) | x ( $ARG2"
    ],
    "task*****video game": [
        "$ARG1 for the $ARG2"
    ],
    "model*****character": [
        "$ARG1 of a $ARG2",
        "$ARG1 is designed , a token may be a word , a $ARG2"
    ],
    "model*****bias parameter": [
        "$ARG1 is an RBM over p ( x ( t ) ) whose $ARG2",
        "$ARG1 and turning them into \u03c9 , where the $ARG2",
        "$ARG1 , with no $ARG2"
    ],
    "bias parameter*****function": [
        "$ARG1 are a linear $ARG2",
        "$ARG1 within \u03c9 are now a $ARG2"
    ],
    "weights*****change": [
        "$ARG1 in the RBM over x never $ARG2",
        "$ARG1 are free to $ARG2",
        "$ARG1 that may $ARG2",
        "$ARG1 w can $ARG2"
    ],
    "change*****probability": [
        "$ARG1 the $ARG2"
    ],
    "other*****conditional rbm": [
        "$ARG1 variants of $ARG2",
        "$ARG1 variants of sequence modeling using $ARG2"
    ],
    "conditional rbm*****other": [
        "$ARG1 ( Mnih et al. , 2011 ) and $ARG2"
    ],
    "task*****model": [
        "$ARG1 is to $ARG2",
        "$ARG1 of classifying between training samples and generated samples ( with the $ARG2",
        "$ARG1 we wish the $ARG2",
        "$ARG1 can be too ambitious if the $ARG2",
        "$ARG1 , then make the $ARG2",
        "$ARG1 where we use our $ARG2",
        "$ARG1 of drawing samples from a $ARG2",
        "$ARG1 with a sparsity penalty can yield a $ARG2",
        "$ARG1 that has been studied extensively , you will probably do well by \ufb01rst copying the $ARG2"
    ],
    "model*****task": [
        "$ARG1 and applied it to this $ARG2",
        "$ARG1 is preferable based on a criterion speci\ufb01c to the practical $ARG2",
        "$ARG1 that accomplished this $ARG2",
        "$ARG1 for a new $ARG2",
        "$ARG1 ( the \u201c $ARG2",
        "$ARG1 to solve a speci\ufb01c $ARG2",
        "$ARG1 is complex and hard to optimize or if the $ARG2",
        "$ARG1 to solve the $ARG2",
        "$ARG1 to solve a simpler $ARG2",
        "$ARG1 to perform the desired $ARG2",
        "$ARG1 applied to such a $ARG2",
        "$ARG1 is perfectly matched to the true structure of the $ARG2",
        "$ARG1 does not have enough capacity to solve the $ARG2",
        "$ARG1 from that $ARG2",
        "$ARG1 to match the complexity of the $ARG2",
        "$ARG1 performing its $ARG2"
    ],
    "bias parameter*****weights": [
        "$ARG1 of the RBM varied from one time step to the next , the RNN-RBM uses the RNN to emit all of the parameters of the RBM , including the $ARG2"
    ],
    "gradient*****loss function": [
        "$ARG1 of the $ARG2",
        "$ARG1 of this $ARG2",
        "$ARG1 which direction its output should move to reduce the $ARG2"
    ],
    "loss*****contrastive divergence": [
        "$ARG1 with respect to the RBM parameters using $ARG2"
    ],
    "contrastive divergence*****algorithm": [
        "$ARG1 or a related $ARG2",
        "$ARG1 ( CD , or CD-k to indicate CD with k Gibbs steps ) $ARG2",
        "$ARG1 ( $ARG2"
    ],
    "gradient*****back-propagation through time": [
        "$ARG1 may then be back-propagated through the RNN using the usual $ARG2"
    ],
    "boltzmann machine*****energy function": [
        "$ARG1 used in practice have only second-order interactions in their $ARG2",
        "$ARG1 ( Sejnowski , 1987 ) whose $ARG2"
    ],
    "energy function*****random variable": [
        "$ARG1 are the sum of many terms and each individual term only includes the product between two $ARG2"
    ],
    "model*****video": [
        "$ARG1 spatial transformations from one frame of $ARG2"
    ],
    "multiplication*****variable": [
        "$ARG1 by a one-hot class $ARG2"
    ],
    "variable*****change": [
        "$ARG1 can $ARG2"
    ],
    "example*****boltzmann machine": [
        "$ARG1 of the use of higher-order interactions is a $ARG2"
    ],
    "boltzmann machine*****group": [
        "$ARG1 with two groups of hidden units , with one $ARG2"
    ],
    "model*****example": [
        "$ARG1 the input using features that are relevant to the class but also to learn extra hidden units that explain nuisance details that are necessary for the samples of v to be realistic but do not determine the class of the $ARG2",
        "$ARG1 in practice , for $ARG2",
        "$ARG1 , monitoring training performance , and comparing models to For $ARG2",
        "$ARG1 in a location that would otherwise be inaccessible\u2014for $ARG2",
        "$ARG1 on every $ARG2",
        "$ARG1 is able to correctly classify every $ARG2",
        "$ARG1 to be evaluated on a single $ARG2",
        "$ARG1 generalizes better if it is large ( for $ARG2",
        "$ARG1 many di\ufb00erent versions of the same input ( for $ARG2",
        "$ARG1 a continuous-valued score for each $ARG2",
        "$ARG1 will eventually converge to its best possible test error before SGD has sampled every $ARG2",
        "$ARG1 makes an error \ue00fi on each $ARG2",
        "$ARG1 parameters or the input $ARG2",
        "$ARG1 that is regularized well , for $ARG2",
        "$ARG1 to consistently emit samples resembling a single $ARG2"
    ],
    "boltzmann machine*****neural network": [
        "$ARG1 requires some more care and creativity than developing a new $ARG2"
    ],
    "neural network*****energy function": [
        "$ARG1 layer , because it is often di\ufb03cult to \ufb01nd an $ARG2"
    ],
    "back-propagation*****neural network": [
        "$ARG1 through Random Operations Traditional $ARG2",
        "$ARG1 is often misunderstood as being speci\ufb01c to multilayer $ARG2",
        "$ARG1 and initiated a very active period of research in multi-layer $ARG2",
        "$ARG1 , $ARG2",
        "$ARG1 ( Rumelhart et al. , 1986a ) to train a $ARG2",
        "$ARG1 to train deep $ARG2"
    ],
    "neural network*****stochastic": [
        "$ARG1 to implement $ARG2"
    ],
    "neural network*****probability distribution": [
        "$ARG1 with extra inputs z that are sampled from some simple $ARG2"
    ],
    "probability distribution*****gaussian distribution": [
        "$ARG1 , such as a uniform or $ARG2"
    ],
    "neural network*****function": [
        "$ARG1 can then continue to perform deterministic computation internally , but the $ARG2",
        "$ARG1 in $ARG2",
        "$ARG1 training is non-deterministic , and converges to a di\ufb00erent $ARG2",
        "$ARG1 as being able to represent any $ARG2",
        "$ARG1 as representing a $ARG2",
        "$ARG1 architectures arrange these layers in a chain structure , with each layer being a $ARG2",
        "$ARG1 may also approximate any $ARG2",
        "$ARG1 training , we usually do not care about \ufb01nding the exact minimum of a $ARG2",
        "$ARG1 libraries implement a related $ARG2",
        "$ARG1 that train with several \u201c dead units. \u201d These are units that do not contribute much to the behavior of the $ARG2"
    ],
    "function*****stochastic": [
        "$ARG1 f ( x , z ) will appear $ARG2"
    ],
    "example*****operation": [
        "$ARG1 , let us consider the $ARG2",
        "$ARG1 , multiplying two matrices that each contain millions of entries might correspond to a single $ARG2",
        "$ARG1 , out-of-plane rotation can not be implemented as a simple geometric $ARG2"
    ],
    "operation*****gaussian distribution": [
        "$ARG1 consisting of drawing samples y from a $ARG2"
    ],
    "gaussian distribution*****variance": [
        "$ARG1 with mean \u00b5 and $ARG2",
        "$ARG1 with high $ARG2",
        "$ARG1 with low $ARG2"
    ],
    "function*****process": [
        "$ARG1 , but rather by a sampling $ARG2"
    ],
    "distribution*****operation": [
        "$ARG1 : y = \u00b5 + \u03c3z We are now able to back-propagate through the sampling $ARG2"
    ],
    "operation*****random variable": [
        "$ARG1 with an extra input z. Crucially , the extra input is a $ARG2",
        "$ARG1 require only as many evaluations of p\u0303 as there are values of a single $ARG2"
    ],
    "random variable*****distribution": [
        "$ARG1 whose $ARG2",
        "$ARG1 xi in the $ARG2"
    ],
    "distribution*****function": [
        "$ARG1 is not a $ARG2",
        "$ARG1 p\u0303 must be a Lebesgue-integrable $ARG2",
        "$ARG1 P ( y | x ) by using the same P ( y | \u03c9 ) as before , but making \u03c9 a $ARG2",
        "$ARG1 over a high-dimensional space is computationally cheaper and unlikely to assign any units to compute the same $ARG2",
        "$ARG1 corresponds to a single speci\ufb01c point in $ARG2"
    ],
    "change*****\u03c3": [
        "$ARG1 in \u00b5 or $ARG2"
    ],
    "\u03c3*****change": [
        "$ARG1 would $ARG2"
    ],
    "change*****operation": [
        "$ARG1 the output if we could repeat the sampling $ARG2"
    ],
    "operation*****graph": [
        "$ARG1 allows us to incorporate it into a larger $ARG2",
        "$ARG1 in the $ARG2",
        "$ARG1 is responsible for knowing how to back-propagate through the edges in the $ARG2",
        "$ARG1 in the $ARG2"
    ],
    "graph*****distribution": [
        "$ARG1 on top of the output of the sampling $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 with all possible edges ) to represent any $ARG2",
        "$ARG1 allows us to quickly see some properties of the $ARG2",
        "$ARG1 allows us to quickly see some properties of the $ARG2"
    ],
    "example*****loss function": [
        "$ARG1 , we can compute the derivatives of some $ARG2",
        "$ARG1 , the $ARG2"
    ],
    "graph*****operation": [
        "$ARG1 whose outputs are the inputs or the parameters of the sampling $ARG2",
        "$ARG1 using the \u00d7 $ARG2",
        "$ARG1 that applies more than one $ARG2",
        "$ARG1 language includes a relu $ARG2",
        "$ARG1 language includes a cross_entropy $ARG2"
    ],
    "example*****graph": [
        "$ARG1 , we could build a larger $ARG2",
        "$ARG1 , we begin with a $ARG2",
        "$ARG1 , we might have a $ARG2",
        "$ARG1 , this means that , using the $ARG2",
        "$ARG1 of reading separation from an undirected $ARG2",
        "$ARG1 of reading separation properties from an undirected $ARG2",
        "$ARG1 of a directed $ARG2",
        "$ARG1 of an undirected $ARG2"
    ],
    "graph*****\u03c3": [
        "$ARG1 with \u00b5 = f ( x ; \u03b8 ) and $ARG2"
    ],
    "graph*****back-propagation": [
        "$ARG1 , we can use $ARG2",
        "$ARG1 to those added for the $ARG2",
        "$ARG1 proposed by the pure $ARG2"
    ],
    "principle*****example": [
        "$ARG1 used in this Gaussian sampling $ARG2"
    ],
    "probability distribution*****variable": [
        "$ARG1 of the form p ( y ; \u03b8 ) or p ( y | x ; \u03b8 ) as p ( y | \u03c9 ) , where \u03c9 is a $ARG2",
        "$ARG1 over a discrete $ARG2",
        "$ARG1 over a binary $ARG2",
        "$ARG1 over a discrete $ARG2"
    ],
    "distribution*****turn": [
        "$ARG1 p ( y | \u03c9 ) , where \u03c9 may in $ARG2"
    ],
    "turn*****function": [
        "$ARG1 be a $ARG2",
        "$ARG1 to a $ARG2"
    ],
    "function*****other": [
        "$ARG1 of $ARG2",
        "$ARG1 v ( a , b ) = ab , where one player controls a and incurs cost ab , while the $ARG2",
        "$ARG1 of x as one of the $ARG2",
        "$ARG1 from all of the $ARG2",
        "$ARG1 from each $ARG2",
        "$ARG1 as each $ARG2",
        "$ARG1 , but it is also capable of representing in\ufb01nitely many $ARG2",
        "$ARG1 that involves $ARG2",
        "$ARG1 and the $ARG2"
    ],
    "algorithm*****almost everywhere": [
        "$ARG1 applied to f , so long as f is continuous and di\ufb00erentiable $ARG2"
    ],
    "technique*****reparametrization trick": [
        "$ARG1 is often called the $ARG2"
    ],
    "reparametrization trick*****stochastic back-propagation": [
        "$ARG1 , $ARG2"
    ],
    "process*****gradient": [
        "$ARG1 that produces discrete-valued samples , it may still be possible to estimate a $ARG2",
        "$ARG1 , rather than restarted at each $ARG2"
    ],
    "gradient*****reinforcement learning": [
        "$ARG1 on \u03c9 , using $ARG2"
    ],
    "reinforcement learning*****algorithm": [
        "$ARG1 algorithms such as variants of the REINFORCE $ARG2"
    ],
    "neural network*****distribution": [
        "$ARG1 applications , we typically choose z to be drawn from some simple $ARG2",
        "$ARG1 predict parameters of the conditional $ARG2",
        "$ARG1 is able to learn nonlinear mappings from the input to the parameters of the output $ARG2"
    ],
    "stochastic*****machine learning": [
        "$ARG1 operations dates back to the mid-twentieth century ( Price , 1958 ; Bonnet , 1964 ) and was \ufb01rst used for $ARG2"
    ],
    "machine learning*****reinforcement learning": [
        "$ARG1 in the context of $ARG2"
    ],
    "stochastic*****neural network": [
        "$ARG1 or generative $ARG2"
    ],
    "denoising autoencoder*****dropout": [
        "$ARG1 or networks regularized with $ARG2"
    ],
    "dropout*****model": [
        "$ARG1 , are also naturally designed to take noise as an input without requiring any special reparametrization to make the noise independent from the $ARG2",
        "$ARG1 , the models share parameters , with each $ARG2",
        "$ARG1 , typically most models are not explicitly trained at all\u2014usually , the $ARG2",
        "$ARG1 has not required that the $ARG2",
        "$ARG1 is that we can approximate pensemble by evaluating p ( y | x ) in one $ARG2",
        "$ARG1 is that it does not signi\ufb01cantly limit the type of $ARG2",
        "$ARG1 to a speci\ufb01c $ARG2",
        "$ARG1 , but this comes at the cost of a much larger $ARG2"
    ],
    "stochastic*****model": [
        "$ARG1 Operations When a $ARG2",
        "$ARG1 estimate of the log-likelihood for $ARG2"
    ],
    "variable*****reparametrization trick": [
        "$ARG1 y , the $ARG2"
    ],
    "boundary*****problem": [
        "$ARG1 , the derivatives are unde\ufb01ned , but that is a small $ARG2"
    ],
    "problem*****almost everywhere": [
        "$ARG1 is that the derivatives are zero $ARG2"
    ],
    "cost function*****information": [
        "$ARG1 J ( y ) therefore do not give any $ARG2"
    ],
    "information*****model": [
        "$ARG1 for how to update the $ARG2",
        "$ARG1 results in uncertainty in the $ARG2"
    ],
    "function*****expected": [
        "$ARG1 with useless derivatives , the $ARG2",
        "$ARG1 f : Rn \u2192 R of this type , the $ARG2",
        "$ARG1 decreases slower than $ARG2",
        "$ARG1 , the $ARG2"
    ],
    "expected*****function": [
        "$ARG1 cost Ez\u223cp ( z ) J ( f ( z ; \u03c9 ) ) is often a smooth $ARG2",
        "$ARG1 deviation from the true value of the $ARG2"
    ],
    "function*****gradient descent": [
        "$ARG1 amenable to $ARG2",
        "$ARG1 approximators based on using $ARG2"
    ],
    "expectation*****stochastic": [
        "$ARG1 is typically not tractable when y is high-dimensional ( or is the result of the composition of many discrete $ARG2"
    ],
    "stochastic*****bias": [
        "$ARG1 decisions ) , it can be estimated without $ARG2"
    ],
    "bias*****average": [
        "$ARG1 using a Monte Carlo $ARG2"
    ],
    "stochastic*****gradient": [
        "$ARG1 estimate of the $ARG2"
    ],
    "gradient*****other": [
        "$ARG1 can be used with SGD or $ARG2",
        "$ARG1 steps , each player reducing their own cost at the expense of the $ARG2",
        "$ARG1 of log p ( x ( k ) = x | h ( k ) ) with respect to the $ARG2",
        "$ARG1 onto the $ARG2",
        "$ARG1 update ( performing the update right after the gradients have been computed ) or used with $ARG2",
        "$ARG1 estimates to be independent from each $ARG2"
    ],
    "stochastic*****optimization": [
        "$ARG1 gradient-based $ARG2"
    ],
    "expected*****assumption": [
        "$ARG1 cost : E z [ J ( y ) ] = J ( y ) p ( y ) \u2202 E [ J ( y ) ] J ( y ) \u2202p ( y ) J ( y ) p ( y ) \u2202 log p ( y ) J ( y ( i ) ) y ( i ) \u223cp ( y ) , i=1 \u2202 log p ( y ( i ) ) Equation 20.60 relies on the $ARG2"
    ],
    "unbiased*****estimator": [
        "$ARG1 Monte Carlo $ARG2"
    ],
    "estimator*****gradient": [
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the exact $ARG2"
    ],
    "estimator*****variance": [
        "$ARG1 is that it has a very high $ARG2",
        "$ARG1 by using $ARG2",
        "$ARG1 ( e.g .. its $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 has reduced $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of \u03c32 we consider is known as the sample $ARG2",
        "$ARG1 is simply the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "variance*****estimator": [
        "$ARG1 , so that many samples of y need to be drawn to obtain a good $ARG2",
        "$ARG1 of that $ARG2",
        "$ARG1 than the $ARG2",
        "$ARG1 rises above that of the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 is a biased $ARG2",
        "$ARG1 of an $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "gradient*****learning rate": [
        "$ARG1 , or equivalently , if only one sample is drawn , SGD will converge very slowly and will require a smaller $ARG2",
        "$ARG1 because the $ARG2",
        "$ARG1 multiplied by the $ARG2",
        "$ARG1 : r \u2190 r + g \ue00c g Compute update : \u2206\u03b8 \u2190 \u2212 \u03b4+\ue00f\u221ar \ue00c g. ( Division and square root applied Apply update : \u03b8 \u2190 \u03b8 + \u2206\u03b8 have made the $ARG2",
        "$ARG1 downhill : \u03b8 \u2190 \u03b8 \u2212 \ue00fg , where \ue00f is the $ARG2"
    ],
    "estimator*****expected value": [
        "$ARG1 so that its $ARG2",
        "$ARG1 i=1 , x ( i ) \u223cq p ( x ( i ) ) f ( x ( i ) ) q ( x ( i ) ) We see readily that the $ARG2"
    ],
    "expected value*****variance": [
        "$ARG1 remains unchanged but its $ARG2",
        "$ARG1 ) and q \u2217 is the optimal one ( in the sense of yielding minimum $ARG2"
    ],
    "change*****expectation": [
        "$ARG1 the $ARG2",
        "$ARG1 our $ARG2"
    ],
    "expectation*****1": [
        "$ARG1 of the estimated \u2202 log p ( y ) \u2202 log p ( y ) Ep ( y ) p ( y ) \ue058 \u2202p ( y ) p ( y ) = $ARG2"
    ],
    "element*****vector": [
        "$ARG1 \u03c9 i of the $ARG2",
        "$ARG1 of the random $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 ci of some $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 is in R , and the $ARG2",
        "$ARG1 of x , and either concatenating some zeros to the result if D is taller than it is wide , or discarding some of the last elements of the $ARG2"
    ],
    "vector*****gradient": [
        "$ARG1 \u03c9 : p ( y ) 2 Ep ( y ) J ( y ) \u2202 log b \u2217 ( \u03c9 ) i = p ( y ) 2 E p ( y ) \u2202 log The $ARG2",
        "$ARG1 , we take the $ARG2",
        "$ARG1 : the $ARG2",
        "$ARG1 whose elements are equal to the sign of the elements of the $ARG2"
    ],
    "reduction*****reinforcement learning": [
        "$ARG1 methods have been introduced in the $ARG2"
    ],
    "algorithm*****variance": [
        "$ARG1 with reduced $ARG2",
        "$ARG1 repeatedly proposed changing the mean and $ARG2",
        "$ARG1 to \u201c perceive \u201d the input X as having higher $ARG2"
    ],
    "variance*****deep learning": [
        "$ARG1 in the context of $ARG2"
    ],
    "standard deviation*****average": [
        "$ARG1 estimated by a moving $ARG2"
    ],
    "average*****learning rate": [
        "$ARG1 during training , as a kind of adaptive $ARG2"
    ],
    "variance*****gradient": [
        "$ARG1 REINFORCE-based estimators can be understood as estimating the $ARG2",
        "$ARG1 in the estimate of the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 in the estimate of the $ARG2",
        "$ARG1 and the goal of initializing all layers to have the same $ARG2",
        "$ARG1 in $ARG2"
    ],
    "directed graphical model*****graphical model": [
        "$ARG1 make up a prominent class of $ARG2"
    ],
    "directed graphical model*****machine learning": [
        "$ARG1 have been very popular within the greater $ARG2"
    ],
    "machine learning*****deep learning": [
        "$ARG1 community , within the smaller $ARG2",
        "$ARG1 algorithms , including $ARG2",
        "$ARG1 techniques became more popular until the modern $ARG2",
        "$ARG1 , including those who are beginning a career in $ARG2",
        "$ARG1 models that shed light on these basic scienti\ufb01c questions are useful apart from their ability to solve The modern term \u201c $ARG2",
        "$ARG1 in general and for $ARG2",
        "$ARG1 , including $ARG2",
        "$ARG1 techniques that have strongly in\ufb02uenced the development of $ARG2",
        "$ARG1 algorithms can work , but they are rarely used in practice when working with $ARG2",
        "$ARG1 algorithms , especially $ARG2"
    ],
    "deep learning*****undirected model": [
        "$ARG1 community they have until roughly 2013 been overshadowed by $ARG2",
        "$ARG1 , this most often happens when pmodel ( x ) is represented by an $ARG2",
        "$ARG1 approach to $ARG2",
        "$ARG1 Figure 16.12 : Converting an $ARG2"
    ],
    "directed graphical model*****deep learning": [
        "$ARG1 that have traditionally been associated with the $ARG2"
    ],
    "deep belief network*****model": [
        "$ARG1 , which are a partially directed $ARG2"
    ],
    "feature*****deep learning": [
        "$ARG1 learners in the context of $ARG2"
    ],
    "deep learning*****density estimation": [
        "$ARG1 , though they tend to perform poorly at sample generation and $ARG2"
    ],
    "sigmoid belief network*****directed graphical model": [
        "$ARG1 ( Neal , 1990 ) are a simple form of $ARG2"
    ],
    "directed graphical model*****conditional probability distribution": [
        "$ARG1 with a speci\ufb01c kind of $ARG2"
    ],
    "sigmoid belief network*****vector": [
        "$ARG1 as having a $ARG2"
    ],
    "vector*****element": [
        "$ARG1 of binary states s , with each $ARG2",
        "$ARG1 of pre-softmax activations ( or scores ) , with one $ARG2",
        "$ARG1 and solve for the point where every $ARG2"
    ],
    "element*****\u03c3": [
        "$ARG1 of the state in\ufb02uenced by its ancestors : p ( si ) = $ARG2"
    ],
    "sigmoid belief network*****ancestral sampling": [
        "$ARG1 is one that is divided into many layers , with $ARG2"
    ],
    "ancestral sampling*****series": [
        "$ARG1 proceeding through a $ARG2"
    ],
    "series*****hidden layer": [
        "$ARG1 of many $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of single-layer autoencoders , each trained to reconstruct the previous autoencoder \u2019 s $ARG2"
    ],
    "hidden layer*****visible layer": [
        "$ARG1 and then ultimately generating the $ARG2"
    ],
    "other*****restricted boltzmann machine": [
        "$ARG1 , rather than sampled from a $ARG2"
    ],
    "universal approximator*****probability distribution": [
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2"
    ],
    "probability distribution*****visible layer": [
        "$ARG1 over binary variables arbitrarily well , given enough depth , even if the width of the individual layers is restricted to the dimensionality of the $ARG2"
    ],
    "sigmoid belief network*****other": [
        "$ARG1 , most $ARG2"
    ],
    "inference*****sigmoid belief network": [
        "$ARG1 in a $ARG2"
    ],
    "sigmoid belief network*****inference": [
        "$ARG1 combined with an $ARG2",
        "$ARG1 still use this $ARG2"
    ],
    "inference*****mean \ufb01eld": [
        "$ARG1 network that predicts the parameters of the $ARG2",
        "$ARG1 procedure such as $ARG2",
        "$ARG1 and variational We can make a $ARG2",
        "$ARG1 than iterating the $ARG2",
        "$ARG1 network , then applying one step of $ARG2"
    ],
    "importance sampling*****wake-sleep": [
        "$ARG1 , reweighted $ARG2"
    ],
    "wake-sleep*****sigmoid belief network": [
        "$ARG1 ( Bornschein and Bengio , 2015 ) and bidirectional Helmholtz machines ( Bornschein et al. , 2015 ) make it possible to quickly train $ARG2"
    ],
    "special case*****sigmoid belief network": [
        "$ARG1 of $ARG2"
    ],
    "belief network*****other": [
        "$ARG1 to $ARG2"
    ],
    "model*****function": [
        "$ARG1 transforms samples of latent variables z to samples x or to distributions over samples x using a di\ufb00erentiable $ARG2",
        "$ARG1 , in which we use a nonlinear $ARG2",
        "$ARG1 provides a $ARG2",
        "$ARG1 encodes a very general belief that the $ARG2",
        "$ARG1 of brain $ARG2",
        "$ARG1 that approximates some desired $ARG2",
        "$ARG1 that is quadratic as a $ARG2",
        "$ARG1 implements a quadratic $ARG2",
        "$ARG1 to learn a $ARG2",
        "$ARG1 that learns a $ARG2",
        "$ARG1 to learn a $ARG2",
        "$ARG1 family does not necessarily include the target $ARG2"
    ],
    "function*****neural network": [
        "$ARG1 g ( z ; \u03b8 ( g ) ) which is typically represented by a $ARG2",
        "$ARG1 space , while $ARG2",
        "$ARG1 near 0 , training a deep $ARG2",
        "$ARG1 , which led to a backlash against the entire $ARG2",
        "$ARG1 was introduced in early $ARG2",
        "$ARG1 to train a $ARG2",
        "$ARG1 , we can approximate it with a $ARG2",
        "$ARG1 that are widely used in practice for $ARG2"
    ],
    "model*****variational autoencoder": [
        "$ARG1 class includes $ARG2"
    ],
    "variational autoencoder*****inference": [
        "$ARG1 , which pair the generator net with an $ARG2",
        "$ARG1 is that it learns an $ARG2"
    ],
    "inference*****generative adversarial networks": [
        "$ARG1 net , $ARG2"
    ],
    "generative adversarial networks*****generator network": [
        "$ARG1 , which pair the $ARG2",
        "$ARG1 are based on a game theoretic scenario in which the $ARG2"
    ],
    "generator network*****distribution": [
        "$ARG1 are essentially just parametrized computational procedures for generating samples , where the architecture provides the family of possible distributions to sample from and the parameters select a $ARG2"
    ],
    "distribution*****example": [
        "$ARG1 from within As an $ARG2",
        "$ARG1 to another , for $ARG2",
        "$ARG1 ( for $ARG2",
        "$ARG1 that is a unit Gaussian , for $ARG2",
        "$ARG1 is then used to generate every train $ARG2",
        "$ARG1 of the form given by equation 16.7 is an $ARG2"
    ],
    "example*****standard": [
        "$ARG1 , the $ARG2",
        "$ARG1 , we depict the $ARG2"
    ],
    "standard*****normal distribution": [
        "$ARG1 procedure for drawing samples from a $ARG2"
    ],
    "normal distribution*****covariance": [
        "$ARG1 with mean \u00b5 and $ARG2"
    ],
    "\u03c3*****normal distribution": [
        "$ARG1 is to feed samples z from a $ARG2",
        "$ARG1 \u2208 ( 0 , \u221e ) control the $ARG2",
        "$ARG1 parameter of a $ARG2"
    ],
    "normal distribution*****identity": [
        "$ARG1 with zero mean and $ARG2"
    ],
    "covariance*****generator network": [
        "$ARG1 into a very simple $ARG2"
    ],
    "contains*****a\ufb03ne": [
        "$ARG1 just one $ARG2"
    ],
    "a\ufb03ne*****\u03c3": [
        "$ARG1 layer : x = g ( z ) = \u00b5 + Lz where L is given by the Cholesky decomposition of $ARG2"
    ],
    "example*****scalar": [
        "$ARG1 , inverse transform sampling ( Devroye , 2013 ) draws a $ARG2",
        "$ARG1 , let us consider the case where the RNN models only a sequence of $ARG2",
        "$ARG1 z ( i ) \u2208 D to a $ARG2",
        "$ARG1 , suppose we have $ARG2"
    ],
    "scalar*****1": [
        "$ARG1 z from U ( 0 , $ARG2"
    ],
    "1*****scalar": [
        "$ARG1 ) and applies a nonlinear transformation to a $ARG2",
        "$ARG1 , d\u2217 = arg min or , exploiting the fact that a $ARG2"
    ],
    "function*****machine learning": [
        "$ARG1 , we can sample from p ( x ) without using $ARG2",
        "$ARG1 approximation techniques were used to motivate $ARG2"
    ],
    "change*****distribution": [
        "$ARG1 of variables that transforms the $ARG2",
        "$ARG1 to the input data changes the $ARG2",
        "$ARG1 sign , Var [ s\u0302q\u2217 ] = 0 , meaning that a single sample is su\ufb03cient when the optimal $ARG2",
        "$ARG1 the $ARG2"
    ],
    "probability distribution*****learning": [
        "$ARG1 over x : pz ( g \u22121 ( x ) ) px ( x ) = \ue00c\ue00c \ue00c det ( \u2202z ) \ue00c Of course , this formula may be di\ufb03cult to evaluate , depending on the choice of g , so we often use indirect means of $ARG2",
        "$ARG1 , but have di\ufb00erent $ARG2",
        "$ARG1 we encounter in real-world applications , then we can design $ARG2",
        "$ARG1 as well as $ARG2"
    ],
    "example*****sigmoid": [
        "$ARG1 , we could use a generator net whose \ufb01nal layer consists of $ARG2"
    ],
    "sigmoid*****bernoulli distribution": [
        "$ARG1 outputs to provide the mean parameters of $ARG2"
    ],
    "bernoulli distribution*****1": [
        "$ARG1 : p ( xi = $ARG2",
        "$ARG1 with mean \u03b8 : P ( x ( i ) ; \u03b8 ) = \u03b8x ( $ARG2"
    ],
    "distribution*****reparametrization trick": [
        "$ARG1 p g ( x ) and allow us to train various criteria of pg using the $ARG2"
    ],
    "forward propagation*****model": [
        "$ARG1 , but doing so would mean the $ARG2",
        "$ARG1 in a NADE $ARG2",
        "$ARG1 in this $ARG2",
        "$ARG1 is still O ( k \u00d7 n ) \u2014but it does further reduce the storage requirements of the $ARG2"
    ],
    "generator network*****gradient descent": [
        "$ARG1 are motivated by the success of $ARG2"
    ],
    "gradient descent*****classi\ufb01cation": [
        "$ARG1 applied to di\ufb00erentiable feedforward networks for $ARG2"
    ],
    "supervised learning*****deep feedforward network": [
        "$ARG1 , $ARG2"
    ],
    "deep feedforward network*****learning": [
        "$ARG1 trained with gradient-based $ARG2",
        "$ARG1 can cause similar di\ufb03culties for $ARG2"
    ],
    "classi\ufb01cation*****learning": [
        "$ARG1 or regression because the $ARG2"
    ],
    "supervised learning*****optimization": [
        "$ARG1 , both the inputs x and the outputs y were given , and the $ARG2"
    ],
    "optimization*****mapping": [
        "$ARG1 procedure needs only to learn how to produce the speci\ufb01ed $ARG2"
    ],
    "problem*****correspondence": [
        "$ARG1 , where the $ARG2"
    ],
    "other*****image": [
        "$ARG1 con\ufb01guration details that a\ufb00ect the rendering of the $ARG2",
        "$ARG1 convolutional models have variable-sized output that automatically scales in size with the input , such as models that denoise or label each pixel in an $ARG2",
        "$ARG1 words , the complexity of the patterns of brightness in an underlying $ARG2",
        "$ARG1 transformations , such as changes in the scale or rotation of an $ARG2",
        "$ARG1 two plots , the \u03b2 values are \ufb01xed to 1.5\u00d7 the $ARG2",
        "$ARG1 two plots , \u03c6 is \ufb01xed to 0 and f is \ufb01xed to 5\u00d7 the $ARG2",
        "$ARG1 operations such as rotating the $ARG2"
    ],
    "convolutional network*****content": [
        "$ARG1 is able to learn to map z descriptions of the $ARG2",
        "$ARG1 have also been applied to learn to extract features from rich $ARG2"
    ],
    "content*****image": [
        "$ARG1 of an $ARG2"
    ],
    "generator network*****model": [
        "$ARG1 have su\ufb03cient $ARG2",
        "$ARG1 forces the $ARG2"
    ],
    "model*****optimization algorithm": [
        "$ARG1 capacity to be good generative models , and that contemporary $ARG2",
        "$ARG1 that is easy to optimize than it is to design a more powerful $ARG2",
        "$ARG1 family that is easy to optimize than to use a powerful $ARG2"
    ],
    "variational autoencoder*****model": [
        "$ARG1 or VAE ( Kingma , 2013 ; Rezende et al. , 2014 ) is a directed $ARG2",
        "$ARG1 is that they may be trained by maximizing the variational lower bound L ( q ) associated with data point x : L ( q ) = Ez\u223cq ( z|x ) log p $ARG2",
        "$ARG1 also has the advantage that it increases a bound on the log-likelihood of the $ARG2"
    ],
    "approximate inference*****encoder": [
        "$ARG1 network ( or $ARG2"
    ],
    "encoder*****decoder": [
        "$ARG1 ) q ( z | x ) is used to obtain z and pmodel ( x | z ) is then viewed as a $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 and recurrent $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 is associated with a $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 and then the $ARG2",
        "$ARG1 RNN that reads the input sequence and a $ARG2",
        "$ARG1 and a $ARG2",
        "$ARG1 functions f and nonlinear $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 and linear $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 and a single layer $ARG2",
        "$ARG1 is itself a feedforward network as is the $ARG2",
        "$ARG1 and the $ARG2",
        "$ARG1 and p $ARG2",
        "$ARG1 pencoder ( h | x ) = pmodel ( h | x ) p $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 f ( x\u0303 ) and pdecoder typically de\ufb01ned by a $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 f ( x ) and a $ARG2"
    ],
    "decoder*****variational autoencoder": [
        "$ARG1 The key insight behind $ARG2"
    ],
    "gaussian distribution*****standard deviation": [
        "$ARG1 , with noise added to a predicted mean value , maximizing this entropy term encourages increasing the $ARG2"
    ],
    "inference*****learning": [
        "$ARG1 and $ARG2",
        "$ARG1 ( above ) as well as in $ARG2",
        "$ARG1 , and $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 can be viewed as maximizing L with respect to q , and how $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 avoid this expense by $ARG2"
    ],
    "learning*****optimization algorithm": [
        "$ARG1 infer q via an $ARG2"
    ],
    "variational autoencoder*****encoder": [
        "$ARG1 is to train a parametric $ARG2",
        "$ARG1 is that simultaneously training a parametric $ARG2"
    ],
    "encoder*****inference": [
        "$ARG1 ( also sometimes called an $ARG2",
        "$ARG1 may be viewed as performing learned approximate MAP $ARG2",
        "$ARG1 is trained to predict the output of iterative $ARG2"
    ],
    "variable*****gradient": [
        "$ARG1 , we can then back-propagate through samples of z drawn from q ( z | x ) = q ( z ; f ( x ; \u03b8 ) ) in order to obtain a $ARG2",
        "$ARG1 whose $ARG2"
    ],
    "learning*****encoder": [
        "$ARG1 then consists solely of maximizing L with respect to the parameters of the $ARG2",
        "$ARG1 of a very high-capacity $ARG2"
    ],
    "maximum likelihood*****model": [
        "$ARG1 , which minimizes DKL ( pdata \ue06bp $ARG2",
        "$ARG1 as trying to achieve balance between two forces , one pushing up on the $ARG2",
        "$ARG1 can be interpreted as a procedure that forces a $ARG2",
        "$ARG1 will drive the $ARG2",
        "$ARG1 applied to our $ARG2",
        "$ARG1 criterion , in which during training the $ARG2",
        "$ARG1 thus speci\ufb01es that during training , rather than feeding the $ARG2",
        "$ARG1 as an attempt to make the $ARG2",
        "$ARG1 encourages the $ARG2",
        "$ARG1 training of a generative $ARG2",
        "$ARG1 to an undirected probabilistic $ARG2"
    ],
    "model*****probability": [
        "$ARG1 will assign high $ARG2",
        "$ARG1 would choose to put $ARG2",
        "$ARG1 usually generalizes better and assigns higher $ARG2",
        "$ARG1 \u2019 s $ARG2",
        "$ARG1 but low $ARG2",
        "$ARG1 will struggle to place high $ARG2",
        "$ARG1 does not assign zero $ARG2",
        "$ARG1 ( especially those based on estimating an explicit $ARG2",
        "$ARG1 computes the $ARG2",
        "$ARG1 consisting of observed variables v and latent variables h. We would like to compute the log $ARG2",
        "$ARG1 to have high $ARG2",
        "$ARG1 from having high $ARG2"
    ],
    "probability*****set": [
        "$ARG1 to points that occur in the training $ARG2",
        "$ARG1 to the test $ARG2",
        "$ARG1 that a player will win a poker game given that she has a certain $ARG2",
        "$ARG1 mass of a $ARG2",
        "$ARG1 that x lies in some $ARG2",
        "$ARG1 associated to each possible input value that is simply equal to the empirical frequency of that value in the training $ARG2",
        "$ARG1 of a continuous vector-valued x lying in some $ARG2"
    ],
    "probability*****other": [
        "$ARG1 to $ARG2",
        "$ARG1 mass on blurry images rather than some $ARG2",
        "$ARG1 to most realistic points while $ARG2",
        "$ARG1 of transitioning from any state x to any $ARG2",
        "$ARG1 density on all $ARG2",
        "$ARG1 of some event , given that some $ARG2",
        "$ARG1 , while $ARG2"
    ],
    "other*****variational autoencoder": [
        "$ARG1 part of the space is that the $ARG2"
    ],
    "variational autoencoder*****gaussian distribution": [
        "$ARG1 used in practice usually have a $ARG2"
    ],
    "distribution*****mean squared error": [
        "$ARG1 is similar to training a traditional autoencoder with $ARG2",
        "$ARG1 and minimization of $ARG2"
    ],
    "mean squared error*****change": [
        "$ARG1 , in the sense that it has a tendency to ignore features of the input that occupy few pixels or that cause only a small $ARG2"
    ],
    "subset*****encoder": [
        "$ARG1 of the dimensions of z , as if the $ARG2"
    ],
    "encoder*****distribution": [
        "$ARG1 was not able to transform enough of the local directions in input space to a space where the marginal $ARG2",
        "$ARG1 drives the contractive penalty \u2126 ( h ) to approach 0 without having learned anything about the $ARG2"
    ],
    "attention*****model": [
        "$ARG1 writer or DRAW $ARG2"
    ],
    "decoder*****attention": [
        "$ARG1 combined with an $ARG2"
    ],
    "process*****model": [
        "$ARG1 for the DRAW $ARG2",
        "$ARG1 in reverse , by training a $ARG2",
        "$ARG1 will then push down strongly on the mode corresponding to 7s , and the $ARG2",
        "$ARG1 of inferring the value of h in this $ARG2",
        "$ARG1 , or to analyze the learned $ARG2",
        "$ARG1 , in which we update the $ARG2"
    ],
    "importance weighted autoencoder*****1": [
        "$ARG1 ( Burda et al. , 2015 ) objective : $ARG2"
    ],
    "importance sampling*****distribution": [
        "$ARG1 of z from proposal $ARG2"
    ],
    "variational autoencoder*****other": [
        "$ARG1 have some interesting connections to the MP-DBM and $ARG2"
    ],
    "other*****back-propagation": [
        "$ARG1 approaches that involve $ARG2"
    ],
    "back-propagation*****approximate inference": [
        "$ARG1 through the $ARG2"
    ],
    "mean \ufb01eld*****computational graph": [
        "$ARG1 \ufb01xed point equations to provide the $ARG2",
        "$ARG1 \ufb01xed point equations specify how to share parameters between the $ARG2"
    ],
    "variational autoencoder*****computational graph": [
        "$ARG1 is de\ufb01ned for arbitrary $ARG2"
    ],
    "computational graph*****model": [
        "$ARG1 , which makes it applicable to a wider range of probabilistic $ARG2",
        "$ARG1 to de\ufb01ne the conditional distributions within a $ARG2"
    ],
    "model*****interpretation": [
        "$ARG1 , while the criteria for the MP-DBM and related models are more heuristic and have little probabilistic $ARG2"
    ],
    "interpretation*****approximate inference": [
        "$ARG1 beyond making the results of $ARG2"
    ],
    "approximate inference*****subset": [
        "$ARG1 over any $ARG2"
    ],
    "subset*****other": [
        "$ARG1 of variables given any $ARG2",
        "$ARG1 of variables whose values are largely determined by the $ARG2",
        "$ARG1 of nodes that are all connected to each $ARG2"
    ],
    "subset*****mean \ufb01eld": [
        "$ARG1 of variables , because the $ARG2"
    ],
    "property*****variational autoencoder": [
        "$ARG1 of the $ARG2"
    ],
    "encoder*****generator network": [
        "$ARG1 in combination with the $ARG2"
    ],
    "model*****system": [
        "$ARG1 to learn a predictable coordinate $ARG2",
        "$ARG1 architecture for processing images that was inspired by the structure of the mammalian visual $ARG2",
        "$ARG1 has low capacity but the $ARG2"
    ],
    "system*****encoder": [
        "$ARG1 that the $ARG2"
    ],
    "manifolds*****variational autoencoder": [
        "$ARG1 learned by the $ARG2",
        "$ARG1 , learned by a $ARG2"
    ],
    "algorithm*****factors of variation": [
        "$ARG1 discovered two independent $ARG2"
    ],
    "generative adversarial networks*****gans": [
        "$ARG1 or $ARG2"
    ],
    "gans*****generator network": [
        "$ARG1 ( Goodfellow et al. , 2014c ) are another generative modeling approach based on di\ufb00erentiable $ARG2",
        "$ARG1 , they do not need to pair the $ARG2"
    ],
    "probability*****example": [
        "$ARG1 that x is a real training $ARG2",
        "$ARG1 that the component produced the $ARG2",
        "$ARG1 density contributes to the prediction of the next $ARG2"
    ],
    "learning*****generative adversarial networks": [
        "$ARG1 in $ARG2"
    ],
    "generative adversarial networks*****function": [
        "$ARG1 is as a zero-sum game , in which a $ARG2"
    ],
    "learning*****convergence": [
        "$ARG1 , each player attempts to maximize its own payo\ufb00 , so that at $ARG2"
    ],
    "model*****manifold": [
        "$ARG1 with a 2-D latent code , even if we believe the intrinsic dimensionality of the data $ARG2"
    ],
    "set*****model": [
        "$ARG1 but images x actually generated by the $ARG2",
        "$ARG1 than does an individual $ARG2",
        "$ARG1 , and allows the $ARG2",
        "$ARG1 of possible minibatches of consecutive examples that all models trained thereafter will use , and each individual $ARG2",
        "$ARG1 error because it is based on feedback from the behavior of the initial $ARG2",
        "$ARG1 because it has a signi\ufb01cant impact on $ARG2",
        "$ARG1 for two di\ufb00erent models : a quadratic $ARG2",
        "$ARG1 and the $ARG2",
        "$ARG1 size m. In practice , we often use a larger $ARG2",
        "$ARG1 with which to train the $ARG2",
        "$ARG1 improves , we store a copy of the $ARG2",
        "$ARG1 , which means some training data is not fed to the $ARG2",
        "$ARG1 with replacement , and then train $ARG2",
        "$ARG1 examples that are the hardest to $ARG2",
        "$ARG1 transcriptions that the $ARG2"
    ],
    "dimension*****other": [
        "$ARG1 that has been discovered ( horizontal ) mostly corresponds to a rotation of the face , while the $ARG2",
        "$ARG1 ( two or three ) , or they can be used to compare users or items against each $ARG2"
    ],
    "gans*****learning": [
        "$ARG1 is that the $ARG2"
    ],
    "process*****approximate inference": [
        "$ARG1 requires neither $ARG2"
    ],
    "approximate inference*****partition function": [
        "$ARG1 nor approximation of a $ARG2"
    ],
    "optimization*****probability density function": [
        "$ARG1 is performed directly in the space of $ARG2"
    ],
    "learning*****gans": [
        "$ARG1 in $ARG2"
    ],
    "gans*****neural network": [
        "$ARG1 can be di\ufb03cult in practice when g and d are represented by $ARG2"
    ],
    "example*****function": [
        "$ARG1 the value $ARG2",
        "$ARG1 , the recti\ufb01ed linear $ARG2",
        "$ARG1 the $ARG2",
        "$ARG1 in section 5.1.4 , but it can also be a whole $ARG2",
        "$ARG1 ( discussed in section 5.2 ) are both examples of scenarios that may be interpreted either as estimating a parameter w or estimating a $ARG2",
        "$ARG1 , imagine that we are interested in approximating the $ARG2",
        "$ARG1 , it can be shown that the linear $ARG2",
        "$ARG1 contributes a term \u03b1ik ( x , x ( i ) ) to the decision $ARG2",
        "$ARG1 , imagine that the target $ARG2",
        "$ARG1 is illustrated in \ufb01gure 14.7 , showing that , by making the reconstruction $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 , let I be a $ARG2",
        "$ARG1 to constrain each output channel i to be a $ARG2",
        "$ARG1 of a $ARG2",
        "$ARG1 , for a $ARG2",
        "$ARG1 of a $ARG2",
        "$ARG1 rejection mechanism to $ARG2"
    ],
    "problem*****gans": [
        "$ARG1 a\ufb00ects $ARG2"
    ],
    "gradient*****maximum likelihood": [
        "$ARG1 as $ARG2",
        "$ARG1 of $ARG2"
    ],
    "convergence*****variance": [
        "$ARG1 in practice , possibly due to suboptimality of the discriminator , or possibly due to high $ARG2"
    ],
    "variance*****expected": [
        "$ARG1 around the $ARG2"
    ],
    "derivative*****cost function": [
        "$ARG1 of the generator \u2019 s $ARG2"
    ],
    "learning*****problem": [
        "$ARG1 remains an open $ARG2"
    ],
    "model*****hyperparameters": [
        "$ARG1 architecture and $ARG2",
        "$ARG1 , including its $ARG2",
        "$ARG1 is con\ufb01dent will perform as well as any $ARG2"
    ],
    "synthesis*****factors of variation": [
        "$ARG1 tasks , and showed that its latent representation space captures important $ARG2"
    ],
    "gans*****dataset": [
        "$ARG1 trained on the LSUN $ARG2"
    ],
    "gans*****distribution": [
        "$ARG1 ( Mirza and Osindero , 2014 ) that learn to sample from a $ARG2"
    ],
    "series*****gans": [
        "$ARG1 of conditional $ARG2"
    ],
    "gans*****image": [
        "$ARG1 can be trained to \ufb01rst generate a very low-resolution version of an $ARG2"
    ],
    "technique*****model": [
        "$ARG1 is called the LAPGAN $ARG2"
    ],
    "capability*****probability distribution": [
        "$ARG1 of the GAN training procedure is that it can \ufb01t $ARG2"
    ],
    "probability distribution*****probability": [
        "$ARG1 that assign zero $ARG2",
        "$ARG1 ( for the sum ) or a $ARG2",
        "$ARG1 that contain zero $ARG2",
        "$ARG1 , we must use the corresponding normalized $ARG2",
        "$ARG1 , we impose the requirement that none of the sub-models assigns $ARG2",
        "$ARG1 that allows us to place a sharp peak of $ARG2"
    ],
    "probability*****trace": [
        "$ARG1 of speci\ufb01c points , the generator net learns to $ARG2"
    ],
    "trace*****manifold": [
        "$ARG1 out a $ARG2",
        "$ARG1 out a $ARG2"
    ],
    "set*****manifold": [
        "$ARG1 , while still representing a $ARG2"
    ],
    "manifold*****human": [
        "$ARG1 that a $ARG2",
        "$ARG1 of images of $ARG2"
    ],
    "human*****task": [
        "$ARG1 observer judges to capture the essence of the generation $ARG2",
        "$ARG1 beings fail to perceive changes in their environment that are not immediately relevant to the $ARG2",
        "$ARG1 e\ufb00ort for each separate $ARG2",
        "$ARG1 designer is con\ufb01dent has no relevance to the $ARG2"
    ],
    "generator network*****probability": [
        "$ARG1 assigns non-zero $ARG2"
    ],
    "probability*****generator network": [
        "$ARG1 to all points simply by making the last layer of the $ARG2"
    ],
    "generator network*****gaussian noise": [
        "$ARG1 add $ARG2",
        "$ARG1 that add $ARG2"
    ],
    "gaussian noise*****distribution": [
        "$ARG1 in this manner sample from the same $ARG2"
    ],
    "distribution*****generator network": [
        "$ARG1 that one obtains by using the $ARG2"
    ],
    "gradient*****generator network": [
        "$ARG1 for the $ARG2"
    ],
    "gradient*****weights": [
        "$ARG1 of the deterministic version of the discriminator with its $ARG2",
        "$ARG1 of all the parameters ( including di\ufb00erent groups of parameters , such as $ARG2",
        "$ARG1 is weighted by wi = \ue050N i These $ARG2",
        "$ARG1 step to update the $ARG2"
    ],
    "generator network*****other": [
        "$ARG1 , similar principles can be used to train $ARG2",
        "$ARG1 with any $ARG2"
    ],
    "example*****logistic regression": [
        "$ARG1 , selfsupervised boosting can be used to train an RBM generator to fool a $ARG2",
        "$ARG1 , when $ARG2",
        "$ARG1 is $ARG2"
    ],
    "generative moment matching networks*****model": [
        "$ARG1 ( Li et al. , 2015 ; Dziugaite et al. , 2015 ) are another form of generative $ARG2"
    ],
    "model*****generator network": [
        "$ARG1 based on di\ufb00erentiable $ARG2"
    ],
    "other*****inference": [
        "$ARG1 network\u2014neither an $ARG2",
        "$ARG1 approximations that provide only a lower bound on p\u0303 ( x ) , such as variational $ARG2",
        "$ARG1 Forms of Learned $ARG2"
    ],
    "inference*****gans": [
        "$ARG1 network as used with VAEs nor a discriminator network as used with $ARG2"
    ],
    "technique*****moment matching": [
        "$ARG1 called $ARG2"
    ],
    "moment matching*****statistics": [
        "$ARG1 is to train the generator in such a way that many of the $ARG2"
    ],
    "statistics*****model": [
        "$ARG1 of samples generated by the $ARG2"
    ],
    "model*****statistics": [
        "$ARG1 are as similar as possible to those of the $ARG2"
    ],
    "statistics*****set": [
        "$ARG1 of the examples in the training $ARG2",
        "$ARG1 of the output in the training $ARG2"
    ],
    "expectation*****random variable": [
        "$ARG1 of di\ufb00erent powers of a $ARG2",
        "$ARG1 is over the data ( seen as samples from a $ARG2"
    ],
    "multiple*****element": [
        "$ARG1 dimensions , each $ARG2",
        "$ARG1 outputs ( every $ARG2"
    ],
    "example*****number": [
        "$ARG1 , if we want to match all the moments of the form xi x j , then we need to minimize the di\ufb00erence between a $ARG2",
        "$ARG1 the sum has an exponential $ARG2",
        "$ARG1 , inserting a $ARG2",
        "$ARG1 by predicting the correct target a $ARG2",
        "$ARG1 , vision tasks require processing a large $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 ? Because in high-dimensional spaces the $ARG2",
        "$ARG1 , there is not a good way of knowing whether to retrain for the same $ARG2",
        "$ARG1 , suppose we \ufb01rst sample a real $ARG2",
        "$ARG1 doubling the $ARG2",
        "$ARG1 : Multi-Digit $ARG2"
    ],
    "number*****dimension": [
        "$ARG1 of values that is quadratic in the $ARG2"
    ],
    "matching*****gaussian distribution": [
        "$ARG1 all of the \ufb01rst and second moments would only be su\ufb03cient to \ufb01t a multivariate $ARG2"
    ],
    "gans*****problem": [
        "$ARG1 avoid this $ARG2"
    ],
    "attention*****statistic": [
        "$ARG1 on whichever $ARG2"
    ],
    "statistic*****generator network": [
        "$ARG1 the $ARG2"
    ],
    "generator network*****matching": [
        "$ARG1 is $ARG2"
    ],
    "generative moment matching networks*****cost function": [
        "$ARG1 can be trained by minimizing a $ARG2"
    ],
    "cost function*****mapping": [
        "$ARG1 measures the error in the \ufb01rst moments in an in\ufb01nite-dimensional space , using an implicit $ARG2"
    ],
    "mapping*****feature": [
        "$ARG1 to $ARG2"
    ],
    "feature*****kernel": [
        "$ARG1 space de\ufb01ned by a $ARG2",
        "$ARG1 space with a tractable $ARG2",
        "$ARG1 , the width of the representation shrinks by one pixel less than the $ARG2"
    ],
    "encoder*****set": [
        "$ARG1 of the autoencoder is used to transform the entire training $ARG2"
    ],
    "generator network*****decoder": [
        "$ARG1 is then trained to generate code samples , which may be mapped to visually pleasing samples via the $ARG2"
    ],
    "gans*****cost function": [
        "$ARG1 , the $ARG2"
    ],
    "cost function*****set": [
        "$ARG1 is de\ufb01ned only with respect to a batch of examples from both the training $ARG2"
    ],
    "set*****generator network": [
        "$ARG1 and the $ARG2"
    ],
    "function*****example": [
        "$ARG1 of only one training $ARG2",
        "$ARG1 f and an $ARG2",
        "$ARG1 is equivariant to g. For $ARG2",
        "$ARG1 in this $ARG2"
    ],
    "example*****generator network": [
        "$ARG1 or only one sample from the $ARG2"
    ],
    "gans*****probability": [
        "$ARG1 , it is possible to train a generator net using MMD even if that generator net assigns zero $ARG2"
    ],
    "generator network*****example": [
        "$ARG1 that includes a convolutional structure ( see for $ARG2"
    ],
    "transpose*****convolution": [
        "$ARG1 \u201d of the $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "convolutional network*****information": [
        "$ARG1 for recognition tasks have $ARG2"
    ],
    "information*****image": [
        "$ARG1 \ufb02ow from the $ARG2",
        "$ARG1 is discarded as the representation of the $ARG2",
        "$ARG1 that there is a nose in the $ARG2",
        "$ARG1 about a nose from an $ARG2"
    ],
    "image*****information": [
        "$ARG1 \ufb02ows upward through the network , $ARG2"
    ],
    "image*****invariant": [
        "$ARG1 becomes more $ARG2",
        "$ARG1 datasets , there are separate benchmarks for models that are permutation $ARG2"
    ],
    "information*****pooling": [
        "$ARG1 in a convolutional recognition network is the $ARG2",
        "$ARG1 , then using $ARG2"
    ],
    "generator network*****information": [
        "$ARG1 seems to need to add $ARG2"
    ],
    "pooling*****generator network": [
        "$ARG1 layer into the $ARG2"
    ],
    "generator network*****pooling": [
        "$ARG1 because most $ARG2"
    ],
    "operation*****conditions": [
        "$ARG1 under certain simplifying $ARG2"
    ],
    "operation*****pooling": [
        "$ARG1 is constrained to be equal to the width of the $ARG2",
        "$ARG1 called $ARG2"
    ],
    "operation*****tensor": [
        "$ARG1 allocates a $ARG2"
    ],
    "conditional probability distribution*****neural network": [
        "$ARG1 in these models are represented by $ARG2"
    ],
    "neural network*****logistic regression": [
        "$ARG1 such as $ARG2"
    ],
    "joint probability*****chain rule of probability": [
        "$ARG1 over the observed variables using the $ARG2"
    ],
    "fully-visible bayes network*****logistic regression": [
        "$ARG1 ( FVBNs ) and used successfully in many forms , \ufb01rst with $ARG2"
    ],
    "logistic regression*****distribution": [
        "$ARG1 for each conditional $ARG2"
    ],
    "distribution*****neural network": [
        "$ARG1 ( Frey , 1998 ) and then with $ARG2",
        "$ARG1 by having outputs of the $ARG2"
    ],
    "belief network*****variable": [
        "$ARG1 predicts the i-th $ARG2"
    ],
    "linear model*****linear regression": [
        "$ARG1 ( $ARG2",
        "$ARG1 such as $ARG2"
    ],
    "linear regression*****logistic regression": [
        "$ARG1 for real-valued data , $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 , or $ARG2"
    ],
    "logistic regression*****softmax": [
        "$ARG1 for binary data , $ARG2"
    ],
    "model*****gaussian distribution": [
        "$ARG1 is merely another way to formulate a multivariate $ARG2",
        "$ARG1 by integrating out h ; the result is just a $ARG2"
    ],
    "generalization*****classi\ufb01cation": [
        "$ARG1 of linear $ARG2"
    ],
    "model*****kernel trick": [
        "$ARG1 itself does not o\ufb00er a way of increasing its capacity , so capacity must be raised using techniques like basis expansions of the input or the $ARG2"
    ],
    "generalization*****parameter sharing": [
        "$ARG1 by introducing a $ARG2"
    ],
    "parameter sharing*****feature": [
        "$ARG1 and $ARG2"
    ],
    "feature*****principle": [
        "$ARG1 sharing $ARG2",
        "$ARG1 regardless of whether the object \u2019 s velocity is high or low , but the slowness $ARG2",
        "$ARG1 mappings are usually based only on the $ARG2"
    ],
    "principle*****deep learning": [
        "$ARG1 common to $ARG2",
        "$ARG1 that recurs throughout $ARG2",
        "$ARG1 of $ARG2"
    ],
    "curse of dimensionality*****graphical model": [
        "$ARG1 arising out of traditional tabular $ARG2"
    ],
    "neural network*****1": [
        "$ARG1 with ( i \u2212 $ARG2",
        "$ARG1 research began with a breakthrough in CHAPTER $ARG2",
        "$ARG1 that is specialized for processing a sequence of values x ( $ARG2",
        "$ARG1 and modify layer $ARG2"
    ],
    "1*****conditional probability": [
        "$ARG1 ) \u00d7 k inputs and k outputs ( if the variables are discrete and take k values , encoded one-hot ) allows one to estimate the $ARG2"
    ],
    "conditional probability*****number": [
        "$ARG1 without requiring an exponential $ARG2"
    ],
    "number*****random variable": [
        "$ARG1 of parameters ( and examples ) , yet still is able to capture high-order dependencies between the $ARG2",
        "$ARG1 of $ARG2"
    ],
    "group*****1": [
        "$ARG1 only depend on the input values x $ARG2",
        "$ARG1 at layer l + $ARG2",
        "$ARG1 at layer l + $ARG2",
        "$ARG1 i , { ( i \u2212 $ARG2"
    ],
    "deep learning*****convolutional network": [
        "$ARG1 in scenarios ranging from recurrent and $ARG2",
        "$ARG1 came when a $ARG2",
        "$ARG1 architectures such as $ARG2"
    ],
    "convolutional network*****transfer learning": [
        "$ARG1 architectures to multi-task and $ARG2"
    ],
    "sigmoid*****variable": [
        "$ARG1 output for a Bernoulli $ARG2",
        "$ARG1 Units for Bernoulli Output Distributions Many tasks require predicting the value of a binary $ARG2"
    ],
    "variable*****softmax": [
        "$ARG1 or $ARG2"
    ],
    "softmax*****variable": [
        "$ARG1 output for a multinoulli $ARG2"
    ],
    "weights*****element": [
        "$ARG1 Wj , k , i from the i-th input xi to the k -th $ARG2"
    ],
    "element*****group": [
        "$ARG1 of the j-th $ARG2"
    ],
    "illustration*****estimator": [
        "$ARG1 of the neural autoregressive density $ARG2"
    ],
    "weights*****group": [
        "$ARG1 going out from x i to the k-th unit of any $ARG2"
    ],
    "recall*****vector": [
        "$ARG1 that the $ARG2"
    ],
    "model*****mean \ufb01eld": [
        "$ARG1 loosely resembles the computations performed in $ARG2",
        "$ARG1 , we show that the use of this $ARG2"
    ],
    "inference*****missing inputs": [
        "$ARG1 to \ufb01ll in $ARG2"
    ],
    "inference*****recurrent network": [
        "$ARG1 corresponds to running a $ARG2"
    ],
    "recurrent network*****weights": [
        "$ARG1 with shared $ARG2",
        "$ARG1 is stable ( can store memories , with gradients not exploding ) , the di\ufb03culty with long-term dependencies arises from the exponentially smaller $ARG2",
        "$ARG1 ( with the hidden-to-hidden recurrent $ARG2",
        "$ARG1 , large $ARG2"
    ],
    "weights*****inference": [
        "$ARG1 and the \ufb01rst step of that $ARG2",
        "$ARG1 thus corresponds to weight As with full Bayesian $ARG2",
        "$ARG1 by 2 once before beginning to run $ARG2"
    ],
    "weights*****transpose": [
        "$ARG1 are the $ARG2"
    ],
    "transpose*****weights": [
        "$ARG1 of the input-to-hidden $ARG2"
    ],
    "gaussian mixture*****weights": [
        "$ARG1 ( introduced in section 3.9.6 ) with mixture $ARG2"
    ],
    "weights*****prior probability": [
        "$ARG1 \u03b1i ( the coe\ufb03cient or $ARG2"
    ],
    "prior probability*****variance": [
        "$ARG1 for component i ) , percomponent conditional mean \u00b5i and per-component conditional $ARG2"
    ],
    "distribution*****softmax": [
        "$ARG1 are outputs of the network , with the mixture weight probabilities produced by a $ARG2"
    ],
    "gradient*****back-propagation": [
        "$ARG1 on the mean , in the $ARG2",
        "$ARG1 using the $ARG2",
        "$ARG1 is given by A\ue03e G. The $ARG2",
        "$ARG1 on W ( 2 ) , using the $ARG2",
        "$ARG1 on h ( t+1 ) propagates one step backward , during $ARG2",
        "$ARG1 patterns are lost in the null space of $ARG2"
    ],
    "probability distribution*****subset": [
        "$ARG1 over any $ARG2",
        "$ARG1 over just a $ARG2",
        "$ARG1 over the $ARG2"
    ],
    "output layer*****number": [
        "$ARG1 can still be computed in O ( nh ) multiply-add operations , as in the regular NADE , where h is the $ARG2",
        "$ARG1 is reduced to be proportional to the $ARG2",
        "$ARG1 is usually relatively inexpensive due to the small $ARG2"
    ],
    "hidden layer*****group": [
        "$ARG1 , the computation is O ( n2h2 ) if every \u201c previous \u201d $ARG2"
    ],
    "1*****group": [
        "$ARG1 only depend on the i -th $ARG2"
    ],
    "score matching*****denoising autoencoder": [
        "$ARG1 , $ARG2"
    ],
    "denoising autoencoder*****contractive autoencoder": [
        "$ARG1 , and $ARG2",
        "$ARG1 and the $ARG2"
    ],
    "variational autoencoder*****probability distribution": [
        "$ARG1 , explicitly represent a $ARG2"
    ],
    "probability distribution*****ancestral sampling": [
        "$ARG1 and admit straightforward $ARG2"
    ],
    "contractive autoencoder*****tangent plane": [
        "$ARG1 are designed to recover an estimate of the $ARG2"
    ],
    "tangent plane*****manifold": [
        "$ARG1 of the data $ARG2",
        "$ARG1 is given by d basis vectors that span the local directions of variation allowed on the $ARG2",
        "$ARG1 touches the $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "manifold*****technique": [
        "$ARG1 di\ufb00usion $ARG2"
    ],
    "technique*****markov chain": [
        "$ARG1 is a kind of There is also a more general $ARG2"
    ],
    "markov chain*****denoising autoencoder": [
        "$ARG1 Associated with any $ARG2",
        "$ARG1 for generalized $ARG2",
        "$ARG1 associated with a trained $ARG2"
    ],
    "denoising autoencoder*****markov chain": [
        "$ARG1 The above discussion left open the question of what noise to inject and where , in order to obtain a $ARG2"
    ],
    "markov chain*****distribution": [
        "$ARG1 that would generate from the $ARG2",
        "$ARG1 that generates from the estimated $ARG2",
        "$ARG1 from a $ARG2",
        "$ARG1 at each step with samples from the data $ARG2",
        "$ARG1 sampling from the $ARG2",
        "$ARG1 is de\ufb01ned by a random state x and a transition $ARG2",
        "$ARG1 are drawn from some $ARG2",
        "$ARG1 until it reaches its equilibrium $ARG2",
        "$ARG1 are thus expensive to use because of the time required to burn in to the equilibrium $ARG2",
        "$ARG1 must run before reaching its equilibrium $ARG2"
    ],
    "denoising autoencoder*****distribution": [
        "$ARG1 are speci\ufb01ed by a denoising $ARG2",
        "$ARG1 and their generalizations ( such as GSNs , described below ) can be used to sample from a conditional $ARG2",
        "$ARG1 : with small levels of noise the learner only sees con\ufb01gurations near the data points , while with large levels of noise it is asked to do an almost impossible job ( because the denoising $ARG2"
    ],
    "distribution*****1": [
        "$ARG1 consists of the following sub-steps , illustrated in \ufb01gure 20.11 : $ARG2",
        "$ARG1 p1 : \u2022 for k = $ARG2",
        "$ARG1 given by : q ( x\u03b7 $ARG2",
        "$ARG1 over b is given by P ( b = $ARG2",
        "$ARG1 P $ARG2",
        "$ARG1 P ( y ( $ARG2",
        "$ARG1 factorizes as P ( y ( t ) | x ( $ARG2",
        "$ARG1 can be decomposed as p ( x ) = p ( xi | x $ARG2",
        "$ARG1 is the mean of the $ARG2",
        "$ARG1 on y ( train ) , we have p ( y ( train ) | X ( train ) , w ) = N ( y ( train ) ; X ( train ) w , I ) $ARG2",
        "$ARG1 must be normalized to integrate to $ARG2",
        "$ARG1 is no longer a k \u00d7 k \u2212 $ARG2",
        "$ARG1 preconstruct ( x | x\u0303 ) estimated from training pairs ( x , x\u0303 ) , as follows : $ARG2"
    ],
    "denoising autoencoder*****model": [
        "$ARG1 , that generates the samples from the probabilistic $ARG2",
        "$ARG1 \u201d refers to a $ARG2"
    ],
    "process*****function": [
        "$ARG1 C in state x , yielding x\u0303 , ( b ) encoding it with $ARG2",
        "$ARG1 , any $ARG2",
        "$ARG1 to learn a $ARG2",
        "$ARG1 as a $ARG2",
        "$ARG1 as just being a $ARG2"
    ],
    "function*****distribution": [
        "$ARG1 g , yielding parameters \u03c9 for the reconstruction $ARG2",
        "$ARG1 fASR that computes the most probable linguistic sequence y given the acoustic sequence X : f \u2217ASR ( X ) = arg max P \u2217 ( y | X = X ) where P \u2217 is the true conditional $ARG2",
        "$ARG1 , no Dirac or mixture of Dirac $ARG2",
        "$ARG1 f that maps an input v to an approximate $ARG2",
        "$ARG1 f ( x ) to an encoding $ARG2"
    ],
    "mean squared error*****hyperparameter": [
        "$ARG1 of reconstructions , whereas the injected noise is a $ARG2"
    ],
    "hyperparameter*****estimator": [
        "$ARG1 that controls the mixing speed as well as the extent to which the $ARG2"
    ],
    "estimator*****empirical distribution": [
        "$ARG1 smooths the $ARG2"
    ],
    "example*****stochastic": [
        "$ARG1 illustrated here , only the C and p conditionals are $ARG2",
        "$ARG1 , suppose we can evaluate a $ARG2",
        "$ARG1 at a time are sometimes called $ARG2",
        "$ARG1 of a $ARG2"
    ],
    "estimator*****distribution": [
        "$ARG1 of the corresponding true conditional $ARG2",
        "$ARG1 of the original Speci\ufb01cally , we introduce a second $ARG2",
        "$ARG1 for the \u03b8 parameter of this $ARG2",
        "$ARG1 ; it instead recovers a blurred version of the $ARG2"
    ],
    "distribution*****markov chain": [
        "$ARG1 of the above $ARG2",
        "$ARG1 of the generative $ARG2",
        "$ARG1 implicitly represented by the $ARG2",
        "$ARG1 , so a $ARG2",
        "$ARG1 that we used to arbitrarily initialize x for each $ARG2",
        "$ARG1 over all the di\ufb00erent $ARG2",
        "$ARG1 over the states of all the various $ARG2",
        "$ARG1 is called \u201c burning in \u201d the $ARG2"
    ],
    "markov chain*****estimator": [
        "$ARG1 forms a consistent $ARG2"
    ],
    "estimator*****data generating distribution": [
        "$ARG1 ( albeit an implicit one ) of the $ARG2"
    ],
    "boltzmann machine*****denoising autoencoder": [
        "$ARG1 , $ARG2"
    ],
    "example*****denoising autoencoder": [
        "$ARG1 , MP-DBMs can be interpreted as a form of $ARG2"
    ],
    "denoising autoencoder*****missing inputs": [
        "$ARG1 , and are able to sample $ARG2"
    ],
    "mapping*****property": [
        "$ARG1 going from one state of the chain to the next ) should satisfy a $ARG2"
    ],
    "property*****markov chain": [
        "$ARG1 called detailed balance , which speci\ufb01es that a $ARG2"
    ],
    "image*****markov chain": [
        "$ARG1 ) and running the $ARG2",
        "$ARG1 and running the $ARG2"
    ],
    "markov chain*****other": [
        "$ARG1 on the $ARG2",
        "$ARG1 , one generates a new sequence for each layer , and that sequence is the input for computing $ARG2"
    ],
    "illustration*****image": [
        "$ARG1 of clamping the right half of the $ARG2"
    ],
    "convergence*****denoising autoencoder": [
        "$ARG1 of generative training of $ARG2"
    ],
    "stochastic*****markov chain": [
        "$ARG1 encode-decode steps ( as in the generative $ARG2"
    ],
    "markov chain*****example": [
        "$ARG1 ) initialized at a training $ARG2"
    ],
    "example*****contrastive divergence": [
        "$ARG1 ( just like with the $ARG2",
        "$ARG1 , $ARG2"
    ],
    "stochastic*****denoising autoencoder": [
        "$ARG1 networks or GSNs ( Bengio et al. , 2014 ) are generalizations of $ARG2"
    ],
    "conditional probability distribution*****markov chain": [
        "$ARG1 which specify one step of the $ARG2"
    ],
    "markov chain*****variable": [
        "$ARG1 : 1. p ( x ( k ) | h ( k ) ) tells how to generate the next visible $ARG2",
        "$ARG1 is really over the output $ARG2",
        "$ARG1 based on making local moves with a single x $ARG2"
    ],
    "distribution*****denoising autoencoder": [
        "$ARG1 \u201d is also found in $ARG2"
    ],
    "denoising autoencoder*****process": [
        "$ARG1 and GSNs di\ufb00er from classical probabilistic models ( directed or undirected ) in that they parametrize the generative $ARG2"
    ],
    "conditions*****distribution": [
        "$ARG1 for existence of the stationary $ARG2",
        "$ARG1 are : ( i ) The unnormalized $ARG2",
        "$ARG1 are : \u2022 The true $ARG2"
    ],
    "distribution*****conditions": [
        "$ARG1 are mild and are the same $ARG2"
    ],
    "conditions*****standard": [
        "$ARG1 required by $ARG2"
    ],
    "conditions*****example": [
        "$ARG1 are necessary to guarantee that the chain mixes , but they can be violated by some choices of the transition distributions ( for $ARG2"
    ],
    "example*****probability": [
        "$ARG1 and maximizing the $ARG2",
        "$ARG1 , some generative models are better at assigning high $ARG2",
        "$ARG1 , computing the $ARG2",
        "$ARG1 x ( typically the class with the highest $ARG2",
        "$ARG1 , if estimating the $ARG2",
        "$ARG1 , if we want to compute the $ARG2",
        "$ARG1 , the $ARG2"
    ],
    "protein*****markov chain": [
        "$ARG1 secondary structure ) and introduced a ( one-dimensional ) convolutional structure in the transition operator of the $ARG2"
    ],
    "variable*****hidden layer": [
        "$ARG1 ( and associated higherlevel $ARG2"
    ],
    "hidden layer*****back-propagation": [
        "$ARG1 ) , and the input sequence only serves to condition that chain , with $ARG2"
    ],
    "back-propagation*****distribution": [
        "$ARG1 allowing to learn how the input sequence can condition the output $ARG2"
    ],
    "classi\ufb01cation*****ancestral sampling": [
        "$ARG1 performance using this The methods we have described so far use either MCMC sampling , $ARG2"
    ],
    "process*****probability distribution": [
        "$ARG1 that incrementally changes the $ARG2"
    ],
    "model*****process": [
        "$ARG1 , we can run the $ARG2",
        "$ARG1 describes the data generation $ARG2",
        "$ARG1 , while \u201c sparse modeling \u201d refers to the $ARG2",
        "$ARG1 Require : x , the input to $ARG2",
        "$ARG1 family treats acoustic waveforms as being generated by the following $ARG2",
        "$ARG1 capacity ( by storing the frequencies of very many tuples ) while requiring very little computation to $ARG2",
        "$ARG1 family being estimated includes the generating $ARG2",
        "$ARG1 recovered by the training $ARG2"
    ],
    "interpretation*****denoising autoencoder": [
        "$ARG1 of the $ARG2"
    ],
    "moment matching*****model": [
        "$ARG1 because it modi\ufb01es the samples themselves , rather than training the $ARG2"
    ],
    "task*****precision": [
        "$ARG1 of interest , e.g. , based on ranking test examples and ranking criteria such as $ARG2"
    ],
    "other*****machine learning": [
        "$ARG1 \ufb01elds of $ARG2",
        "$ARG1 target audience is software engineers who do not have a $ARG2",
        "$ARG1 \ufb01elds of $ARG2",
        "$ARG1 words , in some sense , no $ARG2",
        "$ARG1 approaches to $ARG2",
        "$ARG1 purpose not yet anticipated by the $ARG2"
    ],
    "machine learning*****preprocessing": [
        "$ARG1 usually allow for some variation in the $ARG2"
    ],
    "example*****accuracy": [
        "$ARG1 , when comparing the $ARG2"
    ],
    "accuracy*****object recognition": [
        "$ARG1 of $ARG2"
    ],
    "object recognition*****algorithm": [
        "$ARG1 algorithms , it is usually acceptable to preprocess the input images slightly di\ufb00erently for each $ARG2"
    ],
    "distribution*****task": [
        "$ARG1 to be captured and fundamentally alters the $ARG2",
        "$ARG1 ) can sometimes be useful for another $ARG2",
        "$ARG1 p ( y | x , T ) where T is a description of the $ARG2",
        "$ARG1 due to di\ufb00erent domains , temporal non-stationarity , or changes in the nature of the $ARG2",
        "$ARG1 to solve the missing value imputation $ARG2"
    ],
    "preprocessing*****dataset": [
        "$ARG1 commonly arise when benchmarking generative models on the MNIST $ARG2"
    ],
    "example*****1": [
        "$ARG1 , we might binarize a gray pixel to 0 or $ARG2",
        "$ARG1 and f ( $ARG2",
        "$ARG1 , each user ) , called x ( $ARG2",
        "$ARG1 , the representation value [ $ARG2",
        "$ARG1 , we might have three functions f ( $ARG2",
        "$ARG1 , we apply the recti\ufb01ed linear transformation : \uf8ef $ARG2",
        "$ARG1 is intended to further understanding by showing a simpli\ufb01ed case where all variables are scalars , and we wish to compute the derivatives with respect to u ( $ARG2",
        "$ARG1 , if we unfold equation 10.1 for \u03c4 = 3 time steps , we obtain s ( 3 ) =f ( s ( 2 ) ; \u03b8 ) =f ( f ( s ( $ARG2",
        "$ARG1 , if L ( t ) is the negative log-likelihood of y ( t ) given x ( $ARG2",
        "$ARG1 of one second-order term arising from this update is \ue00f2g $ARG2",
        "$ARG1 , node ( $ARG2",
        "$ARG1 is 0 if it is correctly classi\ufb01ed and $ARG2",
        "$ARG1 to the child node on the left ( 0 ) or or the child node on the right ( $ARG2",
        "$ARG1 from section 16.1 , suppose we name Alice \u2019 s \ufb01nishing time t 0 , Bob \u2019 s \ufb01nishing time t $ARG2",
        "$ARG1 , it is additionally assumed that top-level hidden units h ( $ARG2",
        "$ARG1 , it is possible to construct two sets S1 and S 2 such that p ( x \u2208 S1 ) + p ( x \u2208 S2 ) > $ARG2",
        "$ARG1 , { $ARG2"
    ],
    "1*****probability": [
        "$ARG1 by thresholding at 0.5 , or by drawing a random sample whose $ARG2",
        "$ARG1 parameters are necessary ; the $ARG2",
        "$ARG1 , b1 ( w4 ) = 0 ) , and the $ARG2",
        "$ARG1 , then we need only specify the $ARG2",
        "$ARG1 determines the $ARG2",
        "$ARG1 ) and interpret that value as a $ARG2",
        "$ARG1 ) , then iterate through the table , adding up the $ARG2",
        "$ARG1 , since the $ARG2",
        "$ARG1 indicating that x = x is certain and a $ARG2",
        "$ARG1 ] , which gives the $ARG2",
        "$ARG1 ] k\u22121 , where pi gives the $ARG2",
        "$ARG1 x\u22650 to assign $ARG2",
        "$ARG1 like a $ARG2",
        "$ARG1 | x ) , estimating the $ARG2"
    ],
    "dataset*****example": [
        "$ARG1 once , or we might draw a di\ufb00erent random $ARG2",
        "$ARG1 containing features , but each $ARG2",
        "$ARG1 with only one $ARG2"
    ],
    "example*****multiple": [
        "$ARG1 for each step of training and then draw $ARG2",
        "$ARG1 if t is a $ARG2"
    ],
    "set*****distance": [
        "$ARG1 , according to Euclidean $ARG2",
        "$ARG1 neighbors at $ARG2"
    ],
    "human*****image": [
        "$ARG1 observer would judge each individual $ARG2"
    ],
    "image*****quality": [
        "$ARG1 of a dog to be high $ARG2"
    ],
    "example*****human": [
        "$ARG1 , it would be easy for a $ARG2"
    ],
    "model*****number": [
        "$ARG1 trained on data with tens of thousands of modes may ignore a small $ARG2",
        "$ARG1 can visit is exponentially large in the $ARG2",
        "$ARG1 if depth is restricted to be less than or equal to d. In many cases , the $ARG2",
        "$ARG1 , which could easily $ARG2",
        "$ARG1 is essentially guaranteed to have an extremely large $ARG2",
        "$ARG1 to generalize to a $ARG2",
        "$ARG1 counters this curse by relating each training sentence to an exponential $ARG2",
        "$ARG1 \u2019 s capacity : by changing the $ARG2",
        "$ARG1 capacity grows but shrinks as the $ARG2",
        "$ARG1 has an astronomical $ARG2",
        "$ARG1 of the right size , with the right $ARG2"
    ],
    "number*****human": [
        "$ARG1 of modes , and a $ARG2",
        "$ARG1 of Gibbs steps , the $ARG2",
        "$ARG1 of neurons as the $ARG2"
    ],
    "quality*****model": [
        "$ARG1 of samples is not a reliable guide , we often also evaluate the log-likelihood that the $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "measure*****model": [
        "$ARG1 any attribute of the $ARG2"
    ],
    "example*****variance": [
        "$ARG1 , real-valued models of MNIST can obtain arbitrarily high likelihood by assigning arbitrarily low $ARG2",
        "$ARG1 , we may wish to learn the $ARG2",
        "$ARG1 , parameters encoding the conditional $ARG2",
        "$ARG1 : Estimators of the $ARG2",
        "$ARG1 , we compare two di\ufb00erent estimators of the $ARG2",
        "$ARG1 , we assume that the $ARG2",
        "$ARG1 has more $ARG2"
    ],
    "variance*****change": [
        "$ARG1 to background pixels that never $ARG2",
        "$ARG1 and the normalization step repeatedly undid this $ARG2"
    ],
    "potential*****maximum likelihood": [
        "$ARG1 to achieve a cost approaching negative in\ufb01nity is present for any kind of $ARG2"
    ],
    "undirected graphical model*****unnormalized probability distribution": [
        "$ARG1 ) are de\ufb01ned by an $ARG2"
    ],
    "partition function*****probability distribution": [
        "$ARG1 Z ( \u03b8 ) in order to obtain a valid $ARG2"
    ],
    "partition function*****integral": [
        "$ARG1 is an $ARG2"
    ],
    "integral*****probability": [
        "$ARG1 ( for continuous variables ) or sum ( for discrete variables ) over the unnormalized $ARG2",
        "$ARG1 is of course just an application of the laws of $ARG2"
    ],
    "deep learning*****constant": [
        "$ARG1 models are designed to have a tractable normalizing $ARG2"
    ],
    "other*****challenge": [
        "$ARG1 models directly confront the $ARG2"
    ],
    "partition function*****learning": [
        "$ARG1 What makes $ARG2",
        "$ARG1 at every iteration of $ARG2"
    ],
    "undirected model*****maximum likelihood": [
        "$ARG1 by $ARG2"
    ],
    "maximum likelihood*****partition function": [
        "$ARG1 particularly di\ufb03cult is that the $ARG2"
    ],
    "positive phase*****negative phase": [
        "$ARG1 and $ARG2",
        "$ARG1 and di\ufb03cult $ARG2",
        "$ARG1 and the $ARG2",
        "$ARG1 as pushing down on the energy of training examples and the $ARG2",
        "$ARG1 \u201d and \u201c $ARG2",
        "$ARG1 has the same chance to push up at a point as the $ARG2",
        "$ARG1 term ( pushing a y up ) while the second term is the $ARG2"
    ],
    "negative phase*****learning": [
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2"
    ],
    "undirected model*****negative phase": [
        "$ARG1 of interest , the $ARG2"
    ],
    "model*****positive phase": [
        "$ARG1 with a straightforward $ARG2"
    ],
    "negative phase*****other": [
        "$ARG1 is the RBM , which has hidden units that are conditionally independent from each $ARG2"
    ],
    "derivation*****rule": [
        "$ARG1 , we use Leibniz \u2019 s $ARG2"
    ],
    "rule*****integral": [
        "$ARG1 for di\ufb00erentiation under the $ARG2"
    ],
    "identity*****conditions": [
        "$ARG1 is applicable only under certain regularity $ARG2"
    ],
    "measure*****conditions": [
        "$ARG1 theoretic terms , the $ARG2"
    ],
    "function*****gradient": [
        "$ARG1 of x for every value of \u03b8 ; ( ii ) The $ARG2",
        "$ARG1 does not shrink the $ARG2",
        "$ARG1 of the inner $ARG2",
        "$ARG1 of parameters and therefore does not contribute to the $ARG2"
    ],
    "gradient*****almost all": [
        "$ARG1 \u2207 \u03b8 p\u0303 ( x ) must exist for all \u03b8 and $ARG2"
    ],
    "almost all*****function": [
        "$ARG1 x ; ( iii ) There must exist an integrable $ARG2"
    ],
    "function*****almost all": [
        "$ARG1 R ( x ) that bounds \u2207 \u03b8p\u0303 ( x ) in the sense that maxi | \u2202\u03b8\u2202 i p\u0303 ( x ) | \u2264 R ( x ) for all \u03b8 and $ARG2"
    ],
    "almost all*****machine learning": [
        "$ARG1 x. Fortunately , most $ARG2"
    ],
    "machine learning*****partition function": [
        "$ARG1 models of interest have \u2207 \u03b8 log Z = Ex\u223cp ( x ) \u2207\u03b8 log p\u0303 ( x ) is the basis for a variety of Monte Carlo methods for approximately maximizing the likelihood of models with intractable $ARG2"
    ],
    "undirected model*****positive phase": [
        "$ARG1 provides an intuitive framework in which we can think of both the $ARG2"
    ],
    "negative phase*****partition function": [
        "$ARG1 , we decrease the $ARG2"
    ],
    "partition function*****model": [
        "$ARG1 by decreasing log p\u0303 ( x ) drawn from the $ARG2",
        "$ARG1 initialized from a random starting point after the $ARG2",
        "$ARG1 NCE is thus simple to apply so long as log p\u0303 $ARG2",
        "$ARG1 Noise contrastive estimation is based on the idea that a good generative $ARG2",
        "$ARG1 We can thus determine whether M A is a better $ARG2",
        "$ARG1 of either $ARG2",
        "$ARG1 with respect to the $ARG2"
    ],
    "deep learning*****energy function": [
        "$ARG1 literature , it is common to parametrize log p\u0303 in terms of an $ARG2"
    ],
    "negative phase*****model": [
        "$ARG1 as pushing up on the energy of samples drawn from the $ARG2",
        "$ARG1 involves drawing samples from the $ARG2",
        "$ARG1 , we sample points from the $ARG2"
    ],
    "model*****stochastic maximum likelihood": [
        "$ARG1 , as illustrated in $ARG2"
    ],
    "stochastic maximum likelihood*****set": [
        "$ARG1 and Contrastive The naive way of implementing equation 18.15 is to compute it by burning in a $ARG2"
    ],
    "set*****markov chain": [
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2"
    ],
    "markov chain*****initialization": [
        "$ARG1 from a random $ARG2",
        "$ARG1 from a random $ARG2"
    ],
    "initialization*****gradient": [
        "$ARG1 every time the $ARG2"
    ],
    "learning*****stochastic gradient descent": [
        "$ARG1 is performed using $ARG2"
    ],
    "stochastic gradient descent*****gradient": [
        "$ARG1 , this means the chains must be burned in once per $ARG2",
        "$ARG1 is that it follows the $ARG2",
        "$ARG1 to follow the $ARG2",
        "$ARG1 is that the $ARG2"
    ],
    "partition function*****algorithm": [
        "$ARG1 training procedure presented in $ARG2"
    ],
    "markov chain*****loop": [
        "$ARG1 in the inner $ARG2"
    ],
    "loop*****other": [
        "$ARG1 makes this procedure computationally infeasible , but this procedure is the starting point that $ARG2"
    ],
    "other*****algorithm": [
        "$ARG1 more practical algorithms aim $ARG2",
        "$ARG1 hand , the BFGS $ARG2",
        "$ARG1 variants of the EM $ARG2"
    ],
    "algorithm*****partition function": [
        "$ARG1 for maximizing the log-likelihood with an intractable $ARG2"
    ],
    "partition function*****gradient": [
        "$ARG1 using $ARG2",
        "$ARG1 and its $ARG2",
        "$ARG1 ( computing it or its $ARG2"
    ],
    "minibatch*****1": [
        "$ARG1 of m examples { x ( $ARG2",
        "$ARG1 of m examples { x ( $ARG2",
        "$ARG1 of m examples { x ( $ARG2",
        "$ARG1 , while u ( $ARG2",
        "$ARG1 of examples { x ( $ARG2",
        "$ARG1 of examples B = { x ( $ARG2",
        "$ARG1 to represent something like $ARG2"
    ],
    "normal distribution*****distribution": [
        "$ARG1 , or possibly a $ARG2",
        "$ARG1 , or possibly a $ARG2",
        "$ARG1 when we do not know the true $ARG2"
    ],
    "view*****maximum likelihood": [
        "$ARG1 the MCMC approach to $ARG2",
        "$ARG1 , regularized $ARG2",
        "$ARG1 of sparsity as resulting from the e\ufb00ect of pmodel ( h ) on approximate $ARG2"
    ],
    "negative phase*****probability": [
        "$ARG1 acts to reduce the $ARG2",
        "$ARG1 is that it fails to suppress regions of high $ARG2"
    ],
    "partition function*****view": [
        "$ARG1 pmodel ( x ) pdata ( x ) p ( x ) p ( x ) pmodel ( x ) pdata ( x ) Figure 18.1 : The $ARG2"
    ],
    "view*****algorithm": [
        "$ARG1 of $ARG2",
        "$ARG1 the description of the $ARG2",
        "$ARG1 the momentum $ARG2",
        "$ARG1 on the EM $ARG2"
    ],
    "algorithm*****positive phase": [
        "$ARG1 18.1 as having a \u201c $ARG2"
    ],
    "positive phase*****distribution": [
        "$ARG1 , we sample points from the data $ARG2"
    ],
    "distribution*****probability": [
        "$ARG1 , and push up on their unnormalized $ARG2",
        "$ARG1 , and push down on their unnormalized $ARG2",
        "$ARG1 T ( x\ue030 | x ) specifying the $ARG2",
        "$ARG1 has sharp peaks of high $ARG2",
        "$ARG1 over discrete values with a tabular representation\u2014an array containing a separate entry for each possible assignment of values , with the value of that entry giving the $ARG2",
        "$ARG1 q\u0303 ( hi | v ) = exp Eh\u2212i \u223cq ( h\u2212i |v ) log p\u0303 ( v , h ) so long as p does not assign 0 $ARG2",
        "$ARG1 is as a component of an empirical p\u0302 ( x ) = \u03b4 ( x \u2212 x ( i ) ) which puts $ARG2",
        "$ARG1 places high $ARG2",
        "$ARG1 places low $ARG2"
    ],
    "positive phase*****constant": [
        "$ARG1 \u2019 s tendency to just add a large $ARG2"
    ],
    "constant*****probability": [
        "$ARG1 to the unnormalized $ARG2",
        "$ARG1 of a $ARG2"
    ],
    "distribution*****positive phase": [
        "$ARG1 are equal , the $ARG2"
    ],
    "gradient*****expectation": [
        "$ARG1 ( in $ARG2",
        "$ARG1 is an $ARG2"
    ],
    "gradient*****model": [
        "$ARG1 of log p\u0303 to minimize log Z while sleeping and experiencing events sampled from the current $ARG2",
        "$ARG1 step , the chains are free to wander far enough to \ufb01nd all of the $ARG2",
        "$ARG1 on the $ARG2",
        "$ARG1 of the output of the $ARG2",
        "$ARG1 whenever the $ARG2",
        "$ARG1 through such a $ARG2",
        "$ARG1 provides a good $ARG2",
        "$ARG1 of J\u02dcW with respect to the $ARG2",
        "$ARG1 , we can build a $ARG2"
    ],
    "view*****negative phase": [
        "$ARG1 explains much of the language used to describe algorithms with a positive and $ARG2"
    ],
    "machine learning*****negative phase": [
        "$ARG1 models , it is usually necessary to use the positive and $ARG2"
    ],
    "negative phase*****time period": [
        "$ARG1 simultaneously , rather than in separate $ARG2"
    ],
    "machine learning*****model": [
        "$ARG1 algorithms draw samples from the $ARG2",
        "$ARG1 that it is often much easier to design a $ARG2",
        "$ARG1 researchers neglect these implementation details , but when the performance of an implementation restricts the size of the $ARG2",
        "$ARG1 BASICS b , one can continue to use the $ARG2",
        "$ARG1 is that we must perform well on new , previously unseen inputs\u2014not just those on which our $ARG2",
        "$ARG1 BASICS More generally , we can regularize a $ARG2",
        "$ARG1 systems , practitioners need to decide whether to gather more data , increase or decrease $ARG2"
    ],
    "other*****function": [
        "$ARG1 purposes and such algorithms could also provide an account for the $ARG2",
        "$ARG1 units are able to participate in a $ARG2",
        "$ARG1 is a cosine $ARG2"
    ],
    "function*****dream sleep": [
        "$ARG1 of $ARG2"
    ],
    "learning*****algorithm": [
        "$ARG1 , we can attempt to design a less expensive alternative to $ARG2",
        "$ARG1 problems we ask the $ARG2"
    ],
    "algorithm*****markov chain": [
        "$ARG1 is the cost of burning in the $ARG2",
        "$ARG1 initializes the $ARG2"
    ],
    "solution*****markov chain": [
        "$ARG1 is to initialize the $ARG2"
    ],
    "partition function*****operation": [
        "$ARG1 so that the burn in $ARG2"
    ],
    "distribution*****data set": [
        "$ARG1 is free , because they are already available in the $ARG2"
    ],
    "distribution*****negative phase": [
        "$ARG1 , so the $ARG2",
        "$ARG1 , and the $ARG2"
    ],
    "positive phase*****model": [
        "$ARG1 can still accurately increase the $ARG2",
        "$ARG1 has had some time to act , the $ARG2"
    ],
    "algorithm*****contrastive divergence": [
        "$ARG1 18.2 The $ARG2"
    ],
    "gradient*****optimization": [
        "$ARG1 ascent as the $ARG2",
        "$ARG1 ascent as the $ARG2"
    ],
    "probability*****data generating distribution": [
        "$ARG1 under the $ARG2"
    ],
    "partition function*****illustration": [
        "$ARG1 pmodel ( x ) p ( x ) pdata ( x ) Figure 18.2 : An $ARG2"
    ],
    "illustration*****negative phase": [
        "$ARG1 of how the $ARG2"
    ],
    "negative phase*****contrastive divergence": [
        "$ARG1 of $ARG2",
        "$ARG1 , it can be viewed as a version of $ARG2"
    ],
    "markov chain*****model": [
        "$ARG1 for only a few steps , it is unlikely to visit modes in the $ARG2",
        "$ARG1 to approximately sample from p $ARG2",
        "$ARG1 to sample from p $ARG2",
        "$ARG1 sampling from the $ARG2"
    ],
    "concept*****number": [
        "$ARG1 of distance\u2014the spurious mode is far from the correct mode along the $ARG2"
    ],
    "variable*****markov chain": [
        "$ARG1 in R. For most deep probabilistic models , the $ARG2"
    ],
    "markov chain*****gibbs sampling": [
        "$ARG1 are based on $ARG2",
        "$ARG1 in this case is not $ARG2",
        "$ARG1 that samples from pmodel ( x ) is to use $ARG2"
    ],
    "estimator*****boltzmann machine": [
        "$ARG1 is biased for RBMs and fully visible $ARG2"
    ],
    "gradient*****bias": [
        "$ARG1 , which explains the $ARG2",
        "$ARG1 , only approximately , with $ARG2"
    ],
    "algorithm*****model": [
        "$ARG1 can be thought of as penalizing the $ARG2",
        "$ARG1 are small , then the $ARG2",
        "$ARG1 can move the $ARG2",
        "$ARG1 or what $ARG2",
        "$ARG1 applied to a deterministic cost and $ARG2",
        "$ARG1 that we have discussed concretely is to increase or decrease the $ARG2",
        "$ARG1 is free to invent the concepts it needs to $ARG2",
        "$ARG1 tends to adapt the $ARG2",
        "$ARG1 alternates between minimization with respect to h and minimization with respect to the $ARG2",
        "$ARG1 is the \u201c right \u201d $ARG2",
        "$ARG1 7.2 ) is to initialize the $ARG2",
        "$ARG1 then trains a $ARG2"
    ],
    "other*****pretraining": [
        "$ARG1 training methods , it can be useful for $ARG2"
    ],
    "gradient*****function": [
        "$ARG1 of any $ARG2",
        "$ARG1 \u2207x f ( x , y ) for an arbitrary $ARG2",
        "$ARG1 of a $ARG2",
        "$ARG1 at step t is a $ARG2",
        "$ARG1 of such a $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of any $ARG2",
        "$ARG1 or Jacobian of a vector-valued $ARG2",
        "$ARG1 by using complex numbers as input to the $ARG2"
    ],
    "markov chain*****gradient": [
        "$ARG1 at each $ARG2"
    ],
    "stochastic maximum likelihood*****applied mathematics": [
        "$ARG1 ( SML ) in the $ARG2"
    ],
    "applied mathematics*****statistics": [
        "$ARG1 and $ARG2"
    ],
    "statistics*****persistent contrastive divergence": [
        "$ARG1 community ( Younes , 1998 ) and later independently rediscovered under the name $ARG2"
    ],
    "persistent contrastive divergence*****deep learning": [
        "$ARG1 ( PCD , or PCD-k to indicate the use of k Gibbs steps per update ) in the $ARG2"
    ],
    "markov chain*****learning": [
        "$ARG1 is continually updated throughout the $ARG2",
        "$ARG1 to mix rapidly , though this e\ufb00ect only occurs during $ARG2"
    ],
    "set*****svm": [
        "$ARG1 log-likelihood for an RBM , and that if the RBM \u2019 s hidden units are used as features for an $ARG2"
    ],
    "svm*****classi\ufb01cation": [
        "$ARG1 classi\ufb01er , SML results in the best $ARG2"
    ],
    "learning rate*****number": [
        "$ARG1 is too high for the $ARG2"
    ],
    "human*****variance": [
        "$ARG1 operator will be able to observe that there is much more $ARG2"
    ],
    "variance*****negative phase": [
        "$ARG1 in the $ARG2",
        "$ARG1 is its use of the same training points in both the positive and $ARG2"
    ],
    "negative phase*****gradient": [
        "$ARG1 samples across $ARG2",
        "$ARG1 samples that Monte Carlo training algorithms use to approximate the negative $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "gradient*****markov chain": [
        "$ARG1 steps rather than across di\ufb00erent $ARG2"
    ],
    "stochastic maximum likelihood*****persistent contrastive divergence": [
        "$ARG1 / $ARG2"
    ],
    "1*****image": [
        "$ARG1 for RBM on a small $ARG2",
        "$ARG1 in the $ARG2"
    ],
    "negative phase*****variance": [
        "$ARG1 is initialized from di\ufb00erent training points , the $ARG2"
    ],
    "estimator*****model": [
        "$ARG1 based on All of these methods based on using MCMC to draw samples from the $ARG2",
        "$ARG1 , this is guaranteed to happen so long as the $ARG2",
        "$ARG1 for \u03b8 is then de\ufb01ned as \u03b8ML = arg max p $ARG2"
    ],
    "model*****principle": [
        "$ARG1 can in $ARG2",
        "$ARG1 can , in $ARG2",
        "$ARG1 , but in $ARG2"
    ],
    "learning rate*****negative phase": [
        "$ARG1 , allowing it to adapt rapidly in response to the $ARG2"
    ],
    "learning*****markov chain": [
        "$ARG1 and push the $ARG2"
    ],
    "learning*****weights": [
        "$ARG1 while the fast $ARG2",
        "$ARG1 the $ARG2",
        "$ARG1 provides many ways of combining the ensemble members \u2019 predictions , including uniform weighting and $ARG2"
    ],
    "weight decay*****weights": [
        "$ARG1 to the fast $ARG2",
        "$ARG1 that depend directly on the $ARG2",
        "$ARG1 but with a separate KKT multiplier for the $ARG2"
    ],
    "weights*****markov chain": [
        "$ARG1 , encouraging them to converge to small values , after only transiently taking on large values long enough to encourage the $ARG2"
    ],
    "markov chain*****change": [
        "$ARG1 to $ARG2",
        "$ARG1 ( although transition operator does $ARG2"
    ],
    "gradient*****problem": [
        "$ARG1 of log Z , and thus we can essentially decompose the $ARG2",
        "$ARG1 becomes a $ARG2",
        "$ARG1 does not shrink to insigni\ufb01cant size , the $ARG2"
    ],
    "method*****negative phase": [
        "$ARG1 to tackle log p\u0303 ( x ) , and just add our $ARG2"
    ],
    "method*****gradient": [
        "$ARG1 \u2019 s $ARG2",
        "$ARG1 for computing the $ARG2",
        "$ARG1 to request the $ARG2",
        "$ARG1 to request the $ARG2",
        "$ARG1 and specifying that the desired $ARG2",
        "$ARG1 has the advantage that it guarantees that each step is still in the $ARG2",
        "$ARG1 , however , is designed to solve for a point where the $ARG2",
        "$ARG1 , which simply consists of simulating the dynamics de\ufb01ned by the equation by taking small , \ufb01nite steps in the direction of each $ARG2",
        "$ARG1 In section 4.3 , we introduced second-order $ARG2",
        "$ARG1 can be made more e\ufb03cient by projecting the $ARG2",
        "$ARG1 , then a common source of error is implementing this $ARG2"
    ],
    "partition function*****positive phase": [
        "$ARG1 incompatible with bound-based $ARG2"
    ],
    "model*****partition function": [
        "$ARG1 without computing the $ARG2",
        "$ARG1 than M B without knowing the $ARG2",
        "$ARG1 with a known $ARG2",
        "$ARG1 \u2019 s $ARG2"
    ],
    "partition function*****pseudolikelihood": [
        "$ARG1 appears in both the numerator and the denominator of the ratio and cancels out : p ( x ) p ( y ) Z p\u0303 ( x ) Z p\u0303 ( y ) p\u0303 ( x ) p\u0303 ( y ) The $ARG2",
        "$ARG1 using the same trick as the $ARG2"
    ],
    "pseudolikelihood*****partition function": [
        "$ARG1 is based on the observation that conditional probabilities take this ratio-based form , and thus can be computed without knowledge of the $ARG2",
        "$ARG1 avoid computing quantities related to the $ARG2"
    ],
    "contains*****distribution": [
        "$ARG1 the variables we want to \ufb01nd the conditional $ARG2"
    ],
    "distribution*****contains": [
        "$ARG1 over , b $ARG2"
    ],
    "variable*****operation": [
        "$ARG1 and c can be empty , making this $ARG2",
        "$ARG1 y is computed by applying an $ARG2",
        "$ARG1 , but it is possible to apply more than one $ARG2",
        "$ARG1 V is associated with the following subroutines : \u2022 get_operation ( V ) : This returns the $ARG2"
    ],
    "chain rule of probability*****1": [
        "$ARG1 , log p ( x ) = log p ( x $ARG2"
    ],
    "pseudolikelihood*****objective function": [
        "$ARG1 ( Besag , 1975 ) $ARG2"
    ],
    "objective function*****feature": [
        "$ARG1 , based on predicting the value of $ARG2"
    ],
    "feature*****other": [
        "$ARG1 x i given all of the $ARG2",
        "$ARG1 vectors fx ( xtest ) and fy ( ytest ) have been related to each $ARG2"
    ],
    "partition function*****random variable": [
        "$ARG1 If each $ARG2"
    ],
    "random variable*****pseudolikelihood": [
        "$ARG1 has k di\ufb00erent values , this requires only k \u00d7n evaluations of p\u0303 to compute , as opposed to the kn evaluations needed to compute the partition This may look like an unprincipled hack , but it can be proven that estimation by maximizing the $ARG2"
    ],
    "pseudolikelihood*****maximum likelihood": [
        "$ARG1 may display di\ufb00erent behavior from the $ARG2"
    ],
    "maximum likelihood*****pseudolikelihood": [
        "$ARG1 behavior by using the generalized $ARG2"
    ],
    "estimator*****1": [
        "$ARG1 uses m di\ufb00erent sets S ( i ) , i = $ARG2",
        "$ARG1 , Z\u0302 $ARG2",
        "$ARG1 is biased because E [ s\u0302 BIS ] = \ue036 s , except asymptotically when n \u2192 \u221e and the denominator of equation 17.14 converges to $ARG2",
        "$ARG1 of the Gaussian mean parameter is known as the sample $ARG2",
        "$ARG1 \u03b8\u0302m = m $ARG2",
        "$ARG1 : \u03b8\u0302 = x ( $ARG2"
    ],
    "pseudolikelihood*****model": [
        "$ARG1 tends to perform poorly on tasks that require a good $ARG2"
    ],
    "model*****density estimation": [
        "$ARG1 of the full joint p ( x ) , such as $ARG2"
    ],
    "pseudolikelihood*****correlation": [
        "$ARG1 techniques are especially powerful if the data has regular structure that allows the S index sets to be designed to capture the most important correlations while leaving out groups of variables that only have negligible $ARG2"
    ],
    "example*****natural image": [
        "$ARG1 , in $ARG2",
        "$ARG1 , we would like AI algorithms to be able to understand $ARG2"
    ],
    "natural image*****correlation": [
        "$ARG1 , pixels that are widely separated in space also have weak $ARG2"
    ],
    "correlation*****pseudolikelihood": [
        "$ARG1 , so the generalized $ARG2"
    ],
    "pseudolikelihood*****set": [
        "$ARG1 can be applied with each S $ARG2"
    ],
    "estimator*****other": [
        "$ARG1 is that it can not be used with $ARG2"
    ],
    "pseudolikelihood*****deep boltzmann machine": [
        "$ARG1 approaches to deep models such as $ARG2"
    ],
    "partition function*****other": [
        "$ARG1 that interact with each $ARG2",
        "$ARG1 , r = Z ( \u03b8 Z ( \u03b8 A ) and we knew the actual value of just one of the two , say Z ( \u03b8A ) , we could compute the value of the $ARG2",
        "$ARG1 , but there can be several $ARG2"
    ],
    "pseudolikelihood*****deep learning": [
        "$ARG1 is still useful for $ARG2"
    ],
    "deep learning*****approximate inference": [
        "$ARG1 , because it can be used to train single layer models , or deep models using $ARG2"
    ],
    "pseudolikelihood*****gradient": [
        "$ARG1 has a much greater cost per $ARG2"
    ],
    "pseudolikelihood*****example": [
        "$ARG1 and similar criteria can still perform well if only one randomly selected conditional is computed per $ARG2"
    ],
    "estimator*****negative phase": [
        "$ARG1 does not explicitly minimize log Z , it can still be thought of as having something resembling a $ARG2"
    ],
    "distribution*****learning": [
        "$ARG1 result in the $ARG2",
        "$ARG1 , $ARG2",
        "$ARG1 for our q family to capture , then the $ARG2"
    ],
    "algorithm*****probability": [
        "$ARG1 suppressing the $ARG2"
    ],
    "probability*****variable": [
        "$ARG1 of all states that have only one $ARG2",
        "$ARG1 that a binary $ARG2"
    ],
    "variable*****example": [
        "$ARG1 di\ufb00ering from a training $ARG2",
        "$ARG1 di\ufb00erent from a training $ARG2",
        "$ARG1 size as well as the input , for $ARG2"
    ],
    "score matching*****matching": [
        "$ARG1 and Ratio $ARG2",
        "$ARG1 to discrete data is ratio $ARG2"
    ],
    "score matching*****model": [
        "$ARG1 ( Hyv\u00e4rinen , 2005 ) provides another consistent means of training a $ARG2"
    ],
    "score matching*****expected": [
        "$ARG1 is to minimize the $ARG2"
    ],
    "expected*****model": [
        "$ARG1 squared di\ufb00erence between the derivatives of the $ARG2",
        "$ARG1 reward and the $ARG2",
        "$ARG1 training error of a randomly selected $ARG2",
        "$ARG1 test error of that $ARG2"
    ],
    "model*****objective function": [
        "$ARG1 \u2019 s log density with respect to the input and the derivatives of the data \u2019 s log density with respect to the input : ||\u2207 x log pmodel ( x ; \u03b8 ) \u2212 \u2207x log p data ( x ) ||22 J ( \u03b8 ) = E pdata ( x ) L ( x , \u03b8 ) \u03b8 = min J ( \u03b8 ) L ( x , \u03b8 ) = This $ARG2",
        "$ARG1 , so we could design an $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 has the following total $ARG2"
    ],
    "objective function*****partition function": [
        "$ARG1 avoids the di\ufb03culties associated with di\ufb00erentiating the $ARG2"
    ],
    "partition function*****function": [
        "$ARG1 Z because Z is not a $ARG2"
    ],
    "score matching*****distribution": [
        "$ARG1 appears to have a new di\ufb03culty : computing the score of the data $ARG2",
        "$ARG1 , by \ufb01tting a $ARG2",
        "$ARG1 with the smoothing $ARG2"
    ],
    "partition function*****expected value": [
        "$ARG1 equivalent to minimizing the $ARG2"
    ],
    "expected value*****model": [
        "$ARG1 of L\u0303 ( x , \u03b8 ) = log p $ARG2"
    ],
    "pseudolikelihood*****score matching": [
        "$ARG1 , $ARG2"
    ],
    "score matching*****second derivative": [
        "$ARG1 requires the derivatives and $ARG2"
    ],
    "second derivative*****information": [
        "$ARG1 of log p\u0303 ( x ) and a lower bound conveys no $ARG2"
    ],
    "score matching*****sparse coding": [
        "$ARG1 can not be applied to estimating models with complicated interactions between the hidden units , such as $ARG2"
    ],
    "sparse coding*****deep boltzmann machine": [
        "$ARG1 models or $ARG2"
    ],
    "score matching*****hidden layer": [
        "$ARG1 can be used to pretrain the \ufb01rst $ARG2"
    ],
    "hidden layer*****model": [
        "$ARG1 of a larger $ARG2",
        "$ARG1 determines the width of the $ARG2"
    ],
    "pretraining*****model": [
        "$ARG1 strategy for the deeper layers of a larger $ARG2",
        "$ARG1 became popular , it was understood as initializing the $ARG2",
        "$ARG1 initializes the $ARG2",
        "$ARG1 of a convolutional $ARG2"
    ],
    "hidden layer*****score matching": [
        "$ARG1 of such models usually contain some While $ARG2"
    ],
    "score matching*****negative phase": [
        "$ARG1 does not explicitly have a $ARG2"
    ],
    "gibbs sampling*****gradient": [
        "$ARG1 , but rather a di\ufb00erent approach that makes local moves guided by the $ARG2"
    ],
    "score matching*****markov chain": [
        "$ARG1 is equivalent to CD with this type of $ARG2"
    ],
    "score matching*****derivation": [
        "$ARG1 to the discrete case ( but made an error in their $ARG2"
    ],
    "score matching*****probability": [
        "$ARG1 ( GSM ) does not work in high dimensional discrete spaces where the observed $ARG2"
    ],
    "matching*****average": [
        "$ARG1 consists of minimizing the $ARG2",
        "$ARG1 some of the abilities of $ARG2"
    ],
    "average*****objective function": [
        "$ARG1 over examples of the following $ARG2"
    ],
    "objective function*****model": [
        "$ARG1 : L ( RM ) ( x , \u03b8 ) = pmodel ( x ; \u03b8 ) p $ARG2",
        "$ARG1 that we can not even approximate in a reasonable amount of time , but we are still able to approximately train the $ARG2"
    ],
    "matching*****partition function": [
        "$ARG1 avoids the $ARG2"
    ],
    "estimator*****partition function": [
        "$ARG1 : in a ratio of two probabilities , the $ARG2"
    ],
    "matching*****pseudolikelihood": [
        "$ARG1 outperforms SML , $ARG2"
    ],
    "pseudolikelihood*****matching": [
        "$ARG1 and GSM in terms of the ability of models trained with ratio $ARG2"
    ],
    "matching*****set": [
        "$ARG1 to denoise test $ARG2"
    ],
    "estimator*****matching": [
        "$ARG1 , ratio $ARG2",
        "$ARG1 , ratio $ARG2"
    ],
    "matching*****variable": [
        "$ARG1 can be thought of as pushing down on all fantasy states that have only one $ARG2"
    ],
    "matching*****distance": [
        "$ARG1 applies speci\ufb01cally to binary data , this means that it acts on all fantasy states within Hamming $ARG2"
    ],
    "challenge*****format": [
        "$ARG1 for MCMC-based methods because the data is extremely expensive to represent in dense $ARG2"
    ],
    "format*****model": [
        "$ARG1 , yet the MCMC sampler does not yield sparse values until the $ARG2"
    ],
    "stochastic*****matching": [
        "$ARG1 approximation to ratio $ARG2"
    ],
    "subset*****model": [
        "$ARG1 of the terms of the objective , and does not require the $ARG2"
    ],
    "model*****matching": [
        "$ARG1 to generate complete See Marlin and de Freitas ( 2011 ) for a theoretical analysis of the asymptotic e\ufb03ciency of ratio $ARG2"
    ],
    "distribution*****process": [
        "$ARG1 q ( x | y ) is a corruption $ARG2",
        "$ARG1 , then the sampling $ARG2"
    ],
    "denoising score matching*****empirical distribution": [
        "$ARG1 is especially useful because in practice we usually do not have access to the true pdata but rather only an $ARG2"
    ],
    "estimator*****set": [
        "$ARG1 will , given enough capacity , make pmodel into a $ARG2",
        "$ARG1 of the Mean Now , consider a $ARG2"
    ],
    "problem*****loss": [
        "$ARG1 , at the $ARG2"
    ],
    "loss*****consistency": [
        "$ARG1 of the asymptotic $ARG2"
    ],
    "recall*****score matching": [
        "$ARG1 from section 14.5.1 that several autoencoder training algorithms are equivalent to $ARG2"
    ],
    "score matching*****denoising score matching": [
        "$ARG1 or $ARG2"
    ],
    "score matching*****pseudolikelihood": [
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2"
    ],
    "1*****maximum likelihood": [
        "$ARG1 Such an approach would not be possible using $ARG2"
    ],
    "maximum likelihood*****estimator": [
        "$ARG1 as the criterion for the $ARG2",
        "$ARG1 is a consistent $ARG2",
        "$ARG1 is often considered the preferred $ARG2"
    ],
    "maximum likelihood*****set": [
        "$ARG1 criterion would choose to $ARG2",
        "$ARG1 estimate can be computed simply by counting how many times each possible n gram occurs in the training $ARG2"
    ],
    "problem*****learning": [
        "$ARG1 of estimating p ( x ) to that of $ARG2",
        "$ARG1 by also $ARG2",
        "$ARG1 by $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 is very di\ufb03cult to detect , because we can only know for sure that it happened if we have a superior $ARG2"
    ],
    "problem*****maximum likelihood estimation": [
        "$ARG1 is constructed in such a way that $ARG2"
    ],
    "maximum likelihood estimation*****partition function": [
        "$ARG1 in this supervised NCE is also applicable to problems with a tractable $ARG2"
    ],
    "problem*****estimator": [
        "$ARG1 de\ufb01nes an asymptotically consistent $ARG2"
    ],
    "other*****variable": [
        "$ARG1 words , y is a switch $ARG2",
        "$ARG1 words , so long as each $ARG2"
    ],
    "variable*****model": [
        "$ARG1 that determines whether we will generate x from the $ARG2",
        "$ARG1 P ( y ; \u03b8 ) can be reinterpreted as a $ARG2",
        "$ARG1 in a mixture of Gaussians $ARG2"
    ],
    "variable*****distribution": [
        "$ARG1 determines whether we draw x from the data or from the noise $ARG2",
        "$ARG1 de\ufb01ning such a $ARG2",
        "$ARG1 to de\ufb01ne a conditional $ARG2",
        "$ARG1 \ufb01rst , then use \u223c notation to specify which $ARG2",
        "$ARG1 and the $ARG2"
    ],
    "learning*****supervised learning": [
        "$ARG1 on the $ARG2"
    ],
    "distribution*****logistic regression model": [
        "$ARG1 pjoint is essentially a $ARG2"
    ],
    "logistic regression model*****model": [
        "$ARG1 applied to the di\ufb00erence in log probabilities of the $ARG2"
    ],
    "\u03c3*****model": [
        "$ARG1 \u2212 log p $ARG2",
        "$ARG1 \u2192 0 , the density $ARG2"
    ],
    "model*****\u03c3": [
        "$ARG1 ( x ) = $ARG2",
        "$ARG1 takes advantage of the observation that most variations in the data can be captured by the latent variables h , up to some small residual reconstruction error $ARG2"
    ],
    "random variable*****number": [
        "$ARG1 can take on a high $ARG2"
    ],
    "logistic regression*****variable": [
        "$ARG1 classi\ufb01er can reject a noise sample by identifying any one $ARG2"
    ],
    "learning*****statistics": [
        "$ARG1 slows down greatly after pmodel has learned the basic marginal $ARG2",
        "$ARG1 , but allows the relationships between units and the nonlinear $ARG2"
    ],
    "model*****gaussian noise": [
        "$ARG1 of images of faces , using unstructured $ARG2",
        "$ARG1 by adding $ARG2"
    ],
    "model*****almost all": [
        "$ARG1 learns about eyes , it can reject $ARG2"
    ],
    "almost all*****other": [
        "$ARG1 unstructured noise samples without having learned anything about $ARG2"
    ],
    "distribution*****gradient": [
        "$ARG1 before each $ARG2"
    ],
    "gradient*****expected": [
        "$ARG1 step , NCE de\ufb01nes a procedure called self-contrastive estimation , whose $ARG2",
        "$ARG1 is equivalent to the $ARG2"
    ],
    "special case*****model": [
        "$ARG1 of NCE where the noise samples are those generated by the $ARG2"
    ],
    "model*****maximum likelihood": [
        "$ARG1 suggests that $ARG2",
        "$ARG1 then proceeds as usual , using $ARG2",
        "$ARG1 with $ARG2"
    ],
    "energy function*****gradient": [
        "$ARG1 used in de\ufb01ning the classi\ufb01er ) to provide a $ARG2"
    ],
    "partition function*****undirected graphical model": [
        "$ARG1 Z ( \u03b8 ) associated with an $ARG2"
    ],
    "undirected graphical model*****partition function": [
        "$ARG1 , in this section we discuss several methods for directly estimating the $ARG2"
    ],
    "model*****turn": [
        "$ARG1 assigns to each point , which in $ARG2"
    ],
    "turn*****partition function": [
        "$ARG1 requires evaluating the $ARG2"
    ],
    "importance sampling*****probability": [
        "$ARG1 , provided that the two models If , however , we wanted to compute the actual $ARG2"
    ],
    "probability*****partition function": [
        "$ARG1 of the test data under either MA or MB , we would need to compute the actual value of the $ARG2"
    ],
    "method*****importance sampling": [
        "$ARG1 such as simple $ARG2",
        "$ARG1 that , like AIS , addresses the shortcomings of $ARG2"
    ],
    "partition function*****distribution": [
        "$ARG1 Z0 and the unnormalized $ARG2",
        "$ARG1 If the $ARG2",
        "$ARG1 of a multimodal $ARG2",
        "$ARG1 intermediate distributions , bridge sampling relies on a single $ARG2",
        "$ARG1 , p0 , and a $ARG2"
    ],
    "1*****integral": [
        "$ARG1 , of the $ARG2"
    ],
    "weights*****estimator": [
        "$ARG1 in this sum will result in an $ARG2"
    ],
    "estimator*****quality": [
        "$ARG1 that is of poor $ARG2"
    ],
    "quality*****variance": [
        "$ARG1 due to high $ARG2"
    ],
    "variance*****weights": [
        "$ARG1 of our estimate Z\u03021 : ( k ) ) \u02c6 Z\u03021 = 0 ( k ) ) This quantity is largest when there is signi\ufb01cant deviation in the values of the ( k ) ) importance $ARG2",
        "$ARG1 v \u2217 we may choose the individual $ARG2",
        "$ARG1 , which makes it shrink the $ARG2"
    ],
    "weights*****turn": [
        "$ARG1 p\u0303p\u03031 ( x ( x ( k ) ) We now $ARG2"
    ],
    "turn*****task": [
        "$ARG1 to two related strategies developed to cope with the challenging $ARG2"
    ],
    "task*****partition function": [
        "$ARG1 of estimating $ARG2"
    ],
    "partition function*****annealed importance sampling": [
        "$ARG1 for complex distributions over highdimensional spaces : $ARG2"
    ],
    "importance sampling*****problem": [
        "$ARG1 strategy introduced above and both attempt to overcome the $ARG2"
    ],
    "problem*****1": [
        "$ARG1 of the proposal p0 being too far from p $ARG2",
        "$ARG1 reduces to d\u2217 = arg min ||x ( i ) \u2212 dd\ue03ex ( i ) ||22 subject to ||d||2 = $ARG2",
        "$ARG1 as d \u2217 = arg min ||X \u2212 Xdd\ue03e||2F subject to d\ue03e d = $ARG2"
    ],
    "1*****annealed importance sampling": [
        "$ARG1 by introducing intermediate distributions that attempt to bridge the gap between p0 In situations where D KL ( p0\ue06bp1 ) is large ( i.e. , where there is little overlap between p0 and p1 ) , a strategy called $ARG2"
    ],
    "1*****partition function": [
        "$ARG1 so that the \ufb01rst and last distributions in the sequence are p0 and p1 This approach allows us to estimate the $ARG2",
        "$ARG1 are su\ufb03ciently close ) bridge sampling can be a more e\ufb00ective means of estimating the ratio of $ARG2"
    ],
    "partition function*****weights": [
        "$ARG1 ( such as an RBM with zeroes for $ARG2",
        "$ARG1 \u2013 Sample x \u03b72 \u223c T\u03b71 ( x\u03b72 | x\u03b71 ) \u2013 Sample x \u03b7n\u22121 \u223c T \u03b7n\u22122 ( x \u03b7n\u22121 | x \u03b7n\u22122 ) \u2013 Sample x \u03b7n \u223c T\u03b7n\u22121 ( x\u03b7n | x \u03b7n\u22121 ) For sample k , we can derive the importance weight by chaining together the importance $ARG2"
    ],
    "weights*****model": [
        "$ARG1 ) and estimate the ratio between the two $ARG2",
        "$ARG1 in the $ARG2",
        "$ARG1 \u03b1 ( t ) are produced by the $ARG2",
        "$ARG1 of a linear factor $ARG2",
        "$ARG1 by 2 at the end of training , and then using the $ARG2",
        "$ARG1 correctly , you may want to begin with a simple statistical $ARG2"
    ],
    "average*****distribution": [
        "$ARG1 of the target $ARG2",
        "$ARG1 to encode symbols drawn from a $ARG2"
    ],
    "partition function*****series": [
        "$ARG1 is known ) p0 : p\u03b7 j \u221d p 1j p0 In order to sample from these intermediate distributions , we de\ufb01ne a $ARG2"
    ],
    "series*****markov chain": [
        "$ARG1 of $ARG2"
    ],
    "markov chain*****conditional probability distribution": [
        "$ARG1 transition functions T\u03b7 j ( x \ue030 | x ) that de\ufb01ne the $ARG2"
    ],
    "invariant*****markov chain monte carlo": [
        "$ARG1 : p\u03b7j ( x ) = p\u03b7j ( x \ue030 ) T\u03b7j ( x | x \ue030 ) dx\ue030 These transitions may be constructed as any $ARG2"
    ],
    "method*****multiple": [
        "$ARG1 ( e.g. , Metropolis-Hastings , Gibbs ) , including methods involving $ARG2",
        "$ARG1 consists of applying equation 4.12 $ARG2"
    ],
    "multiple*****random variable": [
        "$ARG1 passes through all of the $ARG2"
    ],
    "random variable*****other": [
        "$ARG1 or $ARG2",
        "$ARG1 corresponding to those nodes interact with each $ARG2"
    ],
    "weights*****partition function": [
        "$ARG1 given in equation 18.52 , the estimate of the ratio of $ARG2"
    ],
    "1*****importance sampling": [
        "$ARG1 \ue058 ( k ) In order to verify that this procedure de\ufb01nes a valid $ARG2"
    ],
    "importance sampling*****1": [
        "$ARG1 on an extended state space with points sampled over the product space [ x\u03b7 $ARG2"
    ],
    "1*****rule": [
        "$ARG1 ( x\u03b71 | x\u03b72 ) , where T\u0303a is the reverse of the transition operator de\ufb01ned by T a ( via an application of Bayes \u2019 $ARG2"
    ],
    "distribution*****weights": [
        "$ARG1 on the extended state space from which we will draw samples , it remains to determine the importance $ARG2",
        "$ARG1 ( denoted q ) , and use appropriate $ARG2",
        "$ARG1 of the input , the $ARG2"
    ],
    "restricted boltzmann machine*****deep belief network": [
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2"
    ],
    "deep belief network*****method": [
        "$ARG1 than with any inherent advantage the $ARG2"
    ],
    "method*****other": [
        "$ARG1 has over the $ARG2",
        "$ARG1 should always pretend that all of its inputs are distinct from each $ARG2"
    ],
    "method*****estimator": [
        "$ARG1 described A discussion of the properties of the AIS $ARG2"
    ],
    "1*****expected": [
        "$ARG1 /Z0 as the ratio of the $ARG2"
    ],
    "expected*****weights": [
        "$ARG1 importance $ARG2"
    ],
    "distribution*****support": [
        "$ARG1 p\u2217 is chosen carefully to have a large overlap of $ARG2"
    ],
    "support*****distance": [
        "$ARG1 with both p0 and p1 , then bridge sampling can allow the $ARG2"
    ],
    "distance*****1": [
        "$ARG1 between two distributions ( or more formally , D KL ( p0 \ue06bp $ARG2",
        "$ARG1 between points x $ARG2"
    ],
    "1*****standard": [
        "$ARG1 ) ) to be much larger than with $ARG2",
        "$ARG1 ( train ) \u221d exp \u2212 ( y \u2212 X ( train ) w ) \ue03e ( y ( train ) \u2212 X ( train ) w ) , where we follow the $ARG2",
        "$ARG1 the $ARG2"
    ],
    "standard*****distribution": [
        "$ARG1 It can be shown that the optimal bridging $ARG2",
        "$ARG1 loglikelihood term , and log p ( \u03b8 ) , corresponding to the prior $ARG2"
    ],
    "solution*****1": [
        "$ARG1 rp\u0303 0 ( x ) +\u02dc p $ARG2",
        "$ARG1 , the two points that must have output $ARG2",
        "$ARG1 to the equation for i = $ARG2"
    ],
    "distribution*****distance": [
        "$ARG1 p \u2217 to bridge the gap then one can at least use AIS with potentially many intermediate distributions to span the $ARG2"
    ],
    "method*****partition function": [
        "$ARG1 leveraged the power of the bridge sampling strategy to bridge the intermediate distributions used in AIS to signi\ufb01cantly improve the overall $ARG2",
        "$ARG1 for estimating the $ARG2"
    ],
    "partition function*****standard": [
        "$ARG1 while training While AIS has become accepted as the $ARG2"
    ],
    "partition function*****tempering": [
        "$ARG1 throughout training Using a combination of bridge sampling , short-chain AIS and parallel $ARG2"
    ],
    "track*****partition function": [
        "$ARG1 the $ARG2"
    ],
    "partition function*****process": [
        "$ARG1 RBM throughout the training $ARG2"
    ],
    "maintenance*****partition function": [
        "$ARG1 of independent estimates of the $ARG2"
    ],
    "partition function*****temperature": [
        "$ARG1 of the RBM at every $ARG2"
    ],
    "temperature*****tempering": [
        "$ARG1 operating in the parallel $ARG2",
        "$ARG1 is gradually reduced ) in order for $ARG2"
    ],
    "tempering*****variance": [
        "$ARG1 ) with AIS estimates across time to come up with a low $ARG2"
    ],
    "variance*****partition function": [
        "$ARG1 estimate of the $ARG2"
    ],
    "algorithm*****machine learning": [
        "$ARG1 can provide an approximate Many problems in $ARG2",
        "$ARG1 for a $ARG2",
        "$ARG1 A and $ARG2",
        "$ARG1 for a particular application and how to monitor and respond to feedback obtained from experiments in order to improve a $ARG2",
        "$ARG1 can choose to begin a new experiment , to \u201c freeze \u201d a running experiment that is not promising , or to \u201c thaw \u201d and resume an experiment that was earlier frozen but now appears promising given When a $ARG2"
    ],
    "machine learning*****probability distribution": [
        "$ARG1 goals are based on drawing samples from some $ARG2",
        "$ARG1 we usually use nats and the natural logarithm ) needed to send a message containing symbols drawn from $ARG2",
        "$ARG1 algorithms often involve $ARG2"
    ],
    "algorithm*****integral": [
        "$ARG1 requires us to approximate an intractable sum or $ARG2"
    ],
    "integral*****gradient": [
        "$ARG1 , such as the $ARG2"
    ],
    "integral*****example": [
        "$ARG1 can not be computed exactly ( for $ARG2"
    ],
    "view*****integral": [
        "$ARG1 the sum or $ARG2"
    ],
    "integral*****expectation": [
        "$ARG1 as if it was an $ARG2",
        "$ARG1 to estimate , rewritten as an $ARG2"
    ],
    "expectation*****distribution": [
        "$ARG1 under some $ARG2",
        "$ARG1 is taken over the true underlying $ARG2",
        "$ARG1 is taken across di\ufb00erent possible inputs , drawn from the $ARG2",
        "$ARG1 : \u2212 E x\u223cp\u0302data ( x ) Ex\u0303\u223cC ( x\u0303|x ) log pdecoder ( x | h = f ( x\u0303 ) ) where p\u0302data ( x ) is the training $ARG2"
    ],
    "distribution*****expectation": [
        "$ARG1 and to approximate the $ARG2"
    ],
    "expectation*****average": [
        "$ARG1 by a corresponding $ARG2"
    ],
    "expectation*****constraint": [
        "$ARG1 , with the $ARG2"
    ],
    "constraint*****probability distribution": [
        "$ARG1 that p is a $ARG2"
    ],
    "integral*****random variable": [
        "$ARG1 ) over $ARG2"
    ],
    "estimator*****unbiased": [
        "$ARG1 s\u0302 is $ARG2",
        "$ARG1 \u03b8\u0302m is said to be $ARG2",
        "$ARG1 \u03b8\u0302 is $ARG2",
        "$ARG1 is $ARG2",
        "$ARG1 is $ARG2"
    ],
    "average*****almost surely": [
        "$ARG1 converges $ARG2"
    ],
    "almost surely*****expected value": [
        "$ARG1 to the $ARG2"
    ],
    "variance*****average": [
        "$ARG1 Var [ s\u0302n ] decreases and converges to 0 , so long as Var [ f ( x ( i ) ) ] < \u221e : Var [ s\u0302n ] = 2 Var [ f ( x ) ] Var [ f ( x ) ] This convenient result also tells us how to estimate the uncertainty in a Monte Carlo $ARG2",
        "$ARG1 of such $ARG2"
    ],
    "average*****expected": [
        "$ARG1 or equivalently the amount of $ARG2"
    ],
    "average*****variance": [
        "$ARG1 of the f ( x ( i ) ) and their empirical variance,1 and then divide the estimated $ARG2"
    ],
    "variance*****number": [
        "$ARG1 by the $ARG2"
    ],
    "number*****estimator": [
        "$ARG1 of samples n to obtain an $ARG2"
    ],
    "central limit theorem*****distribution": [
        "$ARG1 tells us that the $ARG2"
    ],
    "distribution*****average": [
        "$ARG1 of the $ARG2"
    ],
    "average*****normal distribution": [
        "$ARG1 , s\u0302n , converges to a $ARG2"
    ],
    "normal distribution*****variance": [
        "$ARG1 Var [ f ( x ) ] with mean s and $ARG2",
        "$ARG1 with mean \u00b5\u0302m and $ARG2"
    ],
    "method*****probability": [
        "$ARG1 in equation 17.2 is deciding which part of the integrand should play the role the $ARG2"
    ],
    "probability*****expected value": [
        "$ARG1 p ( x ) and which part of the integrand should play the role of the quantity f ( x ) whose $ARG2"
    ],
    "expected value*****probability distribution": [
        "$ARG1 ( under that $ARG2"
    ],
    "problem*****unbiased": [
        "$ARG1 is speci\ufb01ed The $ARG2",
        "$ARG1 is that there exist no $ARG2"
    ],
    "variance*****1": [
        "$ARG1 is often preferred , in which the sum of squared di\ufb00erences is divided by n \u2212 $ARG2",
        "$ARG1 : $ARG2"
    ],
    "problem*****number": [
        "$ARG1 may not be the the optimal choice in terms of the $ARG2",
        "$ARG1 regards the fact that most observations are formed by an extremely large $ARG2",
        "$ARG1 where the address $ARG2"
    ],
    "number*****accuracy": [
        "$ARG1 of samples required to obtain a given level of $ARG2"
    ],
    "identity*****estimator": [
        "$ARG1 shown in equation 17.8 , any Monte Carlo $ARG2"
    ],
    "estimator*****importance sampling": [
        "$ARG1 f ( x ( i ) ) i=1 , x ( i ) \u223cp can be transformed into an $ARG2"
    ],
    "expected value*****estimator": [
        "$ARG1 of the $ARG2"
    ],
    "variance*****importance sampling": [
        "$ARG1 of an $ARG2",
        "$ARG1 ) $ARG2"
    ],
    "variance*****constant": [
        "$ARG1 occurs when q is q\u2217 ( x ) = p ( x ) |f ( x ) | where Z is the normalization $ARG2",
        "$ARG1 is \ufb01xed to some $ARG2"
    ],
    "constant*****1": [
        "$ARG1 , chosen so that q \u2217 ( x ) sums or integrates to $ARG2",
        "$ARG1 , by p ( h | v ) \u221dp ( h , v ) =p ( h1 ) p ( h2 ) p ( v | h ) \u221d exp \u2212 h21 + h22 + ( v \u2212 h 1w $ARG2"
    ],
    "problem*****distribution": [
        "$ARG1 , so it is usually not practical to use this approach of drawing a single sample from the optimal $ARG2",
        "$ARG1 is underdetermined because the $ARG2"
    ],
    "distribution*****expected value": [
        "$ARG1 q is valid ( in the sense of yielding the correct $ARG2"
    ],
    "other*****variance": [
        "$ARG1 choices of q can be feasible while still reducing the $ARG2",
        "$ARG1 values of x , they lose entropy while maintaining the desired $ARG2",
        "$ARG1 possible generating processes\u2014the over\ufb01tting regime where $ARG2"
    ],
    "estimator*****asymptotically unbiased": [
        "$ARG1 is called $ARG2",
        "$ARG1 \u03b8\u0302m is said to be $ARG2"
    ],
    "dimension*****importance sampling": [
        "$ARG1 the dynamic range of joint probabilities can be very In spite of this danger , $ARG2"
    ],
    "importance sampling*****machine learning": [
        "$ARG1 and its variants have been found very useful in many $ARG2"
    ],
    "example*****importance sampling": [
        "$ARG1 , see the use of $ARG2"
    ],
    "importance sampling*****neural language model": [
        "$ARG1 to accelerate training in $ARG2"
    ],
    "neural language model*****other": [
        "$ARG1 with a large vocabulary ( section 12.4.3.3 ) or $ARG2",
        "$ARG1 are able to recognize that two words are similar without losing the ability to encode each word as distinct from the $ARG2",
        "$ARG1 share statistical strength between one word ( and its context ) and $ARG2",
        "$ARG1 for each word as a score and tries to make the score of the correct word ay be ranked high in comparison to the $ARG2"
    ],
    "other*****number": [
        "$ARG1 neural nets with a large $ARG2",
        "$ARG1 words , pmodel ( x ; \u03b8 ) maps any con\ufb01guration x to a real $ARG2"
    ],
    "importance sampling*****partition function": [
        "$ARG1 has been used to estimate a $ARG2"
    ],
    "partition function*****constant": [
        "$ARG1 ( the normalization $ARG2"
    ],
    "distribution*****variational autoencoder": [
        "$ARG1 ) in section 18.7 , and to estimate the log-likelihood in deep directed models such as the $ARG2"
    ],
    "importance sampling*****gradient": [
        "$ARG1 may also be used to improve the estimate of the $ARG2"
    ],
    "gradient*****cost function": [
        "$ARG1 of the $ARG2",
        "$ARG1 to descend the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 is 0 , it is acceptable for the minima of the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the total $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of a $ARG2",
        "$ARG1 , and the $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "cost function*****model": [
        "$ARG1 used to train $ARG2",
        "$ARG1 depends highly on speci\ufb01c pixel values , making it much more di\ufb03cult to determine what features the $ARG2",
        "$ARG1 , and a $ARG2",
        "$ARG1 , and we must choose how to represent the output of the $ARG2",
        "$ARG1 is given by J ( \u03b8 ) = \u2212Ex , y\u223cp\u0302data log p $ARG2",
        "$ARG1 changes from $ARG2",
        "$ARG1 for each $ARG2",
        "$ARG1 to \ufb01nd the parameters of a $ARG2",
        "$ARG1 , a $ARG2",
        "$ARG1 J ( w , b ) = \u2212Ex , y\u223cp\u0302data log pmodel ( y | x ) , the $ARG2",
        "$ARG1 of a more sophisticated $ARG2",
        "$ARG1 between the $ARG2",
        "$ARG1 used to train the $ARG2",
        "$ARG1 used to train the $ARG2",
        "$ARG1 and training procedure regularize the $ARG2"
    ],
    "model*****stochastic gradient descent": [
        "$ARG1 parameters with $ARG2"
    ],
    "stochastic gradient descent*****cost function": [
        "$ARG1 , particularly for models such as classi\ufb01ers where most of the total value of the $ARG2",
        "$ARG1 for training very large models beginning in roughly 2012 , neural net $ARG2"
    ],
    "cost function*****number": [
        "$ARG1 comes from a small $ARG2"
    ],
    "markov chain monte carlo*****technique": [
        "$ARG1 Methods In many cases , we wish to use a Monte Carlo $ARG2"
    ],
    "technique*****method": [
        "$ARG1 but there is no tractable $ARG2"
    ],
    "method*****distribution": [
        "$ARG1 for drawing exact samples from the $ARG2"
    ],
    "distribution*****variance": [
        "$ARG1 pmodel ( x ) or from a good ( low $ARG2",
        "$ARG1 has maximal entropy for \ufb01xed $ARG2"
    ],
    "markov chain*****markov chain monte carlo": [
        "$ARG1 to perform Monte Carlo estimates is called $ARG2"
    ],
    "markov chain monte carlo*****machine learning": [
        "$ARG1 methods for $ARG2"
    ],
    "standard*****model": [
        "$ARG1 , generic guarantees for MCMC techniques are only applicable when the $ARG2"
    ],
    "deep learning*****energy-based model": [
        "$ARG1 , it is most common to rely on the most general theoretical guarantees that naturally apply to all To understand why drawing samples from an $ARG2"
    ],
    "energy-based model*****distribution": [
        "$ARG1 is di\ufb03cult , consider an EBM over just two variables , de\ufb01ning a $ARG2",
        "$ARG1 de\ufb01ning a $ARG2",
        "$ARG1 may be augmented with an extra parameter \u03b2 controlling how sharply peaked the $ARG2"
    ],
    "ancestral sampling*****variable": [
        "$ARG1 one simply samples each of the variables in topological order , conditioning on each $ARG2"
    ],
    "ancestral sampling*****method": [
        "$ARG1 de\ufb01nes an e\ufb03cient , single-pass $ARG2"
    ],
    "problem*****markov chain": [
        "$ARG1 by sampling using a $ARG2",
        "$ARG1 is that successful escape routes are rare for many interesting distributions , so the $ARG2",
        "$ARG1 that the $ARG2",
        "$ARG1 is to make h be a deep representation , that encodes x into h in such a way that a $ARG2"
    ],
    "attention*****random variable": [
        "$ARG1 to the case where the $ARG2"
    ],
    "distribution*****number": [
        "$ARG1 q ( t ) ( x ) , where t indicates the $ARG2",
        "$ARG1 described by a small $ARG2"
    ],
    "problem*****probability distribution": [
        "$ARG1 in terms of positive integer x , we can describe the $ARG2",
        "$ARG1 of \ufb01nding the $ARG2"
    ],
    "probability distribution*****vector": [
        "$ARG1 q using a $ARG2",
        "$ARG1 over a single $ARG2"
    ],
    "markov chain*****matrix": [
        "$ARG1 update repeatedly corresponds to multiplying by the $ARG2",
        "$ARG1 in terms of a $ARG2"
    ],
    "process*****matrix": [
        "$ARG1 as exponentiating the $ARG2"
    ],
    "matrix*****probability distribution": [
        "$ARG1 A has special structure because each of its columns represents a $ARG2"
    ],
    "other*****theorem": [
        "$ARG1 state x\ue030 for some power t , then the Perron-Frobenius $ARG2"
    ],
    "theorem*****eigenvalue": [
        "$ARG1 ( Perron , 1907 ; Frobenius , 1908 ) guarantees that the largest $ARG2"
    ],
    "eigenvalue*****1": [
        "$ARG1 is real and equal to $ARG2",
        "$ARG1 of $ARG2"
    ],
    "process*****1": [
        "$ARG1 causes all of the eigenvalues that are not equal to $ARG2",
        "$ARG1 Require : y , the target output h ( 0 ) = x for k = $ARG2",
        "$ARG1 thus introduces two major advantages : $ARG2"
    ],
    "conditions*****eigenvector": [
        "$ARG1 , A is guaranteed to have only one $ARG2"
    ],
    "eigenvector*****eigenvalue": [
        "$ARG1 with $ARG2",
        "$ARG1 with corresponding $ARG2",
        "$ARG1 v with corresponding $ARG2",
        "$ARG1 of J with $ARG2",
        "$ARG1 of A , f takes on the value of the corresponding $ARG2",
        "$ARG1 of H corresponding to the maximal $ARG2",
        "$ARG1 of the Hessian and has a positive $ARG2",
        "$ARG1 of the Hessian with negative $ARG2"
    ],
    "distribution*****change": [
        "$ARG1 , repeated applications of the transition sampling procedure do not $ARG2",
        "$ARG1 over the underlying causes can $ARG2"
    ],
    "markov chain*****conditions": [
        "$ARG1 to describe both $ARG2",
        "$ARG1 with transition operator T will converge , under mild $ARG2"
    ],
    "expectation*****integral": [
        "$ARG1 corresponds to an $ARG2",
        "$ARG1 is an $ARG2"
    ],
    "markov chain*****stochastic": [
        "$ARG1 methods consist of repeatedly applying $ARG2"
    ],
    "stochastic*****distribution": [
        "$ARG1 updates until eventually the state begins to yield samples from the equilibrium $ARG2"
    ],
    "problem*****statistics": [
        "$ARG1 is to return only every n successive samples , so that our estimate of the $ARG2"
    ],
    "statistics*****distribution": [
        "$ARG1 of the equilibrium $ARG2"
    ],
    "distribution*****correlation": [
        "$ARG1 is not as biased by the $ARG2"
    ],
    "markov chain*****deep learning": [
        "$ARG1 for each desired sample are two extremes ; $ARG2"
    ],
    "deep learning*****number": [
        "$ARG1 practitioners usually use a $ARG2",
        "$ARG1 applications , n will be the $ARG2",
        "$ARG1 models contain a very large $ARG2"
    ],
    "number*****minibatch": [
        "$ARG1 of examples in a $ARG2",
        "$ARG1 of examples in the $ARG2"
    ],
    "minibatch*****set": [
        "$ARG1 and then draw as many samples as are needed from this \ufb01xed $ARG2",
        "$ARG1 of examples from the training $ARG2",
        "$ARG1 of m examples from the training $ARG2",
        "$ARG1 of m examples from the training $ARG2",
        "$ARG1 of m examples from the training $ARG2",
        "$ARG1 of m examples from the training $ARG2",
        "$ARG1 of m examples from the training $ARG2",
        "$ARG1 of m examples from the training $ARG2",
        "$ARG1 of m examples from the training $ARG2"
    ],
    "markov chain*****view": [
        "$ARG1 from the point of $ARG2"
    ],
    "view*****matrix": [
        "$ARG1 of a $ARG2"
    ],
    "vector*****eigenvalue": [
        "$ARG1 of probabilities v , then we know that the chain mixes when A t has e\ufb00ectively lost all of the eigenvalues from A besides the unique $ARG2"
    ],
    "magnitude*****eigenvalue": [
        "$ARG1 of the second largest $ARG2",
        "$ARG1 of the largest and smallest $ARG2"
    ],
    "number*****model": [
        "$ARG1 of states that our probabilistic $ARG2",
        "$ARG1 w , then the $ARG2",
        "$ARG1 of hidden units required by the shallow $ARG2",
        "$ARG1 of parameters in the $ARG2",
        "$ARG1 of parameters in the RNN may be adjusted to control $ARG2",
        "$ARG1 of parameters in a $ARG2",
        "$ARG1 of parameters by a factor of more than 50 ! In general , to $ARG2",
        "$ARG1 of unique $ARG2",
        "$ARG1 of terms , it is intractable to evaluate except in cases where the structure of the $ARG2",
        "$ARG1 of hidden units increases the capacity of the $ARG2",
        "$ARG1 of hidden units increases the of the $ARG2",
        "$ARG1 of parameters in the $ARG2"
    ],
    "other*****markov chain": [
        "$ARG1 obstacles , we usually do not know whether a $ARG2"
    ],
    "deep learning*****markov chain": [
        "$ARG1 , we commonly use $ARG2"
    ],
    "markov chain*****energy-based model": [
        "$ARG1 to draw samples from an $ARG2"
    ],
    "variable*****graph": [
        "$ARG1 x i and sampling it from pmodel conditioned on its neighbors in the undirected $ARG2",
        "$ARG1 y ( t ) via its e\ufb00ect on h. The structure of this $ARG2",
        "$ARG1 has few parents in the $ARG2",
        "$ARG1 and a factor are connected in the $ARG2"
    ],
    "graph*****energy-based model": [
        "$ARG1 G de\ufb01ning the structure of the $ARG2"
    ],
    "example*****other": [
        "$ARG1 in section 16.7.1 , all of the hidden units of an RBM may be sampled simultaneously because they are conditionally independent from each $ARG2",
        "$ARG1 , one whose entries are sparse , or independent from each $ARG2",
        "$ARG1 will then contribute little to the overall training cost , which will be dominated by $ARG2",
        "$ARG1 surrounded by $ARG2",
        "$ARG1 , when recognizing an object in a photo , it is usually possible to ignore the background of It is possible to ask probabilistic models to do many $ARG2",
        "$ARG1 and edges connecting near neighbors to each $ARG2"
    ],
    "example*****algorithm": [
        "$ARG1 , the Metropolis-Hastings $ARG2",
        "$ARG1 of such an $ARG2",
        "$ARG1 , by running a clustering $ARG2"
    ],
    "algorithm*****other": [
        "$ARG1 is widely used in $ARG2",
        "$ARG1 is universally any better than any $ARG2",
        "$ARG1 speci\ufb01cally , $ARG2",
        "$ARG1 to maximize L. On one step , we maximize L with respect to q , and on the $ARG2"
    ],
    "undirected model*****other": [
        "$ARG1 , it is rare to use any approach $ARG2"
    ],
    "other*****gibbs sampling": [
        "$ARG1 than $ARG2"
    ],
    "gradient descent*****energy function": [
        "$ARG1 on the $ARG2"
    ],
    "energy function*****hill climbing": [
        "$ARG1 , or equivalently noisy $ARG2"
    ],
    "hill climbing*****probability": [
        "$ARG1 on the $ARG2"
    ],
    "probability*****random variable": [
        "$ARG1 , with respect to the state of the chain ( the $ARG2",
        "$ARG1 of that $ARG2",
        "$ARG1 density over a continuous $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of a binary $ARG2",
        "$ARG1 of a con\ufb01guration of $ARG2"
    ],
    "example*****image": [
        "$ARG1 , if the variables are pixels in an $ARG2",
        "$ARG1 , having read that a cat has four legs and pointy ears , the learner might be able to guess that an $ARG2",
        "$ARG1 of recognizing cats without having seen an $ARG2",
        "$ARG1 , the same $ARG2",
        "$ARG1 , the features of an $ARG2",
        "$ARG1 is pixel-wise segmentation of images , where the computer program assigns every pixel in an $ARG2",
        "$ARG1 , in $ARG2",
        "$ARG1 , the words produced by an $ARG2",
        "$ARG1 tangent line at one point , with an $ARG2",
        "$ARG1 , if we use a two-dimensional $ARG2",
        "$ARG1 , when processing an $ARG2",
        "$ARG1 , when determining whether an $ARG2",
        "$ARG1 , a color $ARG2",
        "$ARG1 , if we want to tell if an $ARG2",
        "$ARG1 , when using implicit zero padding , detector units at the edge of the $ARG2",
        "$ARG1 if we want to assign a single class label to the entire $ARG2",
        "$ARG1 , if the input is an $ARG2"
    ],
    "image*****manifold": [
        "$ARG1 , a region of low energy might be a connected $ARG2"
    ],
    "probability*****number": [
        "$ARG1 of going from one mode to a nearby mode within a given $ARG2",
        "$ARG1 density at some point x , we can just return the $ARG2"
    ],
    "problem*****multiple": [
        "$ARG1 arises when there are $ARG2"
    ],
    "multiple*****probability": [
        "$ARG1 modes with high $ARG2",
        "$ARG1 modes , q chooses to blur the modes together , in order to put high $ARG2"
    ],
    "probability*****gibbs sampling": [
        "$ARG1 , especially when each $ARG2"
    ],
    "gibbs sampling*****subset": [
        "$ARG1 step must update only a small $ARG2"
    ],
    "example*****energy-based model": [
        "$ARG1 , consider an $ARG2"
    ],
    "energy-based model*****1": [
        "$ARG1 over two variables a and b , which are both binary with a sign , taking on values \u22121 and $ARG2"
    ],
    "sigmoid*****probability": [
        "$ARG1 saturates , and the $ARG2",
        "$ARG1 units as output units , used to predict the $ARG2"
    ],
    "gibbs sampling*****markov chain": [
        "$ARG1 for three distributions , with the $ARG2"
    ],
    "correlation*****markov chain": [
        "$ARG1 between variables makes it di\ufb03cult for the $ARG2"
    ],
    "variable*****other": [
        "$ARG1 must be conditioned on the $ARG2",
        "$ARG1 to be changed to a value that di\ufb00ers signi\ufb01cantly from the current value of the $ARG2",
        "$ARG1 has all $ARG2",
        "$ARG1 interacts with every $ARG2",
        "$ARG1 x i and draw a sample conditioned on all of the $ARG2",
        "$ARG1 tends to take on a relatively high value at the times that the $ARG2"
    ],
    "variable*****correlation": [
        "$ARG1 , the $ARG2"
    ],
    "correlation*****rate": [
        "$ARG1 reduces the $ARG2"
    ],
    "rate*****markov chain": [
        "$ARG1 at which the $ARG2"
    ],
    "gibbs sampling*****change": [
        "$ARG1 mixes very slowly because it is di\ufb03cult to $ARG2"
    ],
    "change*****variable": [
        "$ARG1 modes while altering only one $ARG2"
    ],
    "challenge*****model": [
        "$ARG1 is even greater because we care not only about making transitions between two modes but more generally between all the many modes that a real $ARG2",
        "$ARG1 of training the desired $ARG2"
    ],
    "set*****convergence": [
        "$ARG1 of samples covering most of the modes , and $ARG2"
    ],
    "convergence*****distribution": [
        "$ARG1 of the chain to its stationary $ARG2"
    ],
    "markov chain*****problem": [
        "$ARG1 was originally introduced to solve is this $ARG2"
    ],
    "problem*****group": [
        "$ARG1 of sampling from a large $ARG2"
    ],
    "illustration*****problem": [
        "$ARG1 of the slow mixing $ARG2"
    ],
    "gibbs sampling*****graphical model": [
        "$ARG1 is performed in a deep $ARG2"
    ],
    "graphical model*****similarity": [
        "$ARG1 , this $ARG2"
    ],
    "similarity*****distribution": [
        "$ARG1 is based more on semantic rather than raw visual features , but it is still di\ufb03cult for the Gibbs chain to transition from one mode of the $ARG2"
    ],
    "example*****identity": [
        "$ARG1 by changing the digit $ARG2",
        "$ARG1 , we at least know the user $ARG2"
    ],
    "ancestral sampling*****problem": [
        "$ARG1 generates each sample independently from the others , there is no mixing $ARG2"
    ],
    "view*****learning": [
        "$ARG1 of $ARG2",
        "$ARG1 motivates semisupervised $ARG2",
        "$ARG1 as saying that we believe the $ARG2",
        "$ARG1 of $ARG2"
    ],
    "boltzmann machine*****distribution": [
        "$ARG1 sharper the $ARG2"
    ],
    "distribution*****boltzmann machine": [
        "$ARG1 a $ARG2"
    ],
    "boltzmann machine*****markov chain": [
        "$ARG1 learns , the harder it is for a $ARG2"
    ],
    "distribution*****manifold": [
        "$ARG1 of interest has a $ARG2",
        "$ARG1 , \ufb01nding a $ARG2",
        "$ARG1 in a two-dimensional space that is actually concentrated near a one-dimensional $ARG2"
    ],
    "manifold*****distribution": [
        "$ARG1 for each class : the $ARG2"
    ],
    "distribution*****classi\ufb01cation": [
        "$ARG1 is what we expect in many $ARG2"
    ],
    "tempering*****distribution": [
        "$ARG1 to Mix between Modes When a $ARG2"
    ],
    "probability*****distribution": [
        "$ARG1 , it is di\ufb03cult to mix between the di\ufb00erent modes of the $ARG2",
        "$ARG1 that you will get a meaningful English-language text ? Almost zero , again , because most of the long sequences of letters do not correspond to a natural language sequence : the $ARG2",
        "$ARG1 0 to any event , and we renormalize the resulting $ARG2",
        "$ARG1 anywhere that the true $ARG2",
        "$ARG1 anywhere that the true $ARG2"
    ],
    "energy-based model*****probability": [
        "$ARG1 as de\ufb01ning a $ARG2"
    ],
    "temperature*****energy-based model": [
        "$ARG1 , re\ufb02ecting the origin of $ARG2",
        "$ARG1 falls to zero and \u03b2 rises to in\ufb01nity , the $ARG2"
    ],
    "temperature*****distribution": [
        "$ARG1 rises to in\ufb01nity and \u03b2 falls to zero , the $ARG2"
    ],
    "tempering*****1": [
        "$ARG1 is a general strategy of mixing between modes of p $ARG2"
    ],
    "markov chain*****temperature": [
        "$ARG1 based on tempered transitions ( Neal , 1994 ) temporarily sample from higher-temperature distributions in order to mix to di\ufb00erent modes , then resume sampling from the unit $ARG2"
    ],
    "tempering*****markov chain": [
        "$ARG1 ( Iba , 2001 ) , in which the $ARG2"
    ],
    "tempering*****challenge": [
        "$ARG1 is a promising approach , at this point it has not allowed researchers to make a strong advance in solving the $ARG2"
    ],
    "critical temperature*****temperature": [
        "$ARG1 around which the $ARG2"
    ],
    "representation learning*****distribution": [
        "$ARG1 algorithms , such as autoencoders and RBMs , tend to yield a marginal $ARG2"
    ],
    "deep learning*****model": [
        "$ARG1 involve building a probabilistic $ARG2",
        "$ARG1 practitioners tend to use very di\ufb00erent $ARG2",
        "$ARG1 , the approach most commonly used to $ARG2",
        "$ARG1 the $ARG2",
        "$ARG1 scenarios , we almost always do \ufb01nd\u2014that the best \ufb01tting $ARG2"
    ],
    "principle*****inference": [
        "$ARG1 , use probabilistic $ARG2"
    ],
    "inference*****other": [
        "$ARG1 to predict any of the variables in its environment given any of the $ARG2",
        "$ARG1 problems in which we must predict the value of some variables given $ARG2"
    ],
    "distributed representation*****representation learning": [
        "$ARG1 based on latent variables can obtain all of the advantages of $ARG2"
    ],
    "representation learning*****linear factor models": [
        "$ARG1 that we have seen with deep feedforward and recurrent In this chapter , we describe some of the simplest probabilistic models with latent variables : $ARG2"
    ],
    "model*****stochastic": [
        "$ARG1 is de\ufb01ned by the use of a $ARG2",
        "$ARG1 pmodel ( h , x ) de\ufb01nes a $ARG2"
    ],
    "stochastic*****decoder": [
        "$ARG1 , linear $ARG2"
    ],
    "decoder*****variable": [
        "$ARG1 made these models some of the \ufb01rst latent $ARG2"
    ],
    "directed graphical model*****model": [
        "$ARG1 describing the linear factor $ARG2"
    ],
    "vector*****linear combination": [
        "$ARG1 x is obtained by a $ARG2"
    ],
    "probabilistic pca*****factor analysis": [
        "$ARG1 , $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2"
    ],
    "probabilistic pca*****principal components analysis": [
        "$ARG1 ( $ARG2"
    ],
    "principal components analysis*****factor analysis": [
        "$ARG1 ) , $ARG2"
    ],
    "factor analysis*****other": [
        "$ARG1 and $ARG2"
    ],
    "linear factor models*****special case": [
        "$ARG1 are $ARG2"
    ],
    "special case*****distribution": [
        "$ARG1 of the above equations ( 13.1 and 13.2 ) and only di\ufb00er in the choices made for the noise $ARG2"
    ],
    "factor analysis*****variable": [
        "$ARG1 ( Bartholomew , 1987 ; Basilevsky , 1994 ) , the latent $ARG2"
    ],
    "variance*****diagonal": [
        "$ARG1 Gaussian h \u223c N ( h ; 0 , I ) while the observed variables xi are assumed to be conditionally independent , given h. Speci\ufb01cally , the noise is assumed to be drawn from a $ARG2"
    ],
    "covariance matrix*****\u03c3": [
        "$ARG1 \u03c8 = diag ( $ARG2",
        "$ARG1 ( as given by $ARG2"
    ],
    "linear factor models*****pca": [
        "$ARG1 In order to cast $ARG2",
        "$ARG1 including $ARG2"
    ],
    "pca*****factor analysis": [
        "$ARG1 in a probabilistic framework , we can make a slight modi\ufb01cation to the $ARG2",
        "$ARG1 and $ARG2"
    ],
    "covariance*****\u03c3": [
        "$ARG1 of x is just W W \ue03e + $ARG2"
    ],
    "\u03c3*****scalar": [
        "$ARG1 2 is now a $ARG2"
    ],
    "\u03c3*****gaussian noise": [
        "$ARG1 2I ) x = W h + b + \u03c3z where z \u223c N ( z ; 0 , I ) is $ARG2"
    ],
    "bishop*****algorithm": [
        "$ARG1 ( 1999 ) then show an iterative EM $ARG2"
    ],
    "algorithm*****\u03c3": [
        "$ARG1 for estimating the parameters W and $ARG2"
    ],
    "bishop*****probabilistic pca": [
        "$ARG1 ( 1999 ) , $ARG2"
    ],
    "probabilistic pca*****pca": [
        "$ARG1 becomes $ARG2"
    ],
    "pca*****\u03c3": [
        "$ARG1 as $ARG2"
    ],
    "expected value*****pca": [
        "$ARG1 of h given x becomes an orthogonal projection of x \u2212 b onto the space spanned by the d columns of W , like in $ARG2"
    ],
    "model*****probabilistic pca": [
        "$ARG1 de\ufb01ned by $ARG2"
    ],
    "independent component analysis*****representation learning": [
        "$ARG1 ( ICA ) is among the oldest $ARG2"
    ],
    "linear factor models*****change": [
        "$ARG1 nonlinear $ARG2"
    ],
    "maximum likelihood*****other": [
        "$ARG1 criterion , but instead aim to make the elements of h = W \u22121 x independent from each $ARG2"
    ],
    "determinant*****operation": [
        "$ARG1 of W , which can be an expensive and numerically unstable $ARG2"
    ],
    "linear factor models*****probabilistic pca": [
        "$ARG1 like $ARG2"
    ],
    "factor analysis*****model": [
        "$ARG1 , that often require p ( h ) to be Gaussian in order to make many operations on the $ARG2"
    ],
    "maximum likelihood*****\u03c3": [
        "$ARG1 approach where the $ARG2"
    ],
    "gaussian distribution*****learning": [
        "$ARG1 , so we can also see most implementations of ICA as $ARG2"
    ],
    "pca*****model": [
        "$ARG1 can be generalized to the nonlinear autoencoders described in chapter 14 , ICA can be generalized to a nonlinear generative $ARG2"
    ],
    "series*****encoder": [
        "$ARG1 of invertible transformations ( $ARG2"
    ],
    "encoder*****property": [
        "$ARG1 stages ) that have the $ARG2"
    ],
    "property*****determinant": [
        "$ARG1 that the $ARG2"
    ],
    "distribution*****encoder": [
        "$ARG1 , but is more likely to succeed thanks to the nonlinear $ARG2",
        "$ARG1 , pencoder ( h | x ) for the $ARG2",
        "$ARG1 preconstruct ( x | x\u0303 ) = pdecoder ( x | h ) with h the output of $ARG2"
    ],
    "decoder*****model": [
        "$ARG1 that is its perfect inverse , it is straightforward to generate samples from the $ARG2",
        "$ARG1 ( x | h ) = p $ARG2"
    ],
    "model*****decoder": [
        "$ARG1 ( by \ufb01rst sampling from p ( h ) and then applying the $ARG2"
    ],
    "generalization*****statistical dependence": [
        "$ARG1 of ICA is to learn groups of features , with $ARG2"
    ],
    "statistical dependence*****group": [
        "$ARG1 allowed within a $ARG2"
    ],
    "natural image*****topographic ica": [
        "$ARG1 , this $ARG2"
    ],
    "gabor function*****pooling": [
        "$ARG1 occur within each region , so that $ARG2"
    ],
    "pooling*****translation": [
        "$ARG1 over small regions yields $ARG2",
        "$ARG1 naturally become frequency selective and $ARG2"
    ],
    "slow feature analysis*****model": [
        "$ARG1 ( SFA ) is a linear factor $ARG2"
    ],
    "model*****information": [
        "$ARG1 that uses $ARG2",
        "$ARG1 to read each sentence and extract the year in which the narrator went to Nepal , we would like it to recognize the year 2009 as the relevant piece of $ARG2",
        "$ARG1 learns a hidden unit hi that detects a face by \ufb01nding the nose , then dropping h i corresponds to erasing the $ARG2",
        "$ARG1 that must discard some of the $ARG2"
    ],
    "linear factor models*****invariant": [
        "$ARG1 time signals to learn $ARG2"
    ],
    "slow feature analysis*****principle": [
        "$ARG1 is motivated by a general $ARG2",
        "$ARG1 is a particularly e\ufb03cient application of the slowness $ARG2"
    ],
    "example*****computer vision": [
        "$ARG1 , in $ARG2",
        "$ARG1 of the latter category , one recent $ARG2"
    ],
    "computer vision*****change": [
        "$ARG1 , individual pixel values can $ARG2"
    ],
    "image*****change": [
        "$ARG1 , an individual pixel will rapidly $ARG2",
        "$ARG1 will not $ARG2",
        "$ARG1 gradually $ARG2"
    ],
    "change*****pass": [
        "$ARG1 from black to white and back again as the zebra \u2019 s stripes $ARG2"
    ],
    "change*****feature": [
        "$ARG1 at all , and the $ARG2"
    ],
    "feature*****change": [
        "$ARG1 describing the zebra \u2019 s position will $ARG2"
    ],
    "principle*****slow feature analysis": [
        "$ARG1 predates $ARG2"
    ],
    "principle*****model": [
        "$ARG1 to any di\ufb00erentiable $ARG2",
        "$ARG1 encourages the $ARG2",
        "$ARG1 , we could also treat \u03b2 as a parameter of the $ARG2",
        "$ARG1 , the last layer could be another kind of $ARG2"
    ],
    "model*****gradient descent": [
        "$ARG1 trained with $ARG2",
        "$ARG1 with $ARG2"
    ],
    "principle*****cost function": [
        "$ARG1 may be introduced by adding a term to the $ARG2"
    ],
    "cost function*****hyperparameter": [
        "$ARG1 of the form L ( f ( x ( t+1 ) ) , f ( x ( t ) ) ) where \u03bb is a $ARG2"
    ],
    "hyperparameter*****regularization": [
        "$ARG1 determining the strength of the slowness $ARG2"
    ],
    "regularization*****feature": [
        "$ARG1 term , t is the index into a time sequence of examples , f is the $ARG2"
    ],
    "feature*****loss function": [
        "$ARG1 extractor to be regularized , and L is a $ARG2"
    ],
    "loss function*****distance": [
        "$ARG1 measuring the $ARG2"
    ],
    "model*****linear map": [
        "$ARG1 per se , in the sense that it de\ufb01nes a $ARG2"
    ],
    "linear map*****feature": [
        "$ARG1 between input space and $ARG2"
    ],
    "feature*****distribution": [
        "$ARG1 space and thus does not impose a $ARG2"
    ],
    "algorithm*****optimization problem": [
        "$ARG1 ( Wiskott and Sejnowski , 2002 ) consists of de\ufb01ning f ( x ; \u03b8 ) to be a linear transformation , and solving the $ARG2"
    ],
    "optimization problem*****1": [
        "$ARG1 min Et ( f ( x ( t+1 ) ) i \u2212 f ( x ( t ) ) i ) 2 Etf ( x ( t ) ) i = 0 Et [ f ( x ( t ) ) 2i ] = $ARG2"
    ],
    "linear factor models*****constraint": [
        "$ARG1 The $ARG2"
    ],
    "constraint*****feature": [
        "$ARG1 that the learned $ARG2"
    ],
    "feature*****problem": [
        "$ARG1 have zero mean is necessary to make the $ARG2"
    ],
    "problem*****solution": [
        "$ARG1 have a unique $ARG2",
        "$ARG1 is to attempt to initialize the parameters in a region that is connected to the $ARG2",
        "$ARG1 then re\ufb01ne the $ARG2",
        "$ARG1 has a $ARG2",
        "$ARG1 a unique $ARG2",
        "$ARG1 with no closed form $ARG2"
    ],
    "solution*****constant": [
        "$ARG1 ; otherwise we could add a $ARG2"
    ],
    "constant*****feature": [
        "$ARG1 to all $ARG2"
    ],
    "feature*****solution": [
        "$ARG1 values and obtain a di\ufb00erent $ARG2"
    ],
    "constraint*****variance": [
        "$ARG1 that the features have unit $ARG2"
    ],
    "variance*****solution": [
        "$ARG1 is necessary to prevent the pathological $ARG2"
    ],
    "pca*****feature": [
        "$ARG1 , the SFA features are ordered , with the \ufb01rst $ARG2"
    ],
    "multiple*****constraint": [
        "$ARG1 features , we must also add the $ARG2"
    ],
    "other*****solution": [
        "$ARG1 mechanisms , such as minimizing reconstruction error , to force the features to diversify , but this decorrelation mechanism admits a simple $ARG2",
        "$ARG1 results show that \ufb01nding a $ARG2",
        "$ARG1 methods , such as guessing the $ARG2",
        "$ARG1 words , for all i , we know that at least one of the constraints \u03b1i \u2265 0 and h ( i ) ( x ) \u2264 0 must be active at the $ARG2"
    ],
    "problem*****linear algebra": [
        "$ARG1 may be solved in closed form by a $ARG2"
    ],
    "example*****vector": [
        "$ARG1 , it is common to replace x by the quadratic basis expansion , a $ARG2",
        "$ARG1 , if D is a column $ARG2",
        "$ARG1 as a $ARG2",
        "$ARG1 as a $ARG2",
        "$ARG1 and \u03b1 is a $ARG2",
        "$ARG1 x\u22121 is the $ARG2",
        "$ARG1 with a real-valued $ARG2"
    ],
    "feature*****learning": [
        "$ARG1 extractors by repeatedly $ARG2",
        "$ARG1 extractor , applying a nonlinear basis expansion to its output , and then $ARG2",
        "$ARG1 extractor and a $ARG2"
    ],
    "learning*****feature": [
        "$ARG1 a linear SFA $ARG2",
        "$ARG1 another linear SFA $ARG2",
        "$ARG1 the $ARG2",
        "$ARG1 representations that disentangle more complicated forms of $ARG2"
    ],
    "linear factor models*****other": [
        "$ARG1 This is in comparison to $ARG2"
    ],
    "learning*****cost function": [
        "$ARG1 algorithms where the $ARG2",
        "$ARG1 we must choose a $ARG2",
        "$ARG1 when an appropriate $ARG2",
        "$ARG1 curve will show violent oscillations , with the $ARG2"
    ],
    "model*****feature learning": [
        "$ARG1 that has been heavily studied as an unsupervised $ARG2"
    ],
    "feature learning*****feature": [
        "$ARG1 and $ARG2"
    ],
    "sparse coding*****process": [
        "$ARG1 \u201d refers to the $ARG2",
        "$ARG1 \u2019 s optimization-based encoding $ARG2"
    ],
    "process*****learning": [
        "$ARG1 of designing and $ARG2",
        "$ARG1 to begin by $ARG2",
        "$ARG1 of $ARG2"
    ],
    "linear factor models*****decoder": [
        "$ARG1 , it uses a linear $ARG2"
    ],
    "sparse coding*****gaussian noise": [
        "$ARG1 models typically assume that the linear factors have $ARG2"
    ],
    "gaussian noise*****isotropic": [
        "$ARG1 with $ARG2"
    ],
    "linear factor models*****sparse coding": [
        "$ARG1 Training $ARG2"
    ],
    "sparse coding*****maximum likelihood": [
        "$ARG1 with $ARG2"
    ],
    "pca*****encoder": [
        "$ARG1 , we have seen the use of a parametric $ARG2",
        "$ARG1 but also to any linear autoencoder that learns matrices W and V with the goal of making the reconstruction of x lie as close to x as possible , Let the $ARG2"
    ],
    "function*****multiplication": [
        "$ARG1 that predicts h and consists only of $ARG2"
    ],
    "multiplication*****matrix": [
        "$ARG1 by a weight $ARG2",
        "$ARG1 by the $ARG2",
        "$ARG1 by a $ARG2"
    ],
    "encoder*****sparse coding": [
        "$ARG1 that we use with $ARG2",
        "$ARG1 , which makes it di\ufb03cult to pretrain a $ARG2"
    ],
    "sparse coding*****encoder": [
        "$ARG1 is not a parametric $ARG2",
        "$ARG1 approach combined with the use of the non-parametric $ARG2"
    ],
    "encoder*****optimization algorithm": [
        "$ARG1 is an $ARG2"
    ],
    "optimization algorithm*****optimization problem": [
        "$ARG1 , that solves an $ARG2",
        "$ARG1 are iterative by nature but , when applied to the right class of $ARG2",
        "$ARG1 , and we have little theoretical understanding of the very general non-convex $ARG2"
    ],
    "1*****optimization problem": [
        "$ARG1 because its role in this $ARG2",
        "$ARG1 This $ARG2"
    ],
    "optimization problem*****hyperparameters": [
        "$ARG1 is shared with \u03bb and there is no need for both $ARG2"
    ],
    "learning*****dictionary": [
        "$ARG1 a $ARG2"
    ],
    "dictionary*****inference": [
        "$ARG1 of features with activation values that will often be zero when extracted using this $ARG2"
    ],
    "encoder*****principle": [
        "$ARG1 can in $ARG2"
    ],
    "principle*****encoder": [
        "$ARG1 minimize the combination of reconstruction error and log-prior better than any speci\ufb01c parametric $ARG2"
    ],
    "generalization*****encoder": [
        "$ARG1 error to the $ARG2",
        "$ARG1 error in the $ARG2"
    ],
    "sparse coding*****inference": [
        "$ARG1 models , where the $ARG2",
        "$ARG1 We usually use the term $ARG2"
    ],
    "problem*****optimization": [
        "$ARG1 is convex , the $ARG2",
        "$ARG1 is that gradients propagated over many stages tend to either vanish ( most of the time ) or explode ( rarely , but with much damage to the $ARG2",
        "$ARG1 in most numerical $ARG2",
        "$ARG1 using unconstrained $ARG2"
    ],
    "generalization*****decoder": [
        "$ARG1 error in the $ARG2"
    ],
    "weights*****generalization": [
        "$ARG1 , rather than $ARG2"
    ],
    "generalization*****sparse coding": [
        "$ARG1 error in $ARG2",
        "$ARG1 when $ARG2"
    ],
    "process*****generalization": [
        "$ARG1 may result in better $ARG2"
    ],
    "sparse coding*****feature": [
        "$ARG1 is used as a $ARG2"
    ],
    "feature*****function": [
        "$ARG1 extractor for a classi\ufb01er than when a parametric $ARG2",
        "$ARG1 extraction $ARG2",
        "$ARG1 should be a $ARG2"
    ],
    "sparse coding*****object recognition": [
        "$ARG1 features generalize better for $ARG2"
    ],
    "object recognition*****model": [
        "$ARG1 tasks than the features of a related $ARG2",
        "$ARG1 provide a $ARG2"
    ],
    "model*****encoder": [
        "$ARG1 based on a parametric $ARG2",
        "$ARG1 trains a shallow $ARG2",
        "$ARG1 , under which the $ARG2",
        "$ARG1 is forced to prioritize which aspects of the input should be copied , it often learns useful properties of the Modern autoencoders have generalized the idea of an $ARG2",
        "$ARG1 capacity by keeping the $ARG2",
        "$ARG1 ( section 13.4 ) , but with h being the output of the parametric $ARG2",
        "$ARG1 consists of an $ARG2"
    ],
    "sparse coding*****other": [
        "$ARG1 generalizes better than $ARG2",
        "$ARG1 , like $ARG2"
    ],
    "encoder*****algorithm": [
        "$ARG1 is that it requires greater time to compute h given x because the non-parametric approach requires running an iterative $ARG2"
    ],
    "feature*****model": [
        "$ARG1 may be learned well , but the factorial prior on the hidden code results in the $ARG2",
        "$ARG1 space is only to make the $ARG2"
    ],
    "linear factor models*****example": [
        "$ARG1 Figure 13.2 : $ARG2"
    ],
    "example*****weights": [
        "$ARG1 samples and $ARG2",
        "$ARG1 , we expressed our preference for linear functions de\ufb01ned with smaller $ARG2",
        "$ARG1 , Hinton and Salakhutdinov ( 2006 ) trained a stack of RBMs and then used their $ARG2",
        "$ARG1 , if we are processing college applications , and our features consist of both grades and standardized test scores , but not every applicant took the standardized test , then it does not make sense to convolve the same $ARG2",
        "$ARG1 , suppose that we are training a neural net with several layers parametrized by $ARG2"
    ],
    "weights*****sparse coding": [
        "$ARG1 from a spike and slab $ARG2"
    ],
    "model*****dataset": [
        "$ARG1 trained on the MNIST $ARG2",
        "$ARG1 size over time , due to the availability of faster CPUs , $ARG2",
        "$ARG1 proposes a fact that is not in the $ARG2",
        "$ARG1 , and a $ARG2",
        "$ARG1 a particular $ARG2",
        "$ARG1 parameters to improve the likelihood of a completed $ARG2",
        "$ARG1 i is then trained on $ARG2",
        "$ARG1 i on $ARG2",
        "$ARG1 on this $ARG2"
    ],
    "interpretation*****pca": [
        "$ARG1 of $ARG2",
        "$ARG1 applies not just to traditional $ARG2"
    ],
    "factor analysis*****learning": [
        "$ARG1 can be interpreted as $ARG2"
    ],
    "learning*****manifold": [
        "$ARG1 a $ARG2",
        "$ARG1 algorithms to be able to discover and disentangle such $ARG2"
    ],
    "probabilistic pca*****gaussian distribution": [
        "$ARG1 as de\ufb01ning a thin pancake-shaped region of high probability\u2014a $ARG2"
    ],
    "gaussian distribution*****other": [
        "$ARG1 that is very narrow along some axes , just as a pancake is very \ufb02at along its vertical axis , but is elongated along $ARG2",
        "$ARG1 over x but all $ARG2"
    ],
    "pca*****linear manifold": [
        "$ARG1 can be interpreted as aligning this pancake with a $ARG2"
    ],
    "linear factor models*****encoder": [
        "$ARG1 The $ARG2"
    ],
    "encoder*****view": [
        "$ARG1 computes a low-dimensional representation of h. With the autoencoder $ARG2"
    ],
    "view*****decoder": [
        "$ARG1 , we have a $ARG2"
    ],
    "probability*****manifold": [
        "$ARG1 concentration near a low-dimensional $ARG2"
    ],
    "variance*****manifold": [
        "$ARG1 in the direction orthogonal to the $ARG2"
    ],
    "manifold*****other": [
        "$ARG1 is very small ( arrow pointing out of plane ) and can be considered like \u201c noise , \u201d while the $ARG2",
        "$ARG1 of local maxima that are connected to each $ARG2"
    ],
    "other*****system": [
        "$ARG1 variances are large ( arrows in the plane ) and correspond to \u201c signal , \u201d and a coordinate $ARG2",
        "$ARG1 words , the goal is to build a $ARG2",
        "$ARG1 areas of the visual $ARG2",
        "$ARG1 components of the complete $ARG2"
    ],
    "decoder*****covariance": [
        "$ARG1 that minimize reconstruction error E [ ||x \u2212 x\u0302|| 2 ] correspond to V = W , \u00b5 = b = E [ x ] and the columns of W form an orthonormal basis which spans the same subspace as the principal eigenvectors of the $ARG2"
    ],
    "pca*****magnitude": [
        "$ARG1 , the columns of W are these eigenvectors , ordered by the $ARG2"
    ],
    "eigenvalue*****variance": [
        "$ARG1 \u03bbi of C corresponds to the $ARG2"
    ],
    "variance*****eigenvector": [
        "$ARG1 of x in the direction of $ARG2"
    ],
    "linear factor models*****covariance": [
        "$ARG1 optimal reconstruction error ( choosing \u00b5 , b , V and W as above ) is min E [ ||x \u2212 x\u0302|| ] = Hence , if the $ARG2"
    ],
    "multiple*****task": [
        "$ARG1 modalities or domains , or to transfer learned knowledge to tasks for which few or no examples are given but a $ARG2"
    ],
    "representation learning*****distributed representation": [
        "$ARG1 , starting with the theoretical advantages of $ARG2"
    ],
    "distributed representation*****data generating process": [
        "$ARG1 ( Hinton et al. , 1986 ) and deep representations and ending with the more general idea of underlying assumptions about the $ARG2"
    ],
    "principle*****computer science": [
        "$ARG1 applicable to daily life , $ARG2",
        "$ARG1 throughout $ARG2"
    ],
    "computer science*****machine learning": [
        "$ARG1 in general , and to $ARG2",
        "$ARG1 , and especially so in $ARG2"
    ],
    "number*****list": [
        "$ARG1 into the correct position in a sorted $ARG2"
    ],
    "list*****operation": [
        "$ARG1 of numbers is an O ( n ) $ARG2",
        "$ARG1 of inputs that are supplied to the $ARG2"
    ],
    "operation*****list": [
        "$ARG1 if the $ARG2"
    ],
    "list*****machine learning": [
        "$ARG1 is represented as a In the context of $ARG2",
        "$ARG1 here are intended only to provide examples of what $ARG2"
    ],
    "representation learning*****learning": [
        "$ARG1 another ? Generally speaking , a good representation is one that makes a subsequent $ARG2",
        "$ARG1 Figure 15.1 : Visualization via nonlinear projection of the $ARG2",
        "$ARG1 are based on introducing clues that help the $ARG2"
    ],
    "supervised learning*****representation learning": [
        "$ARG1 as performing a kind of $ARG2"
    ],
    "hidden layer*****classi\ufb01cation": [
        "$ARG1 ) taking on properties that make the $ARG2"
    ],
    "other*****representation learning": [
        "$ARG1 kinds of $ARG2",
        "$ARG1 approaches to $ARG2"
    ],
    "example*****density estimation": [
        "$ARG1 , suppose we want to learn a representation that makes $ARG2",
        "$ARG1 , if we have performed $ARG2",
        "$ARG1 , this arises frequently in the context of $ARG2",
        "$ARG1 , it is di\ufb03cult to generate new fake data for a $ARG2"
    ],
    "objective function*****vector": [
        "$ARG1 that encourages the elements of the representation $ARG2"
    ],
    "multiple*****representation learning": [
        "$ARG1 tasks ( some supervised , some unsupervised ) can be learned together with some shared internal Most $ARG2",
        "$ARG1 explanatory factors : Many $ARG2"
    ],
    "representation learning*****information": [
        "$ARG1 problems face a tradeo\ufb00 between preserving as much $ARG2"
    ],
    "information*****independence": [
        "$ARG1 about the input as possible and attaining nice properties ( such as $ARG2"
    ],
    "representation learning*****semi-supervised learning": [
        "$ARG1 is particularly interesting because it provides one way to perform unsupervised and $ARG2"
    ],
    "supervised learning*****subset": [
        "$ARG1 techniques on the labeled $ARG2"
    ],
    "semi-supervised learning*****problem": [
        "$ARG1 o\ufb00ers the chance to resolve this over\ufb01tting $ARG2",
        "$ARG1 should improve An important research $ARG2"
    ],
    "human*****example": [
        "$ARG1 performance\u2014for $ARG2",
        "$ARG1 observer can not tell the di\ufb00erence between the original $ARG2",
        "$ARG1 su\ufb00ering ( for $ARG2"
    ],
    "example*****inference": [
        "$ARG1 , the brain may use very large ensembles of classi\ufb01ers or Bayesian $ARG2",
        "$ARG1 , performing $ARG2"
    ],
    "unsupervised learning*****neural network": [
        "$ARG1 played a key historical role in the revival of deep $ARG2"
    ],
    "neural network*****convolution": [
        "$ARG1 , enabling researchers for the \ufb01rst time to train a deep supervised network without requiring architectural specializations like $ARG2",
        "$ARG1 that use $ARG2"
    ],
    "example*****task": [
        "$ARG1 of how a representation learned for one $ARG2",
        "$ARG1 with a corpus containing billions of words ) , learn a good representation ( typically of words , but also of sentences ) , and then use this representation or \ufb01ne-tune it for a supervised $ARG2",
        "$ARG1 , consider the $ARG2",
        "$ARG1 of the transfer $ARG2",
        "$ARG1 of a fully functioning feedforward network on a very simple $ARG2",
        "$ARG1 , if we want a robot to be able to walk , then walking is the $ARG2",
        "$ARG1 of a regression $ARG2",
        "$ARG1 of an anomaly detection $ARG2",
        "$ARG1 , imagine we have a regression $ARG2",
        "$ARG1 , we could easily solve the checkerboard $ARG2",
        "$ARG1 of a sampling $ARG2"
    ],
    "task*****unsupervised learning": [
        "$ARG1 ( $ARG2",
        "$ARG1 is to copy the input to the output ( $ARG2"
    ],
    "unsupervised learning*****distribution": [
        "$ARG1 , trying to capture the shape of the input $ARG2",
        "$ARG1 , taking the output of the previous layer and producing as output a new representation of the data , whose $ARG2",
        "$ARG1 technologies to learn the joint $ARG2"
    ],
    "task*****supervised learning": [
        "$ARG1 ( $ARG2",
        "$ARG1 may also be useful for the $ARG2",
        "$ARG1 and a $ARG2",
        "$ARG1 ( with a $ARG2"
    ],
    "greedy layer-wise unsupervised pretraining*****representation learning": [
        "$ARG1 relies on a single-layer $ARG2"
    ],
    "algorithm*****sparse coding": [
        "$ARG1 such as an RBM , a single-layer autoencoder , a $ARG2",
        "$ARG1 , $ARG2",
        "$ARG1 ( spike and slab $ARG2"
    ],
    "distribution*****relation": [
        "$ARG1 ( or its $ARG2"
    ],
    "relation*****other": [
        "$ARG1 to $ARG2"
    ],
    "deep learning*****learning": [
        "$ARG1 renaissance of 2006 began with the discovery that this greedy $ARG2",
        "$ARG1 is capable of $ARG2",
        "$ARG1 researchers typically can not a\ufb00ord the same scale of distributed $ARG2",
        "$ARG1 Time ( epochs ) Figure 7.3 : $ARG2"
    ],
    "learning*****initialization": [
        "$ARG1 procedure could be used to \ufb01nd a good $ARG2"
    ],
    "initialization*****learning": [
        "$ARG1 for a joint $ARG2",
        "$ARG1 may not persist after $ARG2"
    ],
    "pretraining*****unsupervised pretraining": [
        "$ARG1 is not required to train fully connected deep architectures , but the $ARG2"
    ],
    "unsupervised pretraining*****method": [
        "$ARG1 approach was the \ufb01rst $ARG2"
    ],
    "method*****pretraining": [
        "$ARG1 to Greedy layer-wise $ARG2"
    ],
    "representation learning*****solution": [
        "$ARG1 rithm , meaning that it optimizes each piece of the $ARG2"
    ],
    "pretraining*****algorithm": [
        "$ARG1 , because it is supposed to be only a \ufb01rst step before a joint training $ARG2"
    ],
    "task*****pretraining": [
        "$ARG1 , it can be viewed as a regularizer ( in some experiments , $ARG2"
    ],
    "pretraining*****parameter initialization": [
        "$ARG1 decreases test error without decreasing training error ) and a form of $ARG2"
    ],
    "pretraining*****supervised learning": [
        "$ARG1 phase and a $ARG2",
        "$ARG1 algorithms that break $ARG2"
    ],
    "supervised learning*****pretraining": [
        "$ARG1 phase may involve training a simple classi\ufb01er on top of the features learned in the $ARG2",
        "$ARG1 simultaneously , instead of using the $ARG2"
    ],
    "pretraining*****supervised \ufb01ne-tuning": [
        "$ARG1 phase , or it may involve $ARG2"
    ],
    "supervised \ufb01ne-tuning*****pretraining": [
        "$ARG1 of the entire network learned in the $ARG2"
    ],
    "matter*****unsupervised learning": [
        "$ARG1 what kind of $ARG2"
    ],
    "algorithm*****unsupervised pretraining": [
        "$ARG1 will obviously impact the details , most applications of $ARG2"
    ],
    "greedy layer-wise unsupervised pretraining*****initialization": [
        "$ARG1 can also be used as $ARG2"
    ],
    "initialization*****other": [
        "$ARG1 for $ARG2",
        "$ARG1 of $ARG2"
    ],
    "deep belief network*****deep boltzmann machine": [
        "$ARG1 ( Hinton et al. , 2006 ) and $ARG2",
        "$ARG1 and $ARG2"
    ],
    "unsupervised pretraining*****greedy layer-wise unsupervised pretraining": [
        "$ARG1 Work ? On many tasks , $ARG2"
    ],
    "greedy layer-wise unsupervised pretraining*****classi\ufb01cation": [
        "$ARG1 can yield substantial improvements in test error for $ARG2"
    ],
    "algorithm*****greedy layer-wise unsupervised pretraining": [
        "$ARG1 15.1 $ARG2"
    ],
    "algorithm*****set": [
        "$ARG1 L , which takes a training $ARG2",
        "$ARG1 , we are implicitly stating some $ARG2",
        "$ARG1 6.6 Require : T , the target $ARG2",
        "$ARG1 can discover a good $ARG2",
        "$ARG1 to partition the $ARG2",
        "$ARG1 itself ; we discuss how to $ARG2",
        "$ARG1 must learn a $ARG2",
        "$ARG1 is allowed to gain experience by observing a training $ARG2",
        "$ARG1 has the $ARG2",
        "$ARG1 can generalize well from a \ufb01nite training $ARG2",
        "$ARG1 to memorize speci\ufb01c instances of the training $ARG2",
        "$ARG1 is strongly a\ufb00ected not just by how large we make the $ARG2",
        "$ARG1 divides the training $ARG2",
        "$ARG1 that maintains a $ARG2"
    ],
    "set*****encoder": [
        "$ARG1 of examples and returns an $ARG2",
        "$ARG1 fx : $ARG2"
    ],
    "encoder*****feature": [
        "$ARG1 or $ARG2"
    ],
    "function*****row": [
        "$ARG1 f. The raw input data is X , with one $ARG2"
    ],
    "1*****encoder": [
        "$ARG1 ) ( X ) is the output of the \ufb01rst stage $ARG2",
        "$ARG1 ) an $ARG2"
    ],
    "function*****supervised \ufb01ne-tuning": [
        "$ARG1 f , input examples X ( and in the $ARG2"
    ],
    "supervised \ufb01ne-tuning*****function": [
        "$ARG1 case , associated targets Y ) , and returns a tuned $ARG2"
    ],
    "function*****1": [
        "$ARG1 for k = $ARG2",
        "$ARG1 that increases and decreases many times in many di\ufb00erent regions , $ARG2",
        "$ARG1 returns $ARG2",
        "$ARG1 f ( $ARG2",
        "$ARG1 should f ( $ARG2",
        "$ARG1 , we can see that it saturates only when ( $ARG2",
        "$ARG1 , where f ( [ 0 , $ARG2",
        "$ARG1 f maps the state at t to the state at t + $ARG2",
        "$ARG1 be close to $ARG2",
        "$ARG1 of the parameters produced by step t \u2212 $ARG2",
        "$ARG1 f : Rn \u2192 { $ARG2",
        "$ARG1 of the data : \u03b8\u0302m = g ( x ( $ARG2",
        "$ARG1 into the interval ( 0 , $ARG2",
        "$ARG1 with I \ue030 ( x , y ) = I ( x \u2212 $ARG2",
        "$ARG1 ( Dugas et al. , \u03b6 ( x ) = log ( $ARG2"
    ],
    "other*****unsupervised pretraining": [
        "$ARG1 tasks , however , $ARG2",
        "$ARG1 words , $ARG2",
        "$ARG1 algorithms besides $ARG2",
        "$ARG1 approaches to $ARG2"
    ],
    "pretraining*****machine learning": [
        "$ARG1 on $ARG2"
    ],
    "machine learning*****activity": [
        "$ARG1 models for chemical $ARG2"
    ],
    "activity*****average": [
        "$ARG1 prediction and found that , on $ARG2"
    ],
    "average*****pretraining": [
        "$ARG1 , $ARG2"
    ],
    "other*****semi-supervised learning": [
        "$ARG1 , completely di\ufb00erent paradigms for performing $ARG2",
        "$ARG1 arguments about when $ARG2"
    ],
    "semi-supervised learning*****neural network": [
        "$ARG1 with $ARG2"
    ],
    "neural network*****adversarial training": [
        "$ARG1 , such as virtual $ARG2"
    ],
    "representation learning*****neural network": [
        "$ARG1 the idea that the choice of initial parameters for a deep $ARG2"
    ],
    "neural network*****model": [
        "$ARG1 can have a signi\ufb01cant regularizing e\ufb00ect on the $ARG2",
        "$ARG1 is not to perfectly $ARG2",
        "$ARG1 today are based on a $ARG2",
        "$ARG1 with a maximum entropy $ARG2",
        "$ARG1 , but they may also be raw input to the $ARG2",
        "$ARG1 for collaborative \ufb01ltering is based on the RBM undirected probabilistic $ARG2"
    ],
    "model*****optimization": [
        "$ARG1 ( and , to a lesser extent , that it can improve $ARG2",
        "$ARG1 capacity and imperfect $ARG2",
        "$ARG1 family rather than changing the $ARG2",
        "$ARG1 design strategies can help to make $ARG2",
        "$ARG1 by maximizing a lower bound on the log-likelihood of the In practical applications of PSD , the iterative $ARG2",
        "$ARG1 capacity , add or remove regularizing features , improve the $ARG2"
    ],
    "learning*****distribution": [
        "$ARG1 about the input $ARG2",
        "$ARG1 the means of a $ARG2",
        "$ARG1 to draw samples from a $ARG2",
        "$ARG1 to denoise data from some $ARG2",
        "$ARG1 is to impose the restriction that q is a factorial $ARG2",
        "$ARG1 anything useful about the data $ARG2"
    ],
    "distribution*****mapping": [
        "$ARG1 can help to learn about the $ARG2"
    ],
    "mapping*****machine learning": [
        "$ARG1 from inputs to Both of these ideas involve many complicated interactions between several parts of the $ARG2"
    ],
    "problem*****neural network": [
        "$ARG1 for $ARG2",
        "$ARG1 is generally believed to be present in $ARG2",
        "$ARG1 when 12 \ue00f2g \ue03eHg exceeds \ue00fg\ue03e g. To determine whether ill-conditioning is detrimental to a $ARG2",
        "$ARG1 plaguing $ARG2",
        "$ARG1 remains an active area of research , but experts now suspect that , for su\ufb03ciently large $ARG2",
        "$ARG1 for $ARG2"
    ],
    "example*****cost function": [
        "$ARG1 , a region that is surrounded by areas where the $ARG2",
        "$ARG1 , we can design the $ARG2",
        "$ARG1 , consider the $ARG2"
    ],
    "cost function*****example": [
        "$ARG1 varies so much from one $ARG2"
    ],
    "example*****gradient": [
        "$ARG1 to another that minibatches give only a very noisy estimate of the $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the exploding $ARG2"
    ],
    "gradient*****hessian matrix": [
        "$ARG1 , or a region surrounded by areas where the $ARG2",
        "$ARG1 , poor conditioning of the $ARG2",
        "$ARG1 or $ARG2",
        "$ARG1 estimates , improve conditioning of the $ARG2",
        "$ARG1 in this setting is given by \u02c6 w ) = H ( w \u2212 w \u2217 ) , where , again , H is the $ARG2"
    ],
    "hessian matrix*****gradient descent": [
        "$ARG1 is so poorly conditioned that $ARG2"
    ],
    "unsupervised learning*****supervised learning": [
        "$ARG1 and $ARG2",
        "$ARG1 techniques and the ability of deep models to generalize well from small datasets , but today there is more interest in much older $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2"
    ],
    "optimization*****supervised learning": [
        "$ARG1 in the $ARG2"
    ],
    "supervised learning*****information": [
        "$ARG1 stage preserves $ARG2"
    ],
    "information*****unsupervised learning": [
        "$ARG1 from the $ARG2"
    ],
    "unsupervised learning*****feature": [
        "$ARG1 stage by simply freezing the parameters for the $ARG2"
    ],
    "feature*****supervised learning": [
        "$ARG1 extractors and using $ARG2"
    ],
    "algorithm*****information": [
        "$ARG1 can use $ARG2",
        "$ARG1 to achieve more compression while discarding less $ARG2"
    ],
    "information*****supervised learning": [
        "$ARG1 learned in the unsupervised phase to perform better in the $ARG2"
    ],
    "unsupervised learning*****output layer": [
        "$ARG1 can be preferable\u2014the constraints imposed by the $ARG2"
    ],
    "view*****unsupervised pretraining": [
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2"
    ],
    "unsupervised pretraining*****learning": [
        "$ARG1 as $ARG2"
    ],
    "learning*****unsupervised pretraining": [
        "$ARG1 a representation , we can expect $ARG2"
    ],
    "example*****word embedding": [
        "$ARG1 of this is the use of $ARG2"
    ],
    "distance*****other": [
        "$ARG1 from each $ARG2",
        "$ARG1 from each $ARG2",
        "$ARG1 from each $ARG2",
        "$ARG1 2 from each $ARG2",
        "$ARG1 metrics $ARG2"
    ],
    "other*****distance": [
        "$ARG1 ( squared L2 $ARG2"
    ],
    "word embedding*****similarity": [
        "$ARG1 naturally encode $ARG2"
    ],
    "similarity*****distance": [
        "$ARG1 between words by their $ARG2",
        "$ARG1 metric between any pair of points in x space that may be more meaningful than $ARG2",
        "$ARG1 space , in which semantically close concepts ( or inputs ) are close in $ARG2"
    ],
    "vector*****quality": [
        "$ARG1 space where distances provide a low $ARG2"
    ],
    "unsupervised pretraining*****number": [
        "$ARG1 to be most helpful when the $ARG2",
        "$ARG1 to perform best when the $ARG2"
    ],
    "information*****unsupervised pretraining": [
        "$ARG1 added by $ARG2"
    ],
    "semi-supervised learning*****unsupervised pretraining": [
        "$ARG1 via $ARG2"
    ],
    "unsupervised pretraining*****transfer learning": [
        "$ARG1 winning two international $ARG2"
    ],
    "transfer learning*****number": [
        "$ARG1 competitions ( Mesnil et al. , 2011 ; Goodfellow et al. , 2011 ) , in settings where the $ARG2"
    ],
    "number*****task": [
        "$ARG1 of labeled examples in the target $ARG2"
    ],
    "example*****unsupervised pretraining": [
        "$ARG1 , $ARG2"
    ],
    "unsupervised pretraining*****function": [
        "$ARG1 is likely to be most useful when the $ARG2",
        "$ARG1 consistently halt in the same region of $ARG2"
    ],
    "unsupervised learning*****weight decay": [
        "$ARG1 di\ufb00ers from regularizers like $ARG2"
    ],
    "weight decay*****bias": [
        "$ARG1 because it does not $ARG2"
    ],
    "bias*****function": [
        "$ARG1 the learner toward discovering a simple $ARG2"
    ],
    "function*****feature": [
        "$ARG1 but rather toward discovering $ARG2",
        "$ARG1 of a small part of space , but there is no reason to think that the same $ARG2"
    ],
    "feature*****unsupervised learning": [
        "$ARG1 functions that are useful for the $ARG2"
    ],
    "distribution*****unsupervised learning": [
        "$ARG1 , $ARG2",
        "$ARG1 p ( x , y ) and p ( x , y ) p ( y | x ) = \ue050 y\ue030 p ( x , y ) Though $ARG2"
    ],
    "unsupervised pretraining*****view": [
        "$ARG1 has usually been used to improve classi\ufb01ers , and is usually most interesting from the point of $ARG2"
    ],
    "learning*****neural network": [
        "$ARG1 trajectories of di\ufb00erent $ARG2",
        "$ARG1 in deep $ARG2",
        "$ARG1 trajectories of state-of-the-art $ARG2",
        "$ARG1 algorithms that are e\ufb00ective for training $ARG2"
    ],
    "function*****unsupervised pretraining": [
        "$ARG1 space ( not parameter space , to avoid the issue of many-to-one mappings from parameter vectors to functions ) , with di\ufb00erent random initializations and with or without $ARG2"
    ],
    "neural network*****process": [
        "$ARG1 , at a particular time during its training $ARG2",
        "$ARG1 could $ARG2",
        "$ARG1 that can $ARG2",
        "$ARG1 to scale to very large sizes and $ARG2"
    ],
    "function*****vector": [
        "$ARG1 space is an in\ufb01nitedimensional $ARG2",
        "$ARG1 , with its result represented as a one-hot $ARG2",
        "$ARG1 described by a parameter $ARG2",
        "$ARG1 J ( \u03b8 ) by \ufb01nding the input $ARG2",
        "$ARG1 with respect to a $ARG2",
        "$ARG1 with respect to the $ARG2"
    ],
    "learning*****function": [
        "$ARG1 moves the $ARG2",
        "$ARG1 the XOR $ARG2",
        "$ARG1 as choosing a $ARG2",
        "$ARG1 a piecewise linear $ARG2",
        "$ARG1 a nonlinear $ARG2",
        "$ARG1 a hashing $ARG2"
    ],
    "isomap*****estimator": [
        "$ARG1 tries to preserve global relative distances ( and hence volumes ) so the small region corresponding to pretrained models may indicate that the pretraining-based $ARG2"
    ],
    "representation learning*****set": [
        "$ARG1 reducing test $ARG2",
        "$ARG1 hx = fx ( x ) hy = fy ( y ) ( x , y ) pairs in the training $ARG2"
    ],
    "unsupervised pretraining*****other": [
        "$ARG1 can help tasks $ARG2"
    ],
    "other*****classi\ufb01cation": [
        "$ARG1 than $ARG2",
        "$ARG1 variants of the $ARG2"
    ],
    "classi\ufb01cation*****optimization": [
        "$ARG1 , and can act to improve $ARG2"
    ],
    "gradient*****early stopping": [
        "$ARG1 becomes small , a point where $ARG2"
    ],
    "early stopping*****gradient": [
        "$ARG1 ends training to prevent over\ufb01tting , or at a point where the $ARG2"
    ],
    "neural network*****unsupervised pretraining": [
        "$ARG1 that receive $ARG2"
    ],
    "neural network*****pretraining": [
        "$ARG1 without $ARG2"
    ],
    "pretraining*****variance": [
        "$ARG1 reduces the $ARG2",
        "$ARG1 works best\u2014the mean and $ARG2"
    ],
    "variance*****process": [
        "$ARG1 of the estimation $ARG2"
    ],
    "process*****turn": [
        "$ARG1 , which can in $ARG2",
        "$ARG1 , and this in $ARG2"
    ],
    "unsupervised pretraining*****neural network": [
        "$ARG1 initializes $ARG2"
    ],
    "neural network*****initialization": [
        "$ARG1 parameters into a region that they do not escape , and the results following this $ARG2"
    ],
    "variance*****pretraining": [
        "$ARG1 of the test error were most reduced by $ARG2"
    ],
    "recti\ufb01ed linear unit*****dropout": [
        "$ARG1 , $ARG2",
        "$ARG1 and $ARG2"
    ],
    "dropout*****batch normalization": [
        "$ARG1 and $ARG2",
        "$ARG1 or $ARG2"
    ],
    "batch normalization*****unsupervised pretraining": [
        "$ARG1 ) so less is known about the e\ufb00ect of $ARG2"
    ],
    "pretraining*****learning": [
        "$ARG1 encourages the $ARG2"
    ],
    "other*****unsupervised learning": [
        "$ARG1 forms of $ARG2",
        "$ARG1 tasks is usually considered $ARG2"
    ],
    "unsupervised learning*****unsupervised pretraining": [
        "$ARG1 , $ARG2"
    ],
    "regularization*****hyperparameter": [
        "$ARG1 by adjusting the value of a single $ARG2",
        "$ARG1 by scaling the penalty \u2126 using a positive $ARG2"
    ],
    "unsupervised pretraining*****regularization": [
        "$ARG1 does not o\ufb00er a clear way to adjust the the strength of the $ARG2",
        "$ARG1 may o\ufb00er some $ARG2"
    ],
    "representation learning*****hyperparameters": [
        "$ARG1 very many $ARG2"
    ],
    "pretraining*****hyperparameter": [
        "$ARG1 strategy , there is a single $ARG2"
    ],
    "hyperparameter*****model": [
        "$ARG1 , usually a coe\ufb03cient attached to the unsupervised cost , that determines how strongly the unsupervised objective will regularize the supervised $ARG2"
    ],
    "unsupervised pretraining*****model": [
        "$ARG1 , there is not a way of \ufb02exibly adapting the strength of the regularization\u2014either the supervised $ARG2"
    ],
    "set*****hyperparameters": [
        "$ARG1 error in the supervised phase in order to select the $ARG2",
        "$ARG1 , such $ARG2",
        "$ARG1 is used to \u201c train \u201d the $ARG2",
        "$ARG1 error is then chosen as having found the best $ARG2",
        "$ARG1 error that results from training using these $ARG2",
        "$ARG1 with respect to the $ARG2",
        "$ARG1 error , as in the case of discrete-valued $ARG2",
        "$ARG1 of $ARG2"
    ],
    "hyperparameters*****pretraining": [
        "$ARG1 of the $ARG2"
    ],
    "hyperparameters*****number": [
        "$ARG1 , like the $ARG2",
        "$ARG1 are discrete , such as the $ARG2",
        "$ARG1 , each taking at most n values , then the $ARG2"
    ],
    "number*****pretraining": [
        "$ARG1 of $ARG2"
    ],
    "pretraining*****set": [
        "$ARG1 iterations , are more conveniently $ARG2",
        "$ARG1 is that one can pretrain once on a huge unlabeled $ARG2"
    ],
    "set*****pretraining": [
        "$ARG1 during the $ARG2"
    ],
    "pretraining*****early stopping": [
        "$ARG1 phase , using $ARG2"
    ],
    "unsupervised pretraining*****natural language processing": [
        "$ARG1 has been largely abandoned , except in the \ufb01eld of $ARG2"
    ],
    "natural language processing*****similarity": [
        "$ARG1 , where the natural representation of words as one-hot vectors conveys no $ARG2"
    ],
    "set*****example": [
        "$ARG1 ( for $ARG2",
        "$ARG1 , we insert this symbol as an extra member of the sequence , immediately after x ( \u03c4 ) in each training $ARG2",
        "$ARG1 , the learner is similar to a living being who sees a new $ARG2",
        "$ARG1 with the goal of putting each $ARG2",
        "$ARG1 very badly unless additional assumptions are made linking the di\ufb00erent entries in the table ( for $ARG2",
        "$ARG1 sizes on a logarithmic scale , for $ARG2"
    ],
    "task*****set": [
        "$ARG1 for which the training $ARG2",
        "$ARG1 must be found via experimentation guided by monitoring the validation $ARG2",
        "$ARG1 provides a $ARG2",
        "$ARG1 , the computer program sifts through a $ARG2",
        "$ARG1 not only on the training $ARG2",
        "$ARG1 , we often observe that training error decreases steadily over time , but validation $ARG2",
        "$ARG1 ( with the same $ARG2",
        "$ARG1 by specifying a $ARG2"
    ],
    "deep learning*****supervised learning": [
        "$ARG1 techniques based on $ARG2",
        "$ARG1 has largely solved the $ARG2",
        "$ARG1 provides a very powerful framework for $ARG2"
    ],
    "supervised learning*****dropout": [
        "$ARG1 , regularized with $ARG2"
    ],
    "dataset*****unsupervised pretraining": [
        "$ARG1 , Bayesian methods outperform methods based on $ARG2"
    ],
    "unsupervised pretraining*****deep learning": [
        "$ARG1 remains an important milestone in the history of $ARG2"
    ],
    "pretraining*****transfer learning": [
        "$ARG1 discussed in section 8.7.4 , as a very common approach for $ARG2",
        "$ARG1 for $ARG2",
        "$ARG1 extends the idea to the context of $ARG2"
    ],
    "transfer learning*****convolutional network": [
        "$ARG1 is popular ( Oquab et al. , 2014 ; Yosinski et al. , 2014 ) for use with $ARG2"
    ],
    "convolutional network*****dataset": [
        "$ARG1 pretrained on the ImageNet $ARG2"
    ],
    "transfer learning*****domain adaptation": [
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2"
    ],
    "domain adaptation*****distribution": [
        "$ARG1 refer to the situation where what has been learned in one setting ( i.e. , $ARG2"
    ],
    "distribution*****generalization": [
        "$ARG1 P1 ) is exploited to improve $ARG2"
    ],
    "generalization*****distribution": [
        "$ARG1 in another setting ( say $ARG2"
    ],
    "transfer learning*****learning": [
        "$ARG1 , the learner must perform two or more di\ufb00erent tasks , but we assume that many of the factors that explain the variations in P1 are relevant to the variations that need to be captured for $ARG2"
    ],
    "transfer learning*****multi-task learning": [
        "$ARG1 , $ARG2"
    ],
    "multi-task learning*****domain adaptation": [
        "$ARG1 ( section 7.7 ) , and $ARG2"
    ],
    "domain adaptation*****representation learning": [
        "$ARG1 can be achieved via $ARG2"
    ],
    "example*****speech recognition": [
        "$ARG1 , a $ARG2",
        "$ARG1 , in $ARG2",
        "$ARG1 , one might train a $ARG2",
        "$ARG1 is $ARG2",
        "$ARG1 , if we want to train a $ARG2"
    ],
    "system*****output layer": [
        "$ARG1 needs to produce valid sentences at the $ARG2"
    ],
    "neural network*****preprocessing": [
        "$ARG1 , and have a task-speci\ufb01c $ARG2"
    ],
    "1*****example": [
        "$ARG1 ) x ( 2 ) h ( 3 ) x ( 3 ) Figure 15.2 : $ARG2",
        "$ARG1 and w2 , for $ARG2",
        "$ARG1 ) y ( 2 ) y ( ... ) y ( n y ) Figure 10.12 : $ARG2",
        "$ARG1 \u2212 \u03b1 ) v ( t ) the \u03b1 parameter is an $ARG2",
        "$ARG1 , in order to avoid errors when computing , for $ARG2",
        "$ARG1 0 \uf8fb Figure 2.2 : $ARG2",
        "$ARG1 ) H ( 2 ) H ( 3 ) Figure 9.17 : An $ARG2",
        "$ARG1 \uf8fa = \uf8ef \u22121 5 2 \u22123 0 \u22123 \u22125 4 \u22122 2 \u22125 \u22121 B \u2208 R m\u00d7n In the \ufb01rst expression , we have an $ARG2",
        "$ARG1 \ue059 ( i ) \ue010 ( i ) \ue011 p ( x ) = See \ufb01gure 3.8 for an $ARG2"
    ],
    "example*****transfer learning": [
        "$ARG1 architecture for multi-task or $ARG2"
    ],
    "transfer learning*****variable": [
        "$ARG1 when the output $ARG2"
    ],
    "variable*****dimension": [
        "$ARG1 x has a di\ufb00erent meaning ( and possibly even a di\ufb00erent $ARG2"
    ],
    "dimension*****task": [
        "$ARG1 ) for each $ARG2"
    ],
    "task*****example": [
        "$ARG1 ( or , for $ARG2",
        "$ARG1 , for $ARG2"
    ],
    "domain adaptation*****task": [
        "$ARG1 , the $ARG2"
    ],
    "task*****mapping": [
        "$ARG1 ( and the optimal input-tooutput $ARG2"
    ],
    "mapping*****distribution": [
        "$ARG1 ) remains the same between each setting , but the input $ARG2",
        "$ARG1 the raw input to some representation ) , such that when we apply this learned transformation to inputs from the transfer setting ( $ARG2",
        "$ARG1 a variable-length sequence of x values into a $ARG2"
    ],
    "domain adaptation*****content": [
        "$ARG1 scenario can arise when a sentiment predictor trained on customer reviews of media $ARG2"
    ],
    "function*****statement": [
        "$ARG1 that tells whether any $ARG2"
    ],
    "unsupervised pretraining*****denoising autoencoder": [
        "$ARG1 ( with $ARG2"
    ],
    "denoising autoencoder*****domain adaptation": [
        "$ARG1 ) has been found to be very successful for sentiment analysis with $ARG2"
    ],
    "problem*****concept drift": [
        "$ARG1 is that of $ARG2"
    ],
    "concept drift*****view": [
        "$ARG1 , which we can $ARG2"
    ],
    "view*****transfer learning": [
        "$ARG1 as a form of $ARG2"
    ],
    "transfer learning*****distribution": [
        "$ARG1 due to gradual changes in the data $ARG2"
    ],
    "concept drift*****transfer learning": [
        "$ARG1 and $ARG2"
    ],
    "multi-task learning*****supervised learning": [
        "$ARG1 \u201d typically refers to $ARG2"
    ],
    "supervised learning*****transfer learning": [
        "$ARG1 tasks , the more general notion of $ARG2"
    ],
    "transfer learning*****unsupervised learning": [
        "$ARG1 is applicable to $ARG2"
    ],
    "unsupervised learning*****reinforcement learning": [
        "$ARG1 and $ARG2"
    ],
    "information*****learning": [
        "$ARG1 that may be useful when $ARG2",
        "$ARG1 theory is that $ARG2"
    ],
    "deep learning*****transfer learning": [
        "$ARG1 for $ARG2"
    ],
    "transfer learning*****machine learning": [
        "$ARG1 has found success in some $ARG2"
    ],
    "dataset*****distribution": [
        "$ARG1 from the \ufb01rst setting ( from $ARG2",
        "$ARG1 , where all missing variables have their values provided by an estimate of the posterior $ARG2",
        "$ARG1 of training examples as specifying the $ARG2"
    ],
    "feature*****mapping": [
        "$ARG1 space ( $ARG2"
    ],
    "transfer learning*****one-shot learning": [
        "$ARG1 are $ARG2"
    ],
    "one-shot learning*****zero-shot learning": [
        "$ARG1 and $ARG2",
        "$ARG1 , while no labeled examples are given at all for the $ARG2"
    ],
    "zero-shot learning*****zero-data learning": [
        "$ARG1 , sometimes also called $ARG2"
    ],
    "task*****one-shot learning": [
        "$ARG1 is given for $ARG2"
    ],
    "transfer learning*****example": [
        "$ARG1 stage , only one labeled $ARG2"
    ],
    "factors of variation*****other": [
        "$ARG1 corresponding to these invariances have been cleanly separated from the $ARG2"
    ],
    "other*****matter": [
        "$ARG1 factors , in the learned representation space , and we have somehow learned which factors do and do not $ARG2"
    ],
    "example*****zero-shot learning": [
        "$ARG1 of a $ARG2"
    ],
    "zero-shot learning*****problem": [
        "$ARG1 setting , consider the $ARG2"
    ],
    "problem*****collection": [
        "$ARG1 of having a learner read a large $ARG2"
    ],
    "collection*****object recognition": [
        "$ARG1 of text and then solve $ARG2"
    ],
    "representation learning*****image": [
        "$ARG1 It may be possible to recognize a speci\ufb01c object class even without having seen an $ARG2"
    ],
    "zero-data learning*****zero-shot learning": [
        "$ARG1 ( Larochelle et al. , 2008 ) and $ARG2"
    ],
    "zero-shot learning*****information": [
        "$ARG1 ( Palatucci et al. , 2009 ; Socher et al. , 2013b ) are only possible because additional $ARG2"
    ],
    "zero-data learning*****random variable": [
        "$ARG1 scenario as including three $ARG2"
    ],
    "random variable*****task": [
        "$ARG1 describing the $ARG2",
        "$ARG1 is a challenging $ARG2"
    ],
    "example*****variable": [
        "$ARG1 of recognizing cats after having read about cats , the output is a binary $ARG2",
        "$ARG1 , we have one $ARG2",
        "$ARG1 , suppose s is a $ARG2",
        "$ARG1 , suppose s is a $ARG2",
        "$ARG1 , suppose that c is a $ARG2",
        "$ARG1 , we use the $ARG2",
        "$ARG1 , if we want to classify images of $ARG2"
    ],
    "1*****task": [
        "$ARG1 indicating \u201c yes \u201d and y = 0 indicating \u201c no. \u201d The $ARG2",
        "$ARG1 ) and h ( 2 ) are specialized to each $ARG2"
    ],
    "variable*****image": [
        "$ARG1 T then represents questions to be answered such as \u201c Is there a cat in this $ARG2"
    ],
    "image*****set": [
        "$ARG1 ? \u201d If we have a training $ARG2",
        "$ARG1 ( usually described as a $ARG2",
        "$ARG1 at a $ARG2"
    ],
    "zero-shot learning*****generalization": [
        "$ARG1 requires T to be represented in a way that allows some sort of $ARG2"
    ],
    "distributed representation*****word embedding": [
        "$ARG1 of object categories by using a learned $ARG2"
    ],
    "phenomenon*****machine translation": [
        "$ARG1 happens in $ARG2"
    ],
    "machine translation*****other": [
        "$ARG1 ( Klementiev et al. , 2012 ; Mikolov et al. , 2013b ; Gouws et al. , 2014 ) : we have words in one language , and the relationships between words can be learned from unilingual corpora ; on the $ARG2"
    ],
    "translation*****distributed representation": [
        "$ARG1 for word A because we have learned a $ARG2"
    ],
    "zero-shot learning*****transfer learning": [
        "$ARG1 is a particular form of $ARG2"
    ],
    "principle*****multi-modal learning": [
        "$ARG1 explains how one can perform $ARG2"
    ],
    "function*****encoder": [
        "$ARG1 for x fy : $ARG2"
    ],
    "function*****transfer learning": [
        "$ARG1 for y Relationship between embedded points within one of the domains Maps between representation spaces Figure 15.3 : $ARG2"
    ],
    "transfer learning*****zero-shot learning": [
        "$ARG1 between two domains x and y enables $ARG2"
    ],
    "distance*****similarity": [
        "$ARG1 in hx space provides a $ARG2",
        "$ARG1 in hy space provides a $ARG2"
    ],
    "image*****other": [
        "$ARG1 of that word was ever presented , simply because word-representations f y ( ytest ) and image-representations fx ( xtest ) can be related to each $ARG2",
        "$ARG1 , with one axis corresponding to time and the $ARG2",
        "$ARG1 of a face or any $ARG2"
    ],
    "image*****feature": [
        "$ARG1 and that word were never paired , their respective $ARG2"
    ],
    "representation learning*****modality": [
        "$ARG1 in one $ARG2"
    ],
    "modality*****other": [
        "$ARG1 , a representation in the $ARG2",
        "$ARG1 and another observation y in the $ARG2"
    ],
    "distribution*****modality": [
        "$ARG1 ) between pairs ( x , y ) consisting of one observation x in one $ARG2"
    ],
    "learning*****other": [
        "$ARG1 all three sets of parameters ( from x to its representation , from y to its representation , and the relationship between the two representations ) , concepts in one representation are anchored in the $ARG2",
        "$ARG1 in feedforward networks has been viewed since 2012 as a powerful technology that may be applied to many $ARG2"
    ],
    "representation learning*****feature": [
        "$ARG1 is \u201c what makes one representation better than another ? \u201d One hypothesis is that an ideal representation is one in which the features within the representation correspond to the underlying causes of the observed data , with separate features or directions in $ARG2"
    ],
    "semi-supervised learning*****supervised learning": [
        "$ARG1 can outperform pure $ARG2"
    ],
    "representation learning*****example": [
        "$ARG1 , we have often been concerned with a representation that is easy to model\u2014for $ARG2",
        "$ARG1 p ( x ) Figure 15.4 : $ARG2",
        "$ARG1 Figure 15.6 : Predictive generative networks provide an $ARG2"
    ],
    "semi-supervised learning*****representation learning": [
        "$ARG1 via unsupervised $ARG2"
    ],
    "semi-supervised learning*****unsupervised learning": [
        "$ARG1 can fail because $ARG2"
    ],
    "set*****information": [
        "$ARG1 of x values alone gives us no $ARG2",
        "$ARG1 targets , they are unlikely to capture the necessary $ARG2",
        "$ARG1 , one must have $ARG2"
    ],
    "image*****example": [
        "$ARG1 data ) are statistically salient , just modeling p ( x ) in an unsupervised way with no labeled $ARG2"
    ],
    "example*****semi-supervised learning": [
        "$ARG1 of how $ARG2"
    ],
    "representation learning*****factors of variation": [
        "$ARG1 that tries to disentangle the underlying $ARG2"
    ],
    "factors of variation*****semi-supervised learning": [
        "$ARG1 is likely to be useful as a $ARG2"
    ],
    "process*****directed graphical model": [
        "$ARG1 can be conceived as structured according to this $ARG2"
    ],
    "model*****generalization": [
        "$ARG1 of x ( from a $ARG2",
        "$ARG1 allows $ARG2",
        "$ARG1 with the lowest $ARG2",
        "$ARG1 Our modern ideas about improving the $ARG2",
        "$ARG1 to have optimal capacity and yet still have a large gap between training and $ARG2",
        "$ARG1 class in order to promote $ARG2",
        "$ARG1 ( in the sense of minimizing $ARG2",
        "$ARG1 capacity , which achieves the lowest possible $ARG2"
    ],
    "generalization*****view": [
        "$ARG1 point of $ARG2"
    ],
    "representation learning*****variable": [
        "$ARG1 structure , with h as a latent $ARG2"
    ],
    "distribution*****rule": [
        "$ARG1 of y given x is tied by Bayes \u2019 $ARG2",
        "$ARG1 of these observations using the chain $ARG2"
    ],
    "solution*****other": [
        "$ARG1 is for an unsupervised learner to learn a representation that captures all the reasonably salient generative factors hj and disentangles them from each $ARG2"
    ],
    "solution*****factors of variation": [
        "$ARG1 is not feasible because it is not possible to capture all or most of the $ARG2"
    ],
    "example*****phenomenon": [
        "$ARG1 , in a visual scene , should the representation always encode all of the smallest objects in the background ? It is a well-documented psychological $ARG2"
    ],
    "phenomenon*****human": [
        "$ARG1 that $ARG2"
    ],
    "number*****supervised learning": [
        "$ARG1 of underlying causes are to use a $ARG2"
    ],
    "supervised learning*****unsupervised learning": [
        "$ARG1 signal at the same time as the $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 , or for just an input z ( i ) = x ( i ) in the case of $ARG2"
    ],
    "unsupervised learning*****model": [
        "$ARG1 signal so that the $ARG2"
    ],
    "model*****factors of variation": [
        "$ARG1 will choose to capture the most relevant $ARG2",
        "$ARG1 can learn a representation of images of faces , with separate directions in representation space capturing di\ufb00erent underlying $ARG2",
        "$ARG1 is constructed in such a way that it treats the $ARG2"
    ],
    "factors of variation*****unsupervised learning": [
        "$ARG1 , or to use much larger representations if using purely $ARG2"
    ],
    "example*****mean squared error": [
        "$ARG1 , $ARG2",
        "$ARG1 , as we saw in section 5.5.1 , if pmodel ( y | x ) = N ( y ; f ( x ; \u03b8 ) , I ) , then we recover the $ARG2",
        "$ARG1 , $ARG2"
    ],
    "mean squared error*****image": [
        "$ARG1 applied to the pixels of an $ARG2"
    ],
    "representation learning*****mean squared error": [
        "$ARG1 Figure 15.5 : An autoencoder trained with $ARG2"
    ],
    "mean squared error*****robotics": [
        "$ARG1 for a $ARG2"
    ],
    "image*****robotics": [
        "$ARG1 and are relevant to the $ARG2"
    ],
    "example*****group": [
        "$ARG1 , if a $ARG2"
    ],
    "human*****mean squared error": [
        "$ARG1 heads will often neglect to generate the ears when trained with $ARG2"
    ],
    "model*****human": [
        "$ARG1 of a $ARG2"
    ],
    "image*****mean squared error": [
        "$ARG1 produced by a predictive generative network trained with $ARG2"
    ],
    "model*****mean squared error": [
        "$ARG1 trained with a combination of $ARG2",
        "$ARG1 is to compute the $ARG2"
    ],
    "mean squared error*****loss": [
        "$ARG1 and adversarial $ARG2",
        "$ARG1 , the $ARG2"
    ],
    "task*****invariant": [
        "$ARG1 , the causal mechanisms remain $ARG2",
        "$ARG1 facing a classi\ufb01er is to be $ARG2"
    ],
    "invariant*****constant": [
        "$ARG1 ( the laws of the universe are $ARG2"
    ],
    "representation learning*****expected": [
        "$ARG1 be $ARG2"
    ],
    "expected*****learning": [
        "$ARG1 via $ARG2"
    ],
    "distributed representation*****set": [
        "$ARG1 of concepts\u2014representations composed of many elements that can be $ARG2"
    ],
    "set*****representation learning": [
        "$ARG1 separately from each other\u2014are one of the most important tools for $ARG2"
    ],
    "neural network*****multiple": [
        "$ARG1 with $ARG2",
        "$ARG1 and any models with $ARG2",
        "$ARG1 can be divided into $ARG2"
    ],
    "multiple*****distributed representation": [
        "$ARG1 latent variables make use of the strategy of $ARG2"
    ],
    "deep learning*****assumption": [
        "$ARG1 algorithms are motivated by the $ARG2"
    ],
    "distributed representation*****variable": [
        "$ARG1 are natural for this approach , because each direction in representation space can correspond to the value of a di\ufb00erent underlying con\ufb01guration $ARG2"
    ],
    "example*****distributed representation": [
        "$ARG1 of a $ARG2",
        "$ARG1 ) and that a linear classi\ufb01er on top of the $ARG2",
        "$ARG1 , our $ARG2"
    ],
    "distributed representation*****vector": [
        "$ARG1 is a $ARG2"
    ],
    "dictionary*****feature": [
        "$ARG1 , one can imagine n $ARG2"
    ],
    "learning*****k-means": [
        "$ARG1 algorithms based on non-distributed representations \u2022 Clustering methods , including the $ARG2"
    ],
    "1*****multiple": [
        "$ARG1 , there are $ARG2"
    ],
    "representation learning*****1": [
        "$ARG1 h = [ $ARG2"
    ],
    "illustration*****learning": [
        "$ARG1 of how a $ARG2",
        "$ARG1 ) , $ARG2"
    ],
    "algorithm*****distributed representation": [
        "$ARG1 based on a $ARG2",
        "$ARG1 ? $ARG2"
    ],
    "illustration*****boundary": [
        "$ARG1 , each line represents the decision $ARG2"
    ],
    "distributed representation*****algorithm": [
        "$ARG1 with n features assigns unique codes to O ( nd ) di\ufb00erent regions , while the nearest neighbor $ARG2"
    ],
    "distributed representation*****vc dimension": [
        "$ARG1 is not able to assign di\ufb00erent class identities to every neighboring region ; even a deep linear-threshold network has a $ARG2"
    ],
    "vc dimension*****number": [
        "$ARG1 of only O ( w log w ) where w is the $ARG2"
    ],
    "number*****weights": [
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2"
    ],
    "concept*****constraint": [
        "$ARG1 of \u201c person \u201d versus \u201c not a person \u201d does not need to assign a di\ufb00erent class to an input represented as \u201c woman with glasses \u201d than it assigns to an input represented as \u201c man without glasses. \u201d This capacity $ARG2"
    ],
    "representation learning*****other": [
        "$ARG1 values describing each input , but they can not be controlled separately from each $ARG2",
        "$ARG1 makes use of $ARG2",
        "$ARG1 by introducing representations that are expressed in terms of $ARG2",
        "$ARG1 algorithms that develop these criteria in di\ufb00erent ways or introduce $ARG2"
    ],
    "other*****distributed representation": [
        "$ARG1 , so this does not qualify as a true $ARG2"
    ],
    "decision tree*****path": [
        "$ARG1 : only one leaf ( and the nodes on the $ARG2"
    ],
    "algorithm*****multiple": [
        "$ARG1 , each input is represented with $ARG2",
        "$ARG1 to handle $ARG2"
    ],
    "multiple*****other": [
        "$ARG1 values , but those values can not readily be controlled separately from each $ARG2",
        "$ARG1 individual \u201c neurons \u201d that can be processed independently from the $ARG2"
    ],
    "kernel*****gaussian kernel": [
        "$ARG1 machines with a $ARG2",
        "$ARG1 is the $ARG2"
    ],
    "gaussian kernel*****other": [
        "$ARG1 ( or $ARG2"
    ],
    "other*****kernel": [
        "$ARG1 similarly local $ARG2",
        "$ARG1 has edges labeled \u201c c \u201d and \u201c d. \u201d Each time we move one pixel to the right in the output , we move on to using a di\ufb00erent $ARG2"
    ],
    "kernel*****support": [
        "$ARG1 ) : although the degree of activation of each \u201c $ARG2"
    ],
    "vector*****template": [
        "$ARG1 \u201d or $ARG2"
    ],
    "example*****gaussian mixture": [
        "$ARG1 is now continuous-valued , the same issue arises as with $ARG2"
    ],
    "set*****tree structure": [
        "$ARG1 of contexts ( sequences of symbols ) is partitioned according to a $ARG2"
    ],
    "concept*****distributed representation": [
        "$ARG1 that distinguishes a $ARG2",
        "$ARG1 of $ARG2"
    ],
    "distributed representation*****generalization": [
        "$ARG1 from a symbolic one is that $ARG2"
    ],
    "distributed representation*****embedding": [
        "$ARG1 may contain entries such as \u201c has_fur \u201d or \u201c number_of_legs \u201d that have the same value for the $ARG2",
        "$ARG1 ( also called an $ARG2"
    ],
    "embedding*****neural language model": [
        "$ARG1 of both \u201c cat \u201d and \u201c dog. \u201d $ARG2"
    ],
    "neural language model*****distributed representation": [
        "$ARG1 that operate on $ARG2"
    ],
    "distributed representation*****other": [
        "$ARG1 of words generalize much better than $ARG2",
        "$ARG1 of vi ( and probably several $ARG2"
    ],
    "distributed representation*****similarity": [
        "$ARG1 induce a rich $ARG2"
    ],
    "distance*****property": [
        "$ARG1 , a $ARG2"
    ],
    "distributed representation*****learning": [
        "$ARG1 as part of a $ARG2",
        "$ARG1 is ( how can the $ARG2"
    ],
    "representation learning*****illustration": [
        "$ARG1 Figure 15.8 : $ARG2"
    ],
    "algorithm*****example": [
        "$ARG1 provides an $ARG2",
        "$ARG1 6.1 for this $ARG2",
        "$ARG1 behaves , we provide an $ARG2",
        "$ARG1 is capable of using stochasticity to compute di\ufb00erent updates for di\ufb00erent units ( for $ARG2",
        "$ARG1 is , and present an $ARG2",
        "$ARG1 is given a new $ARG2",
        "$ARG1 is given in input a corrupted $ARG2",
        "$ARG1 , but it provides an $ARG2",
        "$ARG1 , for $ARG2"
    ],
    "set*****optimization algorithm": [
        "$ARG1 without solving a di\ufb03cult $ARG2"
    ],
    "function*****number": [
        "$ARG1 with more peaks and troughs than the available $ARG2",
        "$ARG1 ) involve intractable computations that grow exponentially with the $ARG2",
        "$ARG1 of a small $ARG2",
        "$ARG1 after each time step , or equivalently , after each $ARG2",
        "$ARG1 of m , the $ARG2",
        "$ARG1 is linear in the $ARG2",
        "$ARG1 that has more local maxima than the $ARG2",
        "$ARG1 that has many more regions to be distinguished than the $ARG2",
        "$ARG1 is complicated ( we want to distinguish a huge $ARG2",
        "$ARG1 , much as there is no single minimal positive real $ARG2",
        "$ARG1 of a small $ARG2",
        "$ARG1 of the same $ARG2"
    ],
    "contrast*****distributed representation": [
        "$ARG1 this with a $ARG2"
    ],
    "representation learning*****number": [
        "$ARG1 have a statistical advantage when an apparently complicated structure can be compactly represented using a small $ARG2"
    ],
    "learning*****assumption": [
        "$ARG1 algorithms generalize only due to the smoothness $ARG2"
    ],
    "assumption*****function": [
        "$ARG1 , which states that if u \u2248 v , then the target $ARG2",
        "$ARG1 that the target $ARG2"
    ],
    "function*****property": [
        "$ARG1 f to be learned has the $ARG2"
    ],
    "assumption*****example": [
        "$ARG1 , but the end result is that if we have an $ARG2"
    ],
    "example*****estimator": [
        "$ARG1 ( x , y ) for which we know that f ( x ) \u2248 y , then we choose an $ARG2"
    ],
    "assumption*****curse of dimensionality": [
        "$ARG1 is clearly very useful , but it su\ufb00ers from the $ARG2"
    ],
    "curse of dimensionality*****function": [
        "$ARG1 : in order to learn a target $ARG2"
    ],
    "1*****number": [
        "$ARG1 we may need a $ARG2",
        "$ARG1 , and because we wanted the logarithm of the $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 ) then That is , under these assumptions , the $ARG2"
    ],
    "example*****convolutional network": [
        "$ARG1 , a $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 , $ARG2",
        "$ARG1 , when processing images , it is useful to detect edges in the \ufb01rst layer of a $ARG2",
        "$ARG1 of a recurrent $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 , it is common to use the features from a $ARG2"
    ],
    "convolutional network*****image": [
        "$ARG1 with max-pooling can recognize an object regardless of its location in the $ARG2",
        "$ARG1 provides an \u201c $ARG2",
        "$ARG1 that processes a \ufb01xed $ARG2",
        "$ARG1 that processes a variable-sized $ARG2",
        "$ARG1 nicknamed \u201c grandmother cells \u201d \u2014the idea is that a person could have a neuron that activates when seeing an $ARG2"
    ],
    "image*****translation": [
        "$ARG1 , even though spatial $ARG2",
        "$ARG1 , conferring $ARG2"
    ],
    "special case*****distributed representation": [
        "$ARG1 of a $ARG2"
    ],
    "number*****distributed representation": [
        "$ARG1 of intersections of n of the corresponding half-spaces determines how many regions this $ARG2"
    ],
    "number*****feature": [
        "$ARG1 of regions this binary $ARG2",
        "$ARG1 of possible values ( words in the vocabulary ) per input $ARG2",
        "$ARG1 of features x2 through x 100 , not by the lone $ARG2"
    ],
    "function*****dimension": [
        "$ARG1 whose behavior is distinct in exponentially many regions : in a d-dimensional space with at least 2 di\ufb00erent values to distinguish per $ARG2"
    ],
    "representation learning*****generalization": [
        "$ARG1 This provides a geometric argument to explain the $ARG2"
    ],
    "generalization*****distributed representation": [
        "$ARG1 power of $ARG2"
    ],
    "distributed representation*****feature": [
        "$ARG1 could be extended to the case where instead of using linear threshold units we use nonlinear , possibly continuous , $ARG2",
        "$ARG1 , with one $ARG2"
    ],
    "feature*****distributed representation": [
        "$ARG1 extractors for each of the attributes in the $ARG2"
    ],
    "example*****vc dimension": [
        "$ARG1 , the $ARG2"
    ],
    "vc dimension*****neural network": [
        "$ARG1 of a $ARG2"
    ],
    "neural network*****number": [
        "$ARG1 of linear threshold units is only O ( w log w ) , where w is the $ARG2",
        "$ARG1 the $ARG2",
        "$ARG1 \u2019 s $ARG2",
        "$ARG1 can sometimes perform well with only a small $ARG2"
    ],
    "distributed representation*****function": [
        "$ARG1 combined with a linear classi\ufb01er thus expresses a prior belief that the classes to be recognized are linearly separable as a $ARG2"
    ],
    "function*****set": [
        "$ARG1 of the underlying causal factors captured by h. We will typically want to learn categories such as the $ARG2",
        "$ARG1 rather than merely choosing a $ARG2",
        "$ARG1 that will generalize to points not in the training $ARG2",
        "$ARG1 f , where x is a $ARG2",
        "$ARG1 u ( i ) = f ( A ( i ) ) where A ( i ) is the $ARG2",
        "$ARG1 f ( i ) to the $ARG2",
        "$ARG1 f ( x ) , we can generate a training $ARG2",
        "$ARG1 of the training $ARG2",
        "$ARG1 additionally behaves di\ufb00erently in di\ufb00erent regions , it can become extremely complicated to describe with a $ARG2",
        "$ARG1 y\u0302 ( x ) that maps a $ARG2",
        "$ARG1 P must satisfy the following properties : \u2022 The domain of P must be the $ARG2",
        "$ARG1 p must satisfy the \u2022 The domain of p must be the $ARG2"
    ],
    "representation learning*****model": [
        "$ARG1 Figure 15.9 : A generative $ARG2"
    ],
    "model*****distributed representation": [
        "$ARG1 has learned a $ARG2",
        "$ARG1 that uses a $ARG2"
    ],
    "distributed representation*****concept": [
        "$ARG1 that disentangles the $ARG2"
    ],
    "concept*****vector": [
        "$ARG1 of a man with glasses , then subtract the $ARG2",
        "$ARG1 of a man without glasses , and \ufb01nally add the $ARG2",
        "$ARG1 of a woman without glasses , we obtain the $ARG2"
    ],
    "vector*****concept": [
        "$ARG1 representing the $ARG2",
        "$ARG1 representing the $ARG2",
        "$ARG1 representing the $ARG2"
    ],
    "gradient descent*****objective function": [
        "$ARG1 on an $ARG2",
        "$ARG1 parameter update could throw the parameters very far , into a region where the $ARG2"
    ],
    "objective function*****task": [
        "$ARG1 of interest naturally learns semantically interesting features , so long as the $ARG2"
    ],
    "representation learning*****multilayer perceptron": [
        "$ARG1 Exponential Gains from Depth We have seen in section 6.4.1 that $ARG2"
    ],
    "multilayer perceptron*****universal approximator": [
        "$ARG1 are $ARG2"
    ],
    "task*****neural network": [
        "$ARG1 was based on a deep $ARG2",
        "$ARG1 because $ARG2"
    ],
    "hidden layer*****universal approximator": [
        "$ARG1 can be shown to be $ARG2"
    ],
    "model*****universal approximator": [
        "$ARG1 family that is a $ARG2"
    ],
    "number*****1": [
        "$ARG1 of hidden units ( with respect to the input size ) with insu\ufb03cient depth ( depth 2 or depth k \u2212 $ARG2",
        "$ARG1 y\u0302 = P ( y = $ARG2",
        "$ARG1 needed to lie between 0 and $ARG2",
        "$ARG1 z = log P\u0303 ( y = $ARG2",
        "$ARG1 of possible binary functions on vectors v \u2208 { 0 , $ARG2",
        "$ARG1 examples ) CHAPTER $ARG2",
        "$ARG1 of neurons ( logarithmic scale ) CHAPTER $ARG2",
        "$ARG1 of parameters in the RNN is O ( $ARG2",
        "$ARG1 of examples , ranging from $ARG2"
    ],
    "structured probabilistic model*****hidden layer": [
        "$ARG1 with a single $ARG2"
    ],
    "hidden layer*****restricted boltzmann machine": [
        "$ARG1 of latent variables , including $ARG2",
        "$ARG1 , such as $ARG2"
    ],
    "deep belief network*****universal approximator": [
        "$ARG1 , are $ARG2"
    ],
    "representation learning*****deep feedforward network": [
        "$ARG1 In section 6.4.1 , we saw that a su\ufb03ciently $ARG2"
    ],
    "model*****sum-product network": [
        "$ARG1 is the $ARG2"
    ],
    "probability distribution*****set": [
        "$ARG1 over a $ARG2",
        "$ARG1 p ( x , y ) and we sample from it repeatedly to generate the train $ARG2",
        "$ARG1 that a given $ARG2",
        "$ARG1 over one $ARG2",
        "$ARG1 over a $ARG2"
    ],
    "set*****function": [
        "$ARG1 of theoretical results for the expressive power of families of deep circuits related to convolutional nets , highlighting an exponential advantage for the deep circuit even when the shallow circuit is allowed to only approximate the $ARG2",
        "$ARG1 of speci\ufb01c examples and choosing a $ARG2",
        "$ARG1 of prior beliefs we have about what kind of $ARG2",
        "$ARG1 of variables that are inputs to the $ARG2"
    ],
    "learning*****factors of variation": [
        "$ARG1 to \ufb01nd these underlying $ARG2",
        "$ARG1 features , our goal is usually to separate the $ARG2"
    ],
    "supervised learning*****factors of variation": [
        "$ARG1 provides a very strong clue : a label y , presented with each x , that usually speci\ufb01es the value of at least one of the $ARG2"
    ],
    "no free lunch theorem*****regularization": [
        "$ARG1 show that $ARG2"
    ],
    "regularization*****generalization": [
        "$ARG1 strategies are necessary to obtain good $ARG2"
    ],
    "regularization*****deep learning": [
        "$ARG1 strategy , one goal of $ARG2",
        "$ARG1 for $ARG2",
        "$ARG1 available to the $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 has been used for decades prior to the advent of $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 in $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2"
    ],
    "deep learning*****set": [
        "$ARG1 is to \ufb01nd a $ARG2",
        "$ARG1 , we usually have a $ARG2",
        "$ARG1 on the test $ARG2"
    ],
    "set*****regularization": [
        "$ARG1 of fairly generic $ARG2",
        "$ARG1 as well as additional $ARG2"
    ],
    "representation learning*****list": [
        "$ARG1 We provide here a $ARG2"
    ],
    "list*****regularization": [
        "$ARG1 of these generic $ARG2"
    ],
    "list*****learning": [
        "$ARG1 is clearly not exhaustive , but gives some concrete examples of ways that $ARG2"
    ],
    "machine learning*****curse of dimensionality": [
        "$ARG1 algorithms leverage this idea , but it is insu\ufb03cient to overcome the $ARG2",
        "$ARG1 algorithms aim to overcome the $ARG2"
    ],
    "machine learning*****assumption": [
        "$ARG1 algorithms that do not make the smoothness $ARG2"
    ],
    "representation learning*****assumption": [
        "$ARG1 algorithms are motivated by the $ARG2"
    ],
    "assumption*****multiple": [
        "$ARG1 that the data is generated by $ARG2"
    ],
    "learning*****representation learning": [
        "$ARG1 via $ARG2"
    ],
    "view*****distributed representation": [
        "$ARG1 motivates the use of $ARG2"
    ],
    "distributed representation*****model": [
        "$ARG1 , with separate directions in representation space corresponding to separate factors \u2022 Causal factors : the $ARG2",
        "$ARG1 the $ARG2"
    ],
    "semi-supervised learning*****model": [
        "$ARG1 and makes the learned $ARG2"
    ],
    "representation learning*****task": [
        "$ARG1 expresses our belief that the $ARG2"
    ],
    "task*****subset": [
        "$ARG1 is associated with a $ARG2",
        "$ARG1 involving only a $ARG2",
        "$ARG1 is associated with a $ARG2"
    ],
    "subset*****function": [
        "$ARG1 or a $ARG2",
        "$ARG1 of points , with interesting variations in the output of the learned $ARG2"
    ],
    "function*****assumption": [
        "$ARG1 f ( i ) ( x ) of a global input x , the $ARG2"
    ],
    "assumption*****subset": [
        "$ARG1 is that each y i is associated with a di\ufb00erent $ARG2"
    ],
    "subset*****learning": [
        "$ARG1 from a common pool of relevant factors h. Because these subsets overlap , $ARG2"
    ],
    "manifolds*****probability": [
        "$ARG1 : $ARG2",
        "$ARG1 near which $ARG2"
    ],
    "machine learning*****manifold": [
        "$ARG1 algorithms behave sensibly only on this $ARG2",
        "$ARG1 algorithms , especially autoencoders , attempt to explicitly learn the structure of the $ARG2",
        "$ARG1 algorithms assume that each connected $ARG2",
        "$ARG1 is that of a A $ARG2",
        "$ARG1 , we allow the dimensionality of the $ARG2",
        "$ARG1 algorithms to represent the data in terms of coordinates on the $ARG2",
        "$ARG1 algorithms , autoencoders exploit the idea that data concentrates around a low-dimensional $ARG2"
    ],
    "manifolds*****constant": [
        "$ARG1 , but the class remains $ARG2"
    ],
    "assumption*****learning": [
        "$ARG1 motivates a variety of $ARG2",
        "$ARG1 and the associated non-parametric $ARG2"
    ],
    "learning*****tangent prop": [
        "$ARG1 algorithms , including $ARG2"
    ],
    "tangent prop*****double backprop": [
        "$ARG1 , $ARG2",
        "$ARG1 is also related to $ARG2"
    ],
    "double backprop*****manifold tangent classi\ufb01er": [
        "$ARG1 , the $ARG2"
    ],
    "manifold tangent classi\ufb01er*****adversarial training": [
        "$ARG1 and $ARG2"
    ],
    "coherence*****slow feature analysis": [
        "$ARG1 : $ARG2"
    ],
    "slow feature analysis*****assumption": [
        "$ARG1 and related algorithms make the $ARG2"
    ],
    "assumption*****change": [
        "$ARG1 that the most important explanatory factors $ARG2",
        "$ARG1 that a small $ARG2"
    ],
    "representation learning*****independence": [
        "$ARG1 possible is marginal $ARG2"
    ],
    "concept*****representation learning": [
        "$ARG1 of $ARG2"
    ],
    "representation learning*****deep learning": [
        "$ARG1 ties together all of the many forms of $ARG2",
        "$ARG1 and $ARG2"
    ],
    "deep feedforward network*****feedforward neural network": [
        "$ARG1 , also often called $ARG2"
    ],
    "feedforward neural network*****multilayer perceptron": [
        "$ARG1 , or $ARG2"
    ],
    "multilayer perceptron*****deep learning": [
        "$ARG1 ( MLPs ) , are the quintessential $ARG2"
    ],
    "mapping*****function": [
        "$ARG1 y = f ( x ; \u03b8 ) and learns the value of the parameters \u03b8 that result in the best $ARG2",
        "$ARG1 from parameters to predictions is still a linear $ARG2"
    ],
    "information*****function": [
        "$ARG1 \ufb02ows through the $ARG2",
        "$ARG1 through a network is very general , and can be used to compute values such as the Jacobian of a $ARG2",
        "$ARG1 provides us no guide , when the $ARG2",
        "$ARG1 THEORY mass $ARG2"
    ],
    "feedforward neural network*****recurrent neural network": [
        "$ARG1 are extended to include feedback connections , they are called $ARG2"
    ],
    "convolutional network*****object recognition": [
        "$ARG1 used for $ARG2",
        "$ARG1 for $ARG2",
        "$ARG1 can predict IT \ufb01ring rates , and also perform very similarly to ( time limited ) humans on $ARG2"
    ],
    "path*****recurrent network": [
        "$ARG1 to $ARG2"
    ],
    "recurrent network*****feedforward neural network": [
        "$ARG1 , which power many natural $ARG2"
    ],
    "model*****graph": [
        "$ARG1 is associated with a directed acyclic $ARG2",
        "$ARG1 is de\ufb01ned by a $ARG2",
        "$ARG1 , the complete $ARG2",
        "$ARG1 with $ARG2",
        "$ARG1 , we must triangulate the $ARG2",
        "$ARG1 is intended to capture dependencies between visible variables with direct connections , it is usually infeasible to connect all variables , so the $ARG2"
    ],
    "deep feedforward network*****model": [
        "$ARG1 length of the chain gives the depth of the $ARG2"
    ],
    "thinking*****function": [
        "$ARG1 of the layer as representing a single vector-to-vector $ARG2"
    ],
    "function*****generalization": [
        "$ARG1 approximation machines that are designed to achieve statistical $ARG2",
        "$ARG1 and can reduce the amount of $ARG2",
        "$ARG1 , but seek only to reduce its value su\ufb03ciently to obtain good $ARG2"
    ],
    "generalization*****function": [
        "$ARG1 , occasionally drawing some insights from what we know about the brain , rather than as models of brain $ARG2",
        "$ARG1 error has a U-shaped curve as a $ARG2",
        "$ARG1 error as a $ARG2",
        "$ARG1 error typically follows a U-shaped curve when plotted as a $ARG2"
    ],
    "logistic regression*****linear regression": [
        "$ARG1 and $ARG2"
    ],
    "linear regression*****convex optimization": [
        "$ARG1 , are appealing because they may be \ufb01t e\ufb03ciently and reliably , either in closed form or with $ARG2",
        "$ARG1 models or the $ARG2"
    ],
    "linear model*****model": [
        "$ARG1 also have the obvious defect that the $ARG2"
    ],
    "model*****interaction": [
        "$ARG1 can not understand the $ARG2",
        "$ARG1 a direct $ARG2",
        "$ARG1 is divided into two groups of units : v and h , and the $ARG2"
    ],
    "kernel trick*****learning": [
        "$ARG1 described in section 5.7.2 , to obtain a nonlinear $ARG2"
    ],
    "algorithm*****mapping": [
        "$ARG1 based on implicitly applying the \u03c6 $ARG2",
        "$ARG1 6.1 A procedure that performs the computations $ARG2"
    ],
    "option*****kernel": [
        "$ARG1 is to use a very generic \u03c6 , such as the in\ufb01nite-dimensional \u03c6 that is implicitly used by $ARG2"
    ],
    "dimension*****set": [
        "$ARG1 , we can always have enough capacity to \ufb01t the training $ARG2"
    ],
    "set*****generalization": [
        "$ARG1 , but $ARG2",
        "$ARG1 , can be used to estimate the $ARG2",
        "$ARG1 , used to estimate the $ARG2",
        "$ARG1 error will underestimate the $ARG2",
        "$ARG1 size and $ARG2"
    ],
    "generalization*****set": [
        "$ARG1 to the test $ARG2",
        "$ARG1 to sequence lengths that did not appear in the training $ARG2",
        "$ARG1 of a classi\ufb01er by increasing the size of the training $ARG2",
        "$ARG1 error vary as the size of the training $ARG2",
        "$ARG1 error may be estimated using the test $ARG2"
    ],
    "principle*****information": [
        "$ARG1 of local smoothness and do not encode enough prior $ARG2"
    ],
    "task*****speech recognition": [
        "$ARG1 , with practitioners specializing in di\ufb00erent domains such as $ARG2",
        "$ARG1 of $ARG2"
    ],
    "speech recognition*****computer vision": [
        "$ARG1 or $ARG2"
    ],
    "example*****deep feedforward network": [
        "$ARG1 of a $ARG2"
    ],
    "deep feedforward network*****hidden layer": [
        "$ARG1 , with \u03c6 de\ufb01ning a $ARG2"
    ],
    "human*****generalization": [
        "$ARG1 practitioners can encode their knowledge to help $ARG2"
    ],
    "human*****function": [
        "$ARG1 designer only needs to \ufb01nd the right general $ARG2"
    ],
    "principle*****learning": [
        "$ARG1 of improving models by $ARG2",
        "$ARG1 to $ARG2",
        "$ARG1 of $ARG2"
    ],
    "stochastic*****learning": [
        "$ARG1 mappings , $ARG2"
    ],
    "linear model*****cost function": [
        "$ARG1 : choosing the optimizer , the $ARG2",
        "$ARG1 has a quadratic $ARG2",
        "$ARG1 and a least squares $ARG2"
    ],
    "concept*****hidden layer": [
        "$ARG1 of a $ARG2"
    ],
    "hidden layer*****activation function": [
        "$ARG1 , and this requires us to choose the $ARG2",
        "$ARG1 with any \u201c squashing \u201d $ARG2"
    ],
    "activation function*****hidden layer": [
        "$ARG1 that will be used to compute the $ARG2",
        "$ARG1 at each $ARG2"
    ],
    "learning*****example": [
        "$ARG1 XOR To make the idea of a feedforward network more concrete , we begin with an $ARG2",
        "$ARG1 a representation h of a training $ARG2"
    ],
    "task*****learning": [
        "$ARG1 : $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 , the $ARG2"
    ],
    "function*****operation": [
        "$ARG1 ( \u201c exclusive or \u201d ) is an $ARG2",
        "$ARG1 f : R \u2192 R as the $ARG2",
        "$ARG1 that the $ARG2",
        "$ARG1 s providing a smoothed estimate of the position of the spaceship : s ( t ) = x ( a ) w ( t \u2212 a ) da This $ARG2"
    ],
    "1*****function": [
        "$ARG1 , the XOR $ARG2",
        "$ARG1 ) were linear , then the feedforward network as a whole would remain a linear $ARG2",
        "$ARG1 states , we can build a universal $ARG2",
        "$ARG1 } n is 22 and selecting one such $ARG2",
        "$ARG1 ) ) =f ( h ( t\u22121 ) , x ( t ) ; \u03b8 ) The $ARG2",
        "$ARG1 ) ) as input and produces the current state , but the unfolded recurrent structure allows us to factorize g ( t ) into repeated application of a $ARG2",
        "$ARG1 ) as a $ARG2",
        "$ARG1 ) as a $ARG2"
    ],
    "function*****learning": [
        "$ARG1 y = f ( x ; \u03b8 ) and our $ARG2",
        "$ARG1 , $ARG2",
        "$ARG1 describes a $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 E , the $ARG2",
        "$ARG1 , the eigenvalues of the Hessian thus determine the scale of the $ARG2"
    ],
    "example*****generalization": [
        "$ARG1 , we will not be concerned with statistical $ARG2",
        "$ARG1 in D , whose mean is the estimated $ARG2"
    ],
    "challenge*****set": [
        "$ARG1 is to \ufb01t the training $ARG2",
        "$ARG1 is not due to the di\ufb03culty of describing the $ARG2"
    ],
    "problem*****mean squared error": [
        "$ARG1 and use a $ARG2"
    ],
    "loss function*****example": [
        "$ARG1 to simplify the math for this $ARG2"
    ],
    "deep feedforward network*****cost function": [
        "$ARG1 appropriate $ARG2",
        "$ARG1 Di\ufb00erent $ARG2",
        "$ARG1 a good $ARG2"
    ],
    "set*****mse": [
        "$ARG1 , the $ARG2"
    ],
    "linear model*****function": [
        "$ARG1 is not able to represent the XOR $ARG2",
        "$ARG1 applied directly to the original input can not implement the XOR $ARG2",
        "$ARG1 can now describe the $ARG2",
        "$ARG1 can not implement such a $ARG2",
        "$ARG1 family , such as its inability to learn the XOR $ARG2"
    ],
    "problem*****model": [
        "$ARG1 is to use a $ARG2",
        "$ARG1 so that the $ARG2",
        "$ARG1 for a language $ARG2",
        "$ARG1 by drawing samples of both h and v from the $ARG2",
        "$ARG1 with saving the $ARG2"
    ],
    "feature*****linear model": [
        "$ARG1 space in which a $ARG2"
    ],
    "linear model*****solution": [
        "$ARG1 is able to represent the $ARG2"
    ],
    "illustration*****model": [
        "$ARG1 of this $ARG2"
    ],
    "vector*****function": [
        "$ARG1 of hidden units h that are computed by a $ARG2",
        "$ARG1 \ufb01eld has zeros at both maxima of the estimated density $ARG2"
    ],
    "output layer*****linear regression": [
        "$ARG1 is still just a $ARG2"
    ],
    "1*****linear model": [
        "$ARG1 ) compute ? $ARG2"
    ],
    "linear model*****1": [
        "$ARG1 have served us well so far , and it may be tempting to make f ( $ARG2",
        "$ARG1 therefore can not use the value of x $ARG2"
    ],
    "neural network*****a\ufb03ne": [
        "$ARG1 do so using an $ARG2",
        "$ARG1 layers consisting of an $ARG2"
    ],
    "a\ufb03ne*****function": [
        "$ARG1 transformation controlled by learned parameters , followed by a \ufb01xed , nonlinear $ARG2",
        "$ARG1 transformation z = W \ue03e x + b , and then applying an element-wise nonlinear $ARG2"
    ],
    "function*****activation function": [
        "$ARG1 called an $ARG2",
        "$ARG1 of the input x as a traditional layer using the recti\ufb01ed linear $ARG2",
        "$ARG1 as the $ARG2"
    ],
    "deep feedforward network*****problem": [
        "$ARG1 Figure 6.1 : Solving the XOR $ARG2"
    ],
    "1*****change": [
        "$ARG1 to $ARG2"
    ],
    "change*****problem": [
        "$ARG1 the coe\ufb03cient on x2 and can not solve this $ARG2"
    ],
    "neural network*****linear model": [
        "$ARG1 , a $ARG2",
        "$ARG1 y\u0302 = w\ue03e tanh ( U \ue03e tanh ( V \ue03ex ) ) resembles training a $ARG2"
    ],
    "linear model*****problem": [
        "$ARG1 can now solve the $ARG2",
        "$ARG1 can solve the $ARG2"
    ],
    "1*****feature": [
        "$ARG1 have been collapsed into a single point in $ARG2",
        "$ARG1 ] \ue03e to a single point in $ARG2"
    ],
    "feature*****1": [
        "$ARG1 space , h = [ $ARG2"
    ],
    "deep feedforward network*****example": [
        "$ARG1 Figure 6.2 : An $ARG2",
        "$ARG1 Figure 6.10 : An $ARG2"
    ],
    "graph*****vector": [
        "$ARG1 for each entire $ARG2"
    ],
    "matrix*****mapping": [
        "$ARG1 W describes the $ARG2"
    ],
    "mapping*****vector": [
        "$ARG1 from x to h , and a $ARG2",
        "$ARG1 an input $ARG2",
        "$ARG1 from a $ARG2",
        "$ARG1 returns a $ARG2"
    ],
    "vector*****mapping": [
        "$ARG1 w describes the $ARG2",
        "$ARG1 to another , given enough examples of the $ARG2"
    ],
    "vector*****weights": [
        "$ARG1 of $ARG2",
        "$ARG1 , containing the outputs of all the LSTM cells , and bf , Uf , W f are respectively biases , input $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 w to indicate all of the $ARG2"
    ],
    "weights*****scalar": [
        "$ARG1 and a $ARG2"
    ],
    "bias parameter*****a\ufb03ne": [
        "$ARG1 to describe an $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "a\ufb03ne*****vector": [
        "$ARG1 transformation from an input $ARG2",
        "$ARG1 transformation from a $ARG2"
    ],
    "vector*****scalar": [
        "$ARG1 to an output $ARG2",
        "$ARG1 value , rather than a $ARG2",
        "$ARG1 x \u2208 R n as input and predict the value of a $ARG2",
        "$ARG1 x \u2208 R n to predict the value of a $ARG2",
        "$ARG1 v ( i ) by a corresponding $ARG2",
        "$ARG1 d. It is more conventional to write $ARG2"
    ],
    "vector*****bias parameter": [
        "$ARG1 of $ARG2"
    ],
    "activation function*****function": [
        "$ARG1 g is typically chosen to be a $ARG2",
        "$ARG1 such as the recti\ufb01ed linear $ARG2"
    ],
    "neural network*****recti\ufb01ed linear unit": [
        "$ARG1 , the default recommendation is to use the $ARG2",
        "$ARG1 use $ARG2"
    ],
    "recti\ufb01ed linear unit*****activation function": [
        "$ARG1 or ReLU ( Jarrett et al. , 2009 ; Nair and Hinton , 2010 ; Glorot et al. , 2011a ) de\ufb01ned by the $ARG2",
        "$ARG1 use the $ARG2",
        "$ARG1 were avoided due to a somewhat superstitious belief that $ARG2"
    ],
    "deep feedforward network*****activation function": [
        "$ARG1 Figure 6.3 : The recti\ufb01ed linear $ARG2"
    ],
    "activation function*****feedforward neural network": [
        "$ARG1 recommended for use with most $ARG2"
    ],
    "recti\ufb01ed linear unit*****linear model": [
        "$ARG1 are nearly linear , they preserve many of the properties that make $ARG2"
    ],
    "memory*****1": [
        "$ARG1 needs only to be able to store 0 or $ARG2"
    ],
    "design matrix*****example": [
        "$ARG1 containing all four points in the binary input space , with one $ARG2",
        "$ARG1 , with the activations for each $ARG2",
        "$ARG1 , it must be possible to describe each $ARG2",
        "$ARG1 of m $ARG2",
        "$ARG1 of examples , rather than as a sum over separate $ARG2"
    ],
    "example*****row": [
        "$ARG1 per $ARG2",
        "$ARG1 appearing in a $ARG2",
        "$ARG1 with di\ufb00erent kernels or multiply each $ARG2",
        "$ARG1 in each $ARG2"
    ],
    "row*****1": [
        "$ARG1 : \uf8ef 0 $ARG2"
    ],
    "neural network*****matrix": [
        "$ARG1 is to multiply the input $ARG2",
        "$ARG1 layer described by a linear transformation via a $ARG2"
    ],
    "deep feedforward network*****neural network": [
        "$ARG1 The $ARG2",
        "$ARG1 Figure 6.4 : Samples drawn from a $ARG2",
        "$ARG1 So far we have discussed $ARG2"
    ],
    "neural network*****example": [
        "$ARG1 has obtained the correct answer for every $ARG2",
        "$ARG1 , with an $ARG2"
    ],
    "example*****solution": [
        "$ARG1 , we simply speci\ufb01ed the $ARG2"
    ],
    "model*****solution": [
        "$ARG1 parameters and billions of training examples , so one can not simply guess the $ARG2",
        "$ARG1 families , the M step can even be performed analytically , jumping all the way to the optimal $ARG2"
    ],
    "problem*****loss function": [
        "$ARG1 is at a global minimum of the $ARG2",
        "$ARG1 by choosing a surrogate $ARG2"
    ],
    "loss function*****gradient descent": [
        "$ARG1 , so $ARG2"
    ],
    "other*****problem": [
        "$ARG1 equivalent solutions to the XOR $ARG2"
    ],
    "problem*****gradient descent": [
        "$ARG1 that $ARG2",
        "$ARG1 classes , such as $ARG2"
    ],
    "convergence*****gradient descent": [
        "$ARG1 point of $ARG2"
    ],
    "gradient descent*****neural network": [
        "$ARG1 would usually not \ufb01nd clean , easily understood , integer-valued solutions like the one we presented Designing and training a $ARG2",
        "$ARG1 for $ARG2"
    ],
    "neural network*****other": [
        "$ARG1 is not much di\ufb00erent from training any $ARG2",
        "$ARG1 is not much di\ufb00erent from training any $ARG2",
        "$ARG1 are more or less the same as those for $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 outperformed competing AI systems based on $ARG2",
        "$ARG1 training , some of the techniques used to combat it in $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 that otherwise seem di\ufb03cult to train , but $ARG2",
        "$ARG1 ( or $ARG2",
        "$ARG1 in $ARG2"
    ],
    "algorithm*****optimization": [
        "$ARG1 by specifying an $ARG2"
    ],
    "optimization*****cost function": [
        "$ARG1 procedure , a $ARG2",
        "$ARG1 FOR TRAINING DEEP MODELS J ( \u03b8 ) Figure 8.2 : A visualization of the $ARG2",
        "$ARG1 to \ufb01nd a good $ARG2",
        "$ARG1 arise from the global structure of the $ARG2",
        "$ARG1 of an abstract , general , quadratic $ARG2"
    ],
    "linear model*****neural network": [
        "$ARG1 we have seen so far and $ARG2",
        "$ARG1 is also directly applicable to deep $ARG2"
    ],
    "neural network*****loss function": [
        "$ARG1 causes most interesting $ARG2",
        "$ARG1 because their $ARG2",
        "$ARG1 also have $ARG2"
    ],
    "neural network*****cost function": [
        "$ARG1 are usually trained by using iterative , gradient-based optimizers that merely drive the $ARG2",
        "$ARG1 is the choice of the $ARG2",
        "$ARG1 will often combine one of the primary $ARG2",
        "$ARG1 training algorithms do not usually arrive at a local minimum of the $ARG2",
        "$ARG1 and the computation of the $ARG2",
        "$ARG1 that signi\ufb01cantly reduce a $ARG2",
        "$ARG1 , most local minima have a low $ARG2"
    ],
    "cost function*****linear regression": [
        "$ARG1 to a very low value , rather than the linear equation solvers used to train $ARG2"
    ],
    "convex optimization*****convergence": [
        "$ARG1 algorithms with global $ARG2",
        "$ARG1 algorithms can be useful for proving the $ARG2"
    ],
    "convergence*****logistic regression": [
        "$ARG1 guarantees used to train $ARG2"
    ],
    "stochastic gradient descent*****loss function": [
        "$ARG1 applied to non-convex $ARG2"
    ],
    "loss function*****convergence": [
        "$ARG1 has no such $ARG2",
        "$ARG1 but halts when a $ARG2"
    ],
    "feedforward neural network*****weights": [
        "$ARG1 , it is important to initialize all $ARG2"
    ],
    "optimization algorithm*****almost all": [
        "$ARG1 used to train feedforward networks and $ARG2"
    ],
    "other*****8": [
        "$ARG1 deep models will be described in detail in chapter $ARG2"
    ],
    "8*****parameter initialization": [
        "$ARG1 , with $ARG2"
    ],
    "deep feedforward network*****stochastic gradient descent": [
        "$ARG1 more speci\ufb01cally , are most often improvements of the $ARG2"
    ],
    "linear regression*****support vector machine": [
        "$ARG1 and $ARG2"
    ],
    "support vector machine*****gradient descent": [
        "$ARG1 with $ARG2"
    ],
    "gradient descent*****set": [
        "$ARG1 too , and in fact this is common when the training $ARG2",
        "$ARG1 \u201d implies the use of the full training $ARG2"
    ],
    "view*****neural network": [
        "$ARG1 , training a $ARG2"
    ],
    "gradient*****neural network": [
        "$ARG1 is slightly more complicated for a $ARG2",
        "$ARG1 increasing signi\ufb01cantly during the successful training of a $ARG2"
    ],
    "algorithm*****back-propagation": [
        "$ARG1 and modern generalizations of the $ARG2",
        "$ARG1 6.2 Simpli\ufb01ed version of the $ARG2",
        "$ARG1 6.5 The outermost skeleton of the $ARG2",
        "$ARG1 , called by the $ARG2",
        "$ARG1 computes \u2207HJ = GW ( 2 ) \ue03e using the $ARG2",
        "$ARG1 is to use the $ARG2"
    ],
    "machine learning*****learning": [
        "$ARG1 models , to apply gradient-based $ARG2",
        "$ARG1 that achieves great power and \ufb02exibility by $ARG2",
        "$ARG1 algorithms can be broadly categorized as unsupervised or supervised by what kind of experience they are allowed to have during the Most of the $ARG2",
        "$ARG1 BASICS parametric $ARG2",
        "$ARG1 research is not to seek a universal $ARG2",
        "$ARG1 algorithms have several settings that we can use to control the behavior of the $ARG2",
        "$ARG1 BASICS Another type of $ARG2",
        "$ARG1 research on $ARG2",
        "$ARG1 , the $ARG2"
    ],
    "cost function*****neural network": [
        "$ARG1 for $ARG2",
        "$ARG1 used to train a $ARG2",
        "$ARG1 of a $ARG2"
    ],
    "parametric model*****linear model": [
        "$ARG1 , such as $ARG2"
    ],
    "parametric model*****distribution": [
        "$ARG1 de\ufb01nes a $ARG2"
    ],
    "distribution*****principle": [
        "$ARG1 p ( y | x ; \u03b8 ) and we simply use the $ARG2",
        "$ARG1 p ( y | x ; \u03b8 ) , the $ARG2"
    ],
    "principle*****maximum likelihood": [
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 is the $ARG2",
        "$ARG1 of $ARG2"
    ],
    "cross-entropy*****model": [
        "$ARG1 between the training data and the $ARG2",
        "$ARG1 between the training data and the $ARG2"
    ],
    "probability distribution*****statistic": [
        "$ARG1 over y , we merely predict some $ARG2",
        "$ARG1 p ( y | x ; \u03b8 ) we often want to learn just one conditional $ARG2"
    ],
    "cost function*****regularization": [
        "$ARG1 described here with a $ARG2",
        "$ARG1 may also include additional terms , such as $ARG2",
        "$ARG1 that is identical to reconstruction error combined with a $ARG2"
    ],
    "regularization*****linear model": [
        "$ARG1 applied to $ARG2"
    ],
    "weight decay*****linear model": [
        "$ARG1 approach used for $ARG2"
    ],
    "neural network*****regularization": [
        "$ARG1 and is among the most popular $ARG2"
    ],
    "regularization*****neural network": [
        "$ARG1 strategies for $ARG2",
        "$ARG1 approaches are based on limiting the capacity of models , such as $ARG2",
        "$ARG1 behavior of di\ufb00erent norms , we note that for $ARG2"
    ],
    "maximum likelihood*****neural network": [
        "$ARG1 Most modern $ARG2"
    ],
    "neural network*****maximum likelihood": [
        "$ARG1 are trained using $ARG2"
    ],
    "deep feedforward network*****cross-entropy": [
        "$ARG1 as the $ARG2"
    ],
    "constant*****variance": [
        "$ARG1 is based on the $ARG2"
    ],
    "variance*****gaussian distribution": [
        "$ARG1 of the $ARG2",
        "$ARG1 of a $ARG2",
        "$ARG1 parameter \u03c32 of a $ARG2"
    ],
    "maximum likelihood estimation*****distribution": [
        "$ARG1 with an output $ARG2"
    ],
    "mean squared error*****linear model": [
        "$ARG1 holds for a $ARG2"
    ],
    "cost function*****maximum likelihood": [
        "$ARG1 from $ARG2",
        "$ARG1 used with $ARG2"
    ],
    "maximum likelihood*****cost function": [
        "$ARG1 is that it removes the burden of designing $ARG2",
        "$ARG1 is \u2212 log P ( y | x ) , the log in the $ARG2",
        "$ARG1 suggests we use \u2212 log p ( y | x ; \u03b8 ) as our $ARG2"
    ],
    "neural network*****gradient": [
        "$ARG1 design is that the $ARG2",
        "$ARG1 often do not arrive at a region of small $ARG2",
        "$ARG1 usually involve large and numerous bu\ufb00ers of parameters , activation values , and $ARG2"
    ],
    "function*****cost function": [
        "$ARG1 in the negative log-likelihood $ARG2",
        "$ARG1 to the next arriving at the global minimum , but it might require so many incremental $ARG2",
        "$ARG1 f ( x ; \u03b8 ) by adding a penalty called a regularizer to the $ARG2",
        "$ARG1 curves downward , so the $ARG2",
        "$ARG1 curves upward , so the $ARG2"
    ],
    "interaction*****cost function": [
        "$ARG1 between the $ARG2"
    ],
    "cost function*****property": [
        "$ARG1 and the choice of output unit in One unusual $ARG2"
    ],
    "property*****cross-entropy": [
        "$ARG1 of the $ARG2"
    ],
    "cross-entropy*****maximum likelihood estimation": [
        "$ARG1 cost used to perform $ARG2",
        "$ARG1 performs $ARG2"
    ],
    "logistic regression*****example": [
        "$ARG1 is an $ARG2"
    ],
    "deep feedforward network*****distribution": [
        "$ARG1 can control the density of the output $ARG2"
    ],
    "learning*****variance": [
        "$ARG1 the $ARG2"
    ],
    "variance*****distribution": [
        "$ARG1 parameter of a Gaussian output $ARG2",
        "$ARG1 as one of the properties of the $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "distribution*****set": [
        "$ARG1 ) then it becomes possible to assign extremely high density to the correct training $ARG2",
        "$ARG1 as the training $ARG2"
    ],
    "set*****cross-entropy": [
        "$ARG1 outputs , resulting in $ARG2"
    ],
    "regularization*****learning": [
        "$ARG1 techniques described in chapter 7 provide several di\ufb00erent ways of modifying the $ARG2",
        "$ARG1 is any modi\ufb01cation we make to a $ARG2",
        "$ARG1 relative to supervised training , or it may simply allow us to train much larger architectures due to the reduced computational cost of the $ARG2",
        "$ARG1 as \u201c any modi\ufb01cation we make to a $ARG2",
        "$ARG1 causes the $ARG2",
        "$ARG1 strategy , but we have supported this claim only by showing $ARG2"
    ],
    "example*****neural network": [
        "$ARG1 , we may have a predictor f ( x ; \u03b8 ) that we wish to predict the mean If we use a su\ufb03ciently powerful $ARG2",
        "$ARG1 , we may wish for our $ARG2",
        "$ARG1 ( x , y ) , with y\u0302 the output of the $ARG2",
        "$ARG1 , we could take a $ARG2",
        "$ARG1 , suppose we have a deep $ARG2",
        "$ARG1 , in 2011 , the best CPUs available could run $ARG2",
        "$ARG1 , in the 1990s , the $ARG2"
    ],
    "view*****cost function": [
        "$ARG1 the $ARG2"
    ],
    "cost function*****function": [
        "$ARG1 as being a functional rather than just a $ARG2",
        "$ARG1 to have its minimum occur at some speci\ufb01c $ARG2",
        "$ARG1 to have its minimum lie on the $ARG2",
        "$ARG1 gives a $ARG2",
        "$ARG1 J ( H , W ) = |Hi , j | + This $ARG2",
        "$ARG1 where the \ufb01rst is convex and the optimum tracks from one $ARG2",
        "$ARG1 may be a $ARG2",
        "$ARG1 , we can obtain an autoencoder that learns something useful by changing the reconstruction error term of the cost Traditionally , autoencoders minimize some $ARG2"
    ],
    "function*****expected value": [
        "$ARG1 that maps x to the $ARG2"
    ],
    "optimization problem*****function": [
        "$ARG1 with respect to a $ARG2",
        "$ARG1 f \u2217 = arg min Ex , y\u223cpdata ||y \u2212 f ( x ) ||2 f \u2217 ( x ) = Ey\u223cpdata ( y |x ) [ y ] , so long as this $ARG2"
    ],
    "function*****calculus of variations": [
        "$ARG1 requires a mathematical tool called $ARG2"
    ],
    "calculus of variations*****content": [
        "$ARG1 to understand the $ARG2"
    ],
    "calculus of variations*****optimization problem": [
        "$ARG1 is that solving the $ARG2"
    ],
    "other*****data generating distribution": [
        "$ARG1 words , if we could train on in\ufb01nitely many samples from the true $ARG2"
    ],
    "data generating distribution*****mean squared error": [
        "$ARG1 , minimizing the $ARG2"
    ],
    "cost function*****statistics": [
        "$ARG1 give di\ufb00erent $ARG2",
        "$ARG1 to encourage units to have normalized activation $ARG2"
    ],
    "calculus of variations*****function": [
        "$ARG1 is that f \u2217 = arg min Ex , y\u223cpdata ||y \u2212 f ( x ) ||1 yields a $ARG2"
    ],
    "mean squared error*****optimization": [
        "$ARG1 and mean absolute error often lead to poor results when used with gradient-based $ARG2"
    ],
    "cost function*****mean squared error": [
        "$ARG1 is more popular than $ARG2"
    ],
    "mean squared error*****distribution": [
        "$ARG1 or mean absolute error , even when it is not necessary to estimate an entire $ARG2"
    ],
    "cross-entropy*****distribution": [
        "$ARG1 between the data $ARG2",
        "$ARG1 associated with that $ARG2"
    ],
    "output layer*****task": [
        "$ARG1 is then to provide some additional transformation from the features to complete the $ARG2"
    ],
    "deep feedforward network*****maximum likelihood": [
        "$ARG1 Maximizing the log-likelihood is then equivalent to minimizing the mean squared The $ARG2"
    ],
    "maximum likelihood*****covariance": [
        "$ARG1 framework makes it straightforward to learn the $ARG2"
    ],
    "covariance*****function": [
        "$ARG1 of the Gaussian be a $ARG2"
    ],
    "covariance*****positive de\ufb01nite": [
        "$ARG1 must be constrained to be a $ARG2"
    ],
    "output layer*****other": [
        "$ARG1 , so typically $ARG2"
    ],
    "optimization algorithm*****optimization": [
        "$ARG1 and may be used with a wide variety of $ARG2"
    ],
    "bernoulli distribution*****number": [
        "$ARG1 is de\ufb01ned by just a single $ARG2"
    ],
    "number*****probability": [
        "$ARG1 to be a valid $ARG2",
        "$ARG1 estimating the true $ARG2"
    ],
    "distribution*****gradient descent": [
        "$ARG1 , but we would not be able to train it very e\ufb00ectively with $ARG2"
    ],
    "sigmoid*****maximum likelihood": [
        "$ARG1 output units combined with $ARG2"
    ],
    "sigmoid*****\u03c3": [
        "$ARG1 output unit is de\ufb01ned by y\u0302 = $ARG2",
        "$ARG1 is J ( \u03b8 ) = \u2212 log P ( y | x ) = \u2212 log $ARG2",
        "$ARG1 unit : \ue058 f ( t ) \ue058 f ( t\u22121 ) fi = $ARG2",
        "$ARG1 unit for gating : h i = tanh si qi o ( t ) o ( t\u22121 ) \uf8f8 qi = $ARG2"
    ],
    "deep feedforward network*****\u03c3": [
        "$ARG1 where $ARG2"
    ],
    "\u03c3*****logistic sigmoid": [
        "$ARG1 is the $ARG2",
        "$ARG1 is the $ARG2",
        "$ARG1 ( x ) Figure 3.3 : The $ARG2"
    ],
    "activation function*****probability": [
        "$ARG1 to convert z into a $ARG2"
    ],
    "dependence*****probability distribution": [
        "$ARG1 on x for the moment to discuss how to de\ufb01ne a $ARG2"
    ],
    "sigmoid*****unnormalized probability distribution": [
        "$ARG1 can be motivated by constructing an $ARG2"
    ],
    "unnormalized probability distribution*****1": [
        "$ARG1 P\u0303 ( y ) , which does not sum to $ARG2",
        "$ARG1 is guaranteed to be non-negative everywhere , it is not guaranteed to sum or integrate to $ARG2"
    ],
    "constant*****probability distribution": [
        "$ARG1 to obtain a valid $ARG2"
    ],
    "bernoulli distribution*****\u03c3": [
        "$ARG1 controlled by a sigmoidal transformation of z : log P\u0303 ( y ) = yz P\u0303 ( y ) = exp ( yz ) exp ( yz ) P ( y ) = \ue0501 y \ue030=0 exp ( y z ) P ( y ) = $ARG2"
    ],
    "cost function*****sigmoid": [
        "$ARG1 undoes the exp of the $ARG2",
        "$ARG1 can undo the saturation of the $ARG2"
    ],
    "sigmoid*****learning": [
        "$ARG1 could prevent gradientbased $ARG2"
    ],
    "loss function*****maximum likelihood": [
        "$ARG1 for $ARG2"
    ],
    "learning*****sigmoid": [
        "$ARG1 of a Bernoulli parametrized by a $ARG2"
    ],
    "loss*****softplus": [
        "$ARG1 in terms of the $ARG2"
    ],
    "deep feedforward network*****1": [
        "$ARG1 ( $ARG2",
        "$ARG1 u ( $ARG2"
    ],
    "derivative*****softplus": [
        "$ARG1 with respect to z asymptotes to sign ( z ) , so , in the limit of extremely incorrect z , the $ARG2"
    ],
    "property*****learning": [
        "$ARG1 is very useful because it means that gradient-based $ARG2"
    ],
    "loss function*****mean squared error": [
        "$ARG1 , such as $ARG2",
        "$ARG1 penalizing g ( f ( x ) ) for being dissimilar from x , such as the $ARG2"
    ],
    "loss*****\u03c3": [
        "$ARG1 can saturate anytime $ARG2"
    ],
    "activation function*****1": [
        "$ARG1 saturates to 0 when z becomes very negative and saturates to $ARG2",
        "$ARG1 are closely related because tanh ( z ) = 2\u03c3 ( 2z ) \u2212 $ARG2"
    ],
    "maximum likelihood*****sigmoid": [
        "$ARG1 is almost always the preferred approach to training $ARG2"
    ],
    "sigmoid*****1": [
        "$ARG1 returns values restricted to the open interval ( 0 , $ARG2",
        "$ARG1 unit to obtain a gating value between 0 and $ARG2",
        "$ARG1 units saturate to 0 or $ARG2",
        "$ARG1 units must be trained to be saturated to nearly 0 or nearly $ARG2"
    ],
    "function*****\u03c3": [
        "$ARG1 of y\u0302 = $ARG2",
        "$ARG1 more closely , in the sense that tanh ( 0 ) = 0 while $ARG2",
        "$ARG1 can be useful for producing the \u03b2 or $ARG2"
    ],
    "softmax*****probability distribution": [
        "$ARG1 Units for Multinoulli Output Distributions Any time we wish to represent a $ARG2",
        "$ARG1 units naturally represent a $ARG2"
    ],
    "variable*****softmax function": [
        "$ARG1 with n possible values , we may use the $ARG2"
    ],
    "generalization*****sigmoid function": [
        "$ARG1 of the $ARG2"
    ],
    "sigmoid function*****probability distribution": [
        "$ARG1 which was used to represent a $ARG2"
    ],
    "softmax function*****probability distribution": [
        "$ARG1 are most often used as the output of a classi\ufb01er , to represent the $ARG2"
    ],
    "softmax function*****model": [
        "$ARG1 can be used inside the $ARG2",
        "$ARG1 to relevance scores emitted by another portion of the $ARG2"
    ],
    "number*****optimization": [
        "$ARG1 to be well-behaved for gradient-based $ARG2"
    ],
    "optimization*****number": [
        "$ARG1 of the log-likelihood , we chose to instead predict a $ARG2",
        "$ARG1 is that computation time per update does not grow with the $ARG2",
        "$ARG1 techniques to optimize a \ufb01nite $ARG2"
    ],
    "deep feedforward network*****variable": [
        "$ARG1 To generalize to the case of a discrete $ARG2"
    ],
    "variable*****vector": [
        "$ARG1 with n values , we now need to produce a $ARG2",
        "$ARG1 y given an input $ARG2"
    ],
    "1*****vector": [
        "$ARG1 , but also that the entire $ARG2",
        "$ARG1 ) ) to a \ufb01xed length $ARG2",
        "$ARG1 by swapping the incoming weight $ARG2"
    ],
    "bernoulli distribution*****multinoulli distribution": [
        "$ARG1 generalizes to the $ARG2"
    ],
    "softmax function*****softmax": [
        "$ARG1 is given by exp ( zi ) $ARG2",
        "$ARG1 responds to the di\ufb00erence between its inputs , observe that the $ARG2",
        "$ARG1 is exp ( xi ) $ARG2"
    ],
    "softmax*****logistic sigmoid": [
        "$ARG1 ( z ) i = \ue050 j exp ( zj ) As with the $ARG2"
    ],
    "logistic sigmoid*****function": [
        "$ARG1 , the use of the exp $ARG2"
    ],
    "function*****softmax": [
        "$ARG1 works very well when training the $ARG2",
        "$ARG1 is $ARG2",
        "$ARG1 that calculates log $ARG2"
    ],
    "model*****softmax": [
        "$ARG1 to learn parameters that drive the $ARG2",
        "$ARG1 based on a $ARG2",
        "$ARG1 consisted of n di\ufb00erent $ARG2"
    ],
    "deep feedforward network*****set": [
        "$ARG1 the fraction of counts of each outcome observed in the training $ARG2",
        "$ARG1 one of these groups : g ( z ) i = max zj j\u2208G ( i ) where G ( i ) is the $ARG2"
    ],
    "set*****softmax": [
        "$ARG1 : j=1 1y ( j ) =i , x ( j ) =x $ARG2"
    ],
    "softmax*****maximum likelihood": [
        "$ARG1 ( z ( x ; \u03b8 ) ) i \u2248 j=1 1x ( j ) =x Because $ARG2"
    ],
    "optimization*****model": [
        "$ARG1 will mean that the $ARG2",
        "$ARG1 FOR TRAINING DEEP MODELS Sometimes , directly training a $ARG2",
        "$ARG1 procedure and a $ARG2",
        "$ARG1 of a $ARG2",
        "$ARG1 does not seem to be a signi\ufb01cant barrier , provided that the $ARG2",
        "$ARG1 within this $ARG2"
    ],
    "other*****softmax function": [
        "$ARG1 than the log-likelihood do not work as well with the $ARG2"
    ],
    "objective function*****softmax": [
        "$ARG1 that do not use a log to undo the exp of the $ARG2"
    ],
    "softmax*****gradient": [
        "$ARG1 fail to learn when the argument to the exp becomes very negative , causing the $ARG2"
    ],
    "loss function*****softmax": [
        "$ARG1 for $ARG2"
    ],
    "softmax*****model": [
        "$ARG1 units , and can fail to train the $ARG2",
        "$ARG1 , the negative log-likelihood can become arbitrarily close to zero if the $ARG2"
    ],
    "loss function*****softmax function": [
        "$ARG1 can fail , we need to examine the $ARG2"
    ],
    "sigmoid*****softmax": [
        "$ARG1 , the $ARG2",
        "$ARG1 , and $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 output unit , discrete x values correspond to a $ARG2"
    ],
    "softmax*****multiple": [
        "$ARG1 , there are $ARG2"
    ],
    "softmax*****cost function": [
        "$ARG1 saturates , many $ARG2"
    ],
    "cost function*****softmax": [
        "$ARG1 based on the $ARG2"
    ],
    "softmax*****function": [
        "$ARG1 also saturate , unless they are able to invert the saturating activating $ARG2",
        "$ARG1 subroutine then passing the result to the log $ARG2"
    ],
    "softmax*****invariant": [
        "$ARG1 output is $ARG2"
    ],
    "invariant*****scalar": [
        "$ARG1 to adding the same $ARG2"
    ],
    "scalar*****softmax": [
        "$ARG1 to all of its $ARG2"
    ],
    "property*****softmax": [
        "$ARG1 , we can derive a numerically stable variant of the $ARG2"
    ],
    "softmax*****contains": [
        "$ARG1 with only small numerical errors even when z $ARG2"
    ],
    "softmax*****1": [
        "$ARG1 ( z ) i saturates to $ARG2",
        "$ARG1 ( z ) $ARG2",
        "$ARG1 outputs always sum to $ARG2",
        "$ARG1 ( b ) , so a large value of bi actually reduces p ( x j = $ARG2",
        "$ARG1 with k output values by replacing the hard 0 and $ARG2"
    ],
    "generalization*****sigmoid": [
        "$ARG1 of the way that $ARG2"
    ],
    "learning*****loss function": [
        "$ARG1 if the $ARG2"
    ],
    "neural network*****element": [
        "$ARG1 output every $ARG2"
    ],
    "constraint*****1": [
        "$ARG1 that the n outputs must sum to $ARG2",
        "$ARG1 that the sum of the probabilities be $ARG2",
        "$ARG1 that p ( x ) integrates to $ARG2",
        "$ARG1 : arg min \u22122 Tr ( X\ue03eXdd\ue03e ) + Tr ( X \ue03eXdd\ue03edd\ue03e ) subject to d \ue03ed = $ARG2",
        "$ARG1 ) = arg min \u2212 Tr ( X \ue03eXdd\ue03e ) subject to d\ue03ed = $ARG2",
        "$ARG1 , such as ||x|| \u2264 $ARG2",
        "$ARG1 x\ue03e x \u2264 $ARG2",
        "$ARG1 that the factor must sum or integrate to $ARG2"
    ],
    "1*****softmax": [
        "$ARG1 | x ) = $ARG2",
        "$ARG1 argument and the n argument approaches to the $ARG2",
        "$ARG1 to t = \u03c4 , we apply the following update equations : a ( t ) = b + W h ( t\u22121 ) + U x ( t ) h ( t ) o ( t ) = tanh ( a ( t ) ) = c + V h ( t ) y\u0302 ( t ) = $ARG2",
        "$ARG1 ] } ) then p ( x ) = $ARG2"
    ],
    "softmax*****set": [
        "$ARG1 can describe the same $ARG2"
    ],
    "view*****softmax": [
        "$ARG1 , it is interesting to think of the $ARG2"
    ],
    "magnitude*****1": [
        "$ARG1 ) it becomes a form of winner-take-all ( one of the outputs is nearly $ARG2",
        "$ARG1 near $ARG2",
        "$ARG1 of w. However , if we make a non-recurrent network that has a di\ufb00erent weight w ( t ) at each time step , the situation If the initial state is given by $ARG2",
        "$ARG1 or vanish if they are less than $ARG2"
    ],
    "softmax function*****convention": [
        "$ARG1 \u201c softargmax , \u201d but the current name is an entrenched $ARG2"
    ],
    "neural network*****output layer": [
        "$ARG1 can generalize to almost any kind of $ARG2"
    ],
    "cost function*****output layer": [
        "$ARG1 for nearly any kind of $ARG2"
    ],
    "\u03c3*****constant": [
        "$ARG1 2 is a $ARG2"
    ],
    "constant*****maximum likelihood": [
        "$ARG1 , there is a closed form expression because the $ARG2"
    ],
    "variance*****expected value": [
        "$ARG1 is simply the empirical mean of the squared di\ufb00erence between observations y and their $ARG2",
        "$ARG1 is low , the values of f ( x ) cluster near their $ARG2"
    ],
    "cost function*****optimization": [
        "$ARG1 with the appropriate terms necessary to make our $ARG2",
        "$ARG1 , an $ARG2"
    ],
    "optimization*****variance": [
        "$ARG1 procedure incrementally learn the $ARG2"
    ],
    "\u03c3*****distribution": [
        "$ARG1 itself or could be a parameter v representing \u03c32 or it could be a parameter \u03b2 representing \u03c312 , depending on how we choose to parametrize the $ARG2"
    ],
    "model*****variance": [
        "$ARG1 to predict a di\ufb00erent amount of $ARG2"
    ],
    "heteroscedastic*****variance": [
        "$ARG1 case , we simply make the speci\ufb01cation of the $ARG2"
    ],
    "gaussian distribution*****precision": [
        "$ARG1 using $ARG2"
    ],
    "precision*****variance": [
        "$ARG1 , rather than $ARG2",
        "$ARG1 or inverse $ARG2"
    ],
    "gradient descent*****gaussian distribution": [
        "$ARG1 because the formula for the log-likelihood of the $ARG2"
    ],
    "gaussian distribution*****multiplication": [
        "$ARG1 parametrized by \u03b2 involves only $ARG2"
    ],
    "gradient*****multiplication": [
        "$ARG1 of $ARG2",
        "$ARG1 into the prenonlinearity activation ( element-wise $ARG2"
    ],
    "gradient*****operation": [
        "$ARG1 through the squaring $ARG2",
        "$ARG1 of performing such a Jacobian-gradient product for each $ARG2",
        "$ARG1 by the Jacobian of the $ARG2",
        "$ARG1 on the output of the $ARG2",
        "$ARG1 on the unnormalized log probabilities U ( 2 ) provided by the cross_entropy $ARG2",
        "$ARG1 will never propose an $ARG2",
        "$ARG1 computations , or if you are adding a new $ARG2"
    ],
    "deep feedforward network*****standard deviation": [
        "$ARG1 Regardless of whether we use $ARG2"
    ],
    "standard deviation*****variance": [
        "$ARG1 , $ARG2"
    ],
    "precision*****covariance matrix": [
        "$ARG1 , we must ensure that the $ARG2"
    ],
    "matrix*****covariance matrix": [
        "$ARG1 are the reciprocals of the eigenvalues of the $ARG2",
        "$ARG1 X \ue03e X in equation 7.16 is proportional to the $ARG2"
    ],
    "matrix*****positive de\ufb01nite": [
        "$ARG1 is $ARG2",
        "$ARG1 whose eigenvalues are all positive is called $ARG2"
    ],
    "diagonal matrix*****scalar": [
        "$ARG1 , or a $ARG2"
    ],
    "scalar*****diagonal matrix": [
        "$ARG1 times the $ARG2"
    ],
    "diagonal matrix*****model": [
        "$ARG1 , then the only condition we need to enforce on the output of the $ARG2"
    ],
    "model*****diagonal": [
        "$ARG1 used to determine the $ARG2"
    ],
    "precision*****softplus": [
        "$ARG1 , we can use the $ARG2"
    ],
    "function*****precision": [
        "$ARG1 to obtain a positive $ARG2"
    ],
    "variance*****standard deviation": [
        "$ARG1 or $ARG2",
        "$ARG1 is known as the $ARG2"
    ],
    "standard deviation*****precision": [
        "$ARG1 rather than $ARG2"
    ],
    "precision*****scalar": [
        "$ARG1 or if using a $ARG2"
    ],
    "scalar*****identity": [
        "$ARG1 times $ARG2"
    ],
    "identity*****covariance": [
        "$ARG1 rather than It is rare to learn a $ARG2"
    ],
    "covariance*****precision": [
        "$ARG1 or $ARG2"
    ],
    "matrix*****diagonal": [
        "$ARG1 with richer structure than $ARG2",
        "$ARG1 , and \u03b2 is a learnable , $ARG2",
        "$ARG1 across a $ARG2",
        "$ARG1 D is $ARG2",
        "$ARG1 is the same as the original one , but with the addition of \u03b1 to the $ARG2"
    ],
    "covariance*****covariance matrix": [
        "$ARG1 is full and conditional , then a parametrization must be chosen that guarantees positive-de\ufb01niteness of the predicted $ARG2"
    ],
    "\u03c3*****square matrix": [
        "$ARG1 ( x ) = B ( x ) B\ue03e ( x ) , where B is an unconstrained $ARG2"
    ],
    "matrix*****determinant": [
        "$ARG1 requiring O ( d3 ) computation for the $ARG2"
    ],
    "determinant*****\u03c3": [
        "$ARG1 and inverse of $ARG2"
    ],
    "\u03c3*****eigendecomposition": [
        "$ARG1 ( x ) ( or equivalently , and more commonly done , its $ARG2"
    ],
    "gaussian mixture*****bishop": [
        "$ARG1 is a natural representation for the output ( Jacobs et al. , 1991 ; $ARG2"
    ],
    "neural network*****gaussian mixture": [
        "$ARG1 with $ARG2"
    ],
    "gaussian mixture*****mixture density networks": [
        "$ARG1 as their output are often called $ARG2"
    ],
    "gaussian mixture*****conditional probability distribution": [
        "$ARG1 output with n components is de\ufb01ned by the $ARG2"
    ],
    "conditional probability distribution*****\u03c3": [
        "$ARG1 p ( y | x ) = p ( c = i | x ) N ( y ; \u00b5 ( i ) ( x ) , $ARG2"
    ],
    "neural network*****vector": [
        "$ARG1 must have three outputs : a $ARG2"
    ],
    "vector*****matrix": [
        "$ARG1 de\ufb01ning p ( c = i | x ) , a $ARG2",
        "$ARG1 , $ARG2",
        "$ARG1 is therefore a $ARG2",
        "$ARG1 , yielding another $ARG2",
        "$ARG1 by that $ARG2",
        "$ARG1 x \u2208 Rn is an n \u00d7 n $ARG2"
    ],
    "matrix*****tensor": [
        "$ARG1 providing \u00b5 ( i ) ( x ) for all i , and a $ARG2",
        "$ARG1 , $ARG2"
    ],
    "tensor*****\u03c3": [
        "$ARG1 providing $ARG2"
    ],
    "multinoulli distribution*****variable": [
        "$ARG1 over the n di\ufb00erent components associated with latent $ARG2"
    ],
    "deep feedforward network*****softmax": [
        "$ARG1 typically be obtained by a $ARG2"
    ],
    "softmax*****vector": [
        "$ARG1 over an n-dimensional $ARG2",
        "$ARG1 regression classi\ufb01er with n input variables represented by the $ARG2"
    ],
    "maximum likelihood*****learning": [
        "$ARG1 is slightly more complicated than $ARG2"
    ],
    "weights*****example": [
        "$ARG1 each $ARG2"
    ],
    "example*****loss": [
        "$ARG1 \u2019 s contribution to the $ARG2",
        "$ARG1 , the negative log-likelihood of the correct class is typically used as a surrogate for the 0-1 $ARG2",
        "$ARG1 is the ranking $ARG2",
        "$ARG1 , the negative conditional log-likelihood of the training data can be written as J ( \u03b8 ) = Ex , y\u223cp\u0302data L ( x , y , \u03b8 ) = L ( x ( i ) , y ( i ) , \u03b8 ) where L is the per-example $ARG2"
    ],
    "loss*****probability": [
        "$ARG1 for each component by the $ARG2"
    ],
    "\u03c3*****covariance matrix": [
        "$ARG1 ( i ) ( x ) : these specify the $ARG2",
        "$ARG1 gives the $ARG2"
    ],
    "learning*****diagonal matrix": [
        "$ARG1 a single Gaussian component , we typically use a $ARG2"
    ],
    "gradient descent*****process": [
        "$ARG1 will automatically follow the correct $ARG2"
    ],
    "process*****mixture model": [
        "$ARG1 if given the correct speci\ufb01cation of the negative log-likelihood under the $ARG2"
    ],
    "optimization*****gaussian mixture": [
        "$ARG1 of conditional $ARG2"
    ],
    "gaussian mixture*****neural network": [
        "$ARG1 ( on the output of $ARG2"
    ],
    "neural network*****variance": [
        "$ARG1 ) can be unreliable , in part because one gets divisions ( by the $ARG2"
    ],
    "variance*****example": [
        "$ARG1 gets to be small for a particular $ARG2",
        "$ARG1 ? How do we choose between them ? For $ARG2"
    ],
    "gaussian mixture*****speech": [
        "$ARG1 outputs are particularly e\ufb00ective in generative models of $ARG2"
    ],
    "mixture density*****multiple": [
        "$ARG1 strategy gives a way for the network to represent $ARG2"
    ],
    "multiple*****variance": [
        "$ARG1 output modes and to control the $ARG2"
    ],
    "variance*****quality": [
        "$ARG1 of its output , which is crucial for obtaining a high degree of $ARG2"
    ],
    "example*****mixture density": [
        "$ARG1 of a $ARG2"
    ],
    "neural network*****mixture density": [
        "$ARG1 with a $ARG2"
    ],
    "uniform distribution*****model": [
        "$ARG1 and the output y is sampled from p $ARG2"
    ],
    "recurrent neural network*****neural network": [
        "$ARG1 to de\ufb01ne such models over sequences , and part III describes advanced techniques for modeling arbitrary So far we have focused our discussion on design choices for $ARG2",
        "$ARG1 or RNNs ( Rumelhart et al. , 1986a ) are a family of $ARG2",
        "$ARG1 is a $ARG2"
    ],
    "neural network*****machine learning": [
        "$ARG1 that are common to most parametric $ARG2",
        "$ARG1 used for $ARG2",
        "$ARG1 used for $ARG2",
        "$ARG1 ? $ARG2"
    ],
    "machine learning*****optimization": [
        "$ARG1 models trained with gradientbased $ARG2",
        "$ARG1 is still very similar to straightforward $ARG2",
        "$ARG1 has avoided the di\ufb03culty of general $ARG2",
        "$ARG1 from $ARG2",
        "$ARG1 , rivaled in its importance only by $ARG2",
        "$ARG1 is $ARG2"
    ],
    "turn*****feedforward neural network": [
        "$ARG1 to an issue that is unique to $ARG2"
    ],
    "feedforward neural network*****hidden layer": [
        "$ARG1 : how to choose the type of hidden unit to use in the $ARG2",
        "$ARG1 with at least one $ARG2"
    ],
    "process*****set": [
        "$ARG1 consists of trial and error , intuiting that a kind of hidden unit may work well , and then training a network with that kind of hidden unit and evaluating its performance on a validation $ARG2",
        "$ARG1 , where we $ARG2"
    ],
    "gradient descent*****machine learning": [
        "$ARG1 still performs well enough for these models to be used for $ARG2"
    ],
    "cost function*****gradient": [
        "$ARG1 to correspond to points with unde\ufb01ned $ARG2",
        "$ARG1 , which may be due to high noise in the $ARG2",
        "$ARG1 has put a $ARG2",
        "$ARG1 we would expect based on the $ARG2",
        "$ARG1 actually decreases faster than the $ARG2",
        "$ARG1 predicted by the $ARG2"
    ],
    "function*****derivative": [
        "$ARG1 g ( z ) has a left $ARG2",
        "$ARG1 immediately to the left of z and a right $ARG2",
        "$ARG1 is di\ufb00erentiable at z only if both the left $ARG2",
        "$ARG1 ( for some functions , the correct response is to report that the $ARG2",
        "$ARG1 where the functional $ARG2",
        "$ARG1 must have a large $ARG2",
        "$ARG1 f : Rn \u2192 R , the $ARG2"
    ],
    "derivative*****slope": [
        "$ARG1 de\ufb01ned by the $ARG2",
        "$ARG1 de\ufb01ned by the $ARG2",
        "$ARG1 f ( x ) gives the $ARG2"
    ],
    "slope*****function": [
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "derivative*****other": [
        "$ARG1 are de\ufb01ned and equal to each $ARG2"
    ],
    "derivative*****1": [
        "$ARG1 is $ARG2"
    ],
    "neural network*****derivative": [
        "$ARG1 training usually return one of the one-sided derivatives rather than reporting that the $ARG2"
    ],
    "vector*****a\ufb03ne": [
        "$ARG1 of inputs x , computing an $ARG2"
    ],
    "other*****activation function": [
        "$ARG1 only by the choice of the form of the $ARG2"
    ],
    "second derivative*****operation": [
        "$ARG1 of the rectifying $ARG2"
    ],
    "operation*****almost everywhere": [
        "$ARG1 is 0 $ARG2"
    ],
    "almost everywhere*****derivative": [
        "$ARG1 , and the $ARG2"
    ],
    "derivative*****operation": [
        "$ARG1 of the rectifying $ARG2"
    ],
    "operation*****1": [
        "$ARG1 is $ARG2",
        "$ARG1 , with t % t = 0 , ( t + $ARG2",
        "$ARG1 , but many cases of interest , including the case of stride greater than $ARG2"
    ],
    "learning*****activation function": [
        "$ARG1 than it would be with $ARG2",
        "$ARG1 the $ARG2",
        "$ARG1 is far easier in deep recti\ufb01ed linear networks than in deep networks that have curvature or two-sided saturation in their $ARG2",
        "$ARG1 in these models captures many of the qualitative features observed in the training of deep models with nonlinear $ARG2"
    ],
    "recti\ufb01ed linear unit*****a\ufb03ne": [
        "$ARG1 are typically used on top of an $ARG2"
    ],
    "a\ufb03ne*****set": [
        "$ARG1 transformation , it can be a good practice to $ARG2",
        "$ARG1 transformation followed by an element-wise nonlinearity , so it is straightforward to $ARG2"
    ],
    "recti\ufb01ed linear unit*****set": [
        "$ARG1 will be initially active for most inputs in the training $ARG2"
    ],
    "set*****pass": [
        "$ARG1 and allow the derivatives to $ARG2"
    ],
    "recti\ufb01ed linear unit*****gradient": [
        "$ARG1 guarantee that they receive $ARG2"
    ],
    "recti\ufb01ed linear unit*****slope": [
        "$ARG1 are based on using a non-zero $ARG2"
    ],
    "object recognition*****invariant": [
        "$ARG1 from images ( Jarrett et al. , 2009 ) , where it makes sense to seek features that are $ARG2"
    ],
    "other*****recti\ufb01ed linear unit": [
        "$ARG1 generalizations of $ARG2"
    ],
    "leaky relu*****parametric relu": [
        "$ARG1 ( Maas et al. , 2013 ) \ufb01xes \u03b1i to a small value like 0.01 while a $ARG2"
    ],
    "maxout*****recti\ufb01ed linear unit": [
        "$ARG1 units ( Goodfellow et al. , 2013a ) generalize $ARG2"
    ],
    "function*****maxout": [
        "$ARG1 g ( z ) , $ARG2"
    ],
    "maxout*****element": [
        "$ARG1 unit then outputs the maximum $ARG2"
    ],
    "set*****group": [
        "$ARG1 of indices into the inputs for $ARG2",
        "$ARG1 , while the use of the term \u201c batch \u201d to describe a $ARG2"
    ],
    "function*****multiple": [
        "$ARG1 that responds to $ARG2",
        "$ARG1 we want to learn is a computer program consisting of $ARG2",
        "$ARG1 f with $ARG2",
        "$ARG1 has $ARG2"
    ],
    "maxout*****function": [
        "$ARG1 unit can learn a piecewise linear , convex $ARG2",
        "$ARG1 unit can learn to approximate any convex $ARG2",
        "$ARG1 layer with two pieces can learn to implement the same $ARG2",
        "$ARG1 learns to implement the same $ARG2"
    ],
    "maxout*****learning": [
        "$ARG1 units can thus be seen as $ARG2"
    ],
    "activation function*****absolute value recti\ufb01cation": [
        "$ARG1 , $ARG2"
    ],
    "function*****parametric relu": [
        "$ARG1 , or the leaky or $ARG2"
    ],
    "parametric relu*****function": [
        "$ARG1 , or can learn to implement a totally di\ufb00erent $ARG2"
    ],
    "maxout*****other": [
        "$ARG1 layer will of course be parametrized di\ufb00erently from any of these $ARG2",
        "$ARG1 units have a few $ARG2",
        "$ARG1 units that have several \ufb01lters that must be carefully coordinated with each $ARG2",
        "$ARG1 networks ( Goodfellow et al. , 2013a ) and $ARG2"
    ],
    "learning*****maxout": [
        "$ARG1 dynamics will be di\ufb00erent even in the cases where $ARG2"
    ],
    "maxout*****regularization": [
        "$ARG1 units typically need more $ARG2"
    ],
    "regularization*****recti\ufb01ed linear unit": [
        "$ARG1 than $ARG2"
    ],
    "regularization*****set": [
        "$ARG1 if the training $ARG2"
    ],
    "information*****group": [
        "$ARG1 by taking the max over each $ARG2"
    ],
    "group*****weights": [
        "$ARG1 of k features , then the next layer can get by with k times fewer $ARG2"
    ],
    "multiple*****maxout": [
        "$ARG1 \ufb01lters , $ARG2"
    ],
    "maxout*****phenomenon": [
        "$ARG1 units have some redundancy that helps them to resist a $ARG2"
    ],
    "phenomenon*****neural network": [
        "$ARG1 called catastrophic forgetting in which $ARG2"
    ],
    "recti\ufb01ed linear unit*****principle": [
        "$ARG1 and all of these generalizations of them are based on the $ARG2"
    ],
    "principle*****optimization": [
        "$ARG1 of using linear behavior to obtain easier $ARG2",
        "$ARG1 , this approach could fail due to $ARG2"
    ],
    "optimization*****other": [
        "$ARG1 also applies in $ARG2"
    ],
    "information*****directional derivative": [
        "$ARG1 through several time steps , which is much easier when some linear computations ( with some $ARG2"
    ],
    "directional derivative*****magnitude": [
        "$ARG1 being of $ARG2"
    ],
    "deep feedforward network*****information": [
        "$ARG1 architectures , the LSTM , propagates $ARG2"
    ],
    "logistic sigmoid*****recti\ufb01ed linear unit": [
        "$ARG1 and Hyperbolic Tangent Prior to the introduction of $ARG2"
    ],
    "recti\ufb01ed linear unit*****neural network": [
        "$ARG1 , most $ARG2"
    ],
    "neural network*****logistic sigmoid": [
        "$ARG1 used the $ARG2"
    ],
    "activation function*****\u03c3": [
        "$ARG1 g ( z ) = $ARG2"
    ],
    "\u03c3*****activation function": [
        "$ARG1 ( z ) or the hyperbolic tangent $ARG2"
    ],
    "sigmoid*****activation function": [
        "$ARG1 in the output When a sigmoidal $ARG2"
    ],
    "activation function*****logistic sigmoid": [
        "$ARG1 typically performs better than the $ARG2",
        "$ARG1 ( such as the $ARG2"
    ],
    "activation function*****other": [
        "$ARG1 are more common in settings $ARG2"
    ],
    "recurrent network*****rule": [
        "$ARG1 , many probabilistic models , and some autoencoders have additional requirements that $ARG2"
    ],
    "rule*****activation function": [
        "$ARG1 out the use of piecewise linear $ARG2"
    ],
    "deep feedforward network*****other": [
        "$ARG1 Many $ARG2",
        "$ARG1 A few $ARG2"
    ],
    "example*****dataset": [
        "$ARG1 , the authors tested a feedforward network using h = cos ( W x + b ) on the MNIST $ARG2",
        "$ARG1 inputs from the MNIST $ARG2",
        "$ARG1 in the entire $ARG2",
        "$ARG1 , we might have a $ARG2",
        "$ARG1 , the Iris $ARG2",
        "$ARG1 , suppose that we run two clustering algorithms on a $ARG2"
    ],
    "dataset*****rate": [
        "$ARG1 and obtained an error $ARG2"
    ],
    "rate*****1": [
        "$ARG1 of less than $ARG2",
        "$ARG1 CHAPTER $ARG2"
    ],
    "1*****activation function": [
        "$ARG1 % , which is competitive with results obtained using more conventional $ARG2"
    ],
    "activation function*****standard": [
        "$ARG1 and \ufb01nd that several variations on $ARG2"
    ],
    "matrix*****other": [
        "$ARG1 U and the $ARG2",
        "$ARG1 has several entries constrained to be equal to $ARG2"
    ],
    "other*****matrix": [
        "$ARG1 using weight $ARG2",
        "$ARG1 groups of units , so that the interactions between two groups may be described by a single $ARG2"
    ],
    "activation function*****matrix": [
        "$ARG1 , then we have essentially factored the weight $ARG2"
    ],
    "other*****radial basis function": [
        "$ARG1 reasonably common hidden unit types include : \u2022 $ARG2"
    ],
    "radial basis function*****\u03c3": [
        "$ARG1 or RBF unit : hi = exp \u2212 $ARG2"
    ],
    "function*****template": [
        "$ARG1 becomes more active as x approaches a $ARG2"
    ],
    "softplus*****1": [
        "$ARG1 : g ( a ) = \u03b6 ( a ) = log ( $ARG2"
    ],
    "hard tanh*****1": [
        "$ARG1 : this is shaped similarly to the tanh and the recti\ufb01er but unlike the latter , it is bounded , g ( a ) = max ( \u22121 , min ( $ARG2"
    ],
    "hidden layer*****set": [
        "$ARG1 is su\ufb03cient to \ufb01t the training $ARG2",
        "$ARG1 , the student network performs very poorly in the experiments , both on the training and test $ARG2"
    ],
    "linear model*****mapping": [
        "$ARG1 , $ARG2"
    ],
    "mapping*****matrix multiplication": [
        "$ARG1 from features to outputs via $ARG2"
    ],
    "loss function*****convex optimization": [
        "$ARG1 result in $ARG2"
    ],
    "convex optimization*****linear model": [
        "$ARG1 problems when applied to $ARG2"
    ],
    "function*****model": [
        "$ARG1 requires designing a specialized $ARG2",
        "$ARG1 of their input , but they are useful to study as a $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 over another is a more general way of controlling a $ARG2",
        "$ARG1 estimation is really just the same as f with a $ARG2",
        "$ARG1 shown in \ufb01gure 5.2 and we are only o\ufb00ered the choice between a $ARG2",
        "$ARG1 in a $ARG2",
        "$ARG1 of the current value of the $ARG2"
    ],
    "universal approximation theorem*****output layer": [
        "$ARG1 ( Hornik et al. , 1989 ; Cybenko , 1989 ) states that a feedforward network with a linear $ARG2"
    ],
    "output layer*****hidden layer": [
        "$ARG1 and at least one $ARG2",
        "$ARG1 and going backwards to the \ufb01rst $ARG2",
        "$ARG1 poses Suppose that h is the top $ARG2"
    ],
    "activation function*****measurable function": [
        "$ARG1 ) can approximate any Borel $ARG2"
    ],
    "concept*****function": [
        "$ARG1 of Borel measurability is beyond the scope of this book ; for our purposes it su\ufb03ces to say that any continuous $ARG2"
    ],
    "function*****subset": [
        "$ARG1 on a closed and bounded $ARG2",
        "$ARG1 corresponds to classifying x with a di\ufb00erent $ARG2",
        "$ARG1 must be a $ARG2",
        "$ARG1 of only a $ARG2"
    ],
    "subset*****neural network": [
        "$ARG1 of R n is Borel measurable and therefore may be approximated by a $ARG2",
        "$ARG1 of the layers in the \ufb01nal $ARG2",
        "$ARG1 of many $ARG2",
        "$ARG1 of parameters from the parent $ARG2"
    ],
    "activation function*****universal approximation theorem": [
        "$ARG1 that saturate both for very negative and for very positive arguments , $ARG2"
    ],
    "universal approximation theorem*****activation function": [
        "$ARG1 have also been proved for a wider class of $ARG2"
    ],
    "activation function*****recti\ufb01ed linear unit": [
        "$ARG1 , which includes the now commonly used $ARG2",
        "$ARG1 , including $ARG2"
    ],
    "universal approximation theorem*****function": [
        "$ARG1 means that regardless of what $ARG2"
    ],
    "algorithm*****function": [
        "$ARG1 will be able to learn that $ARG2",
        "$ARG1 might choose the wrong $ARG2",
        "$ARG1 is usually asked to produce a $ARG2",
        "$ARG1 only has to de\ufb01ne a single $ARG2",
        "$ARG1 is asked to output a $ARG2",
        "$ARG1 is asked to learn a $ARG2",
        "$ARG1 does not actually \ufb01nd the best $ARG2",
        "$ARG1 , seen as a $ARG2",
        "$ARG1 as not having any parameters , but rather implementing a simple $ARG2",
        "$ARG1 to observe high points on most peaks and low points on most valleys of the true underlying $ARG2",
        "$ARG1 uses the derivatives of a $ARG2",
        "$ARG1 converges to the global minimum in a single Now suppose we wish to minimize the same $ARG2"
    ],
    "deep feedforward network*****function": [
        "$ARG1 may not be able to \ufb01nd the value of the parameters that corresponds to the desired $ARG2"
    ],
    "recall*****theorem": [
        "$ARG1 from section 5.2.1 that the \u201c no free lunch \u201d $ARG2"
    ],
    "theorem*****machine learning": [
        "$ARG1 shows that there is no universally superior $ARG2"
    ],
    "system*****function": [
        "$ARG1 for representing functions , in the sense that , given a $ARG2",
        "$ARG1 or a density $ARG2"
    ],
    "universal approximation theorem*****accuracy": [
        "$ARG1 says that there exists a network large enough to achieve any degree of $ARG2"
    ],
    "accuracy*****theorem": [
        "$ARG1 we desire , but the $ARG2"
    ],
    "number*****function": [
        "$ARG1 of units required to represent the desired $ARG2",
        "$ARG1 of times the transition $ARG2",
        "$ARG1 of training examples ? Clearly , assuming only smoothness of the underlying $ARG2",
        "$ARG1 of examples ) , is there any hope to The answer to both of these questions\u2014whether it is possible to represent a complicated $ARG2",
        "$ARG1 of parameters in the next layer is a $ARG2"
    ],
    "model*****neural network": [
        "$ARG1 is exponential in n. Such results were \ufb01rst proved for models that do not resemble the continuous , di\ufb00erentiable $ARG2",
        "$ARG1 , such as a biological $ARG2",
        "$ARG1 of nonlinear $ARG2",
        "$ARG1 is a large $ARG2"
    ],
    "recti\ufb01ed linear unit*****function": [
        "$ARG1 , have universal approximation properties , but these results do not address the questions of depth or e\ufb03ciency\u2014they specify only that a su\ufb03ciently wide recti\ufb01er network could represent any $ARG2"
    ],
    "deep feedforward network*****number": [
        "$ARG1 ( 2014 ) showed that functions representable with a deep recti\ufb01er net can require an exponential $ARG2"
    ],
    "number*****hidden layer": [
        "$ARG1 of hidden units with a shallow ( one $ARG2",
        "$ARG1 of linear regions carved out by a deep recti\ufb01er network with d inputs , depth l , and n units per $ARG2",
        "$ARG1 of $ARG2"
    ],
    "maxout*****number": [
        "$ARG1 units ) can represent functions with a $ARG2",
        "$ARG1 networks with k \ufb01lters per unit , the $ARG2"
    ],
    "absolute value recti\ufb01cation*****function": [
        "$ARG1 creates mirror images of the $ARG2"
    ],
    "symmetry*****weights": [
        "$ARG1 is given by the hyperplane de\ufb01ned by the $ARG2"
    ],
    "weights*****bias": [
        "$ARG1 and $ARG2",
        "$ARG1 are small enough that the output of the unit is determined only by the $ARG2",
        "$ARG1 to correct for the $ARG2"
    ],
    "function*****image": [
        "$ARG1 computed on top of that unit ( the green decision surface ) will be a mirror $ARG2",
        "$ARG1 for detecting edges in an $ARG2",
        "$ARG1 giving $ARG2",
        "$ARG1 to another $ARG2",
        "$ARG1 , such that I\ue030 = g ( I ) is the $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "image*****symmetry": [
        "$ARG1 of a simpler pattern across that axis of $ARG2"
    ],
    "function*****symmetry": [
        "$ARG1 can be obtained by folding the space around the axis of $ARG2"
    ],
    "symmetry*****hidden layer": [
        "$ARG1 ( which is now repeated four times , with two $ARG2"
    ],
    "deep feedforward network*****machine learning": [
        "$ARG1 Of course , there is no guarantee that the kinds of functions we want to learn in applications of $ARG2"
    ],
    "machine learning*****property": [
        "$ARG1 ( and in particular for AI ) share such a $ARG2",
        "$ARG1 library , because there is less variation in the range of valid values of m The commutative $ARG2"
    ],
    "function*****algorithm": [
        "$ARG1 the $ARG2",
        "$ARG1 of earlier time steps , the BPTT $ARG2"
    ],
    "representation learning*****view": [
        "$ARG1 point of $ARG2"
    ],
    "problem*****set": [
        "$ARG1 consists of discovering a $ARG2",
        "$ARG1 is so important and so expensive , a specialized $ARG2",
        "$ARG1 , with a training $ARG2",
        "$ARG1 based on adding a moderate amount of noise to a degree-5 polynomial , generated a single test $ARG2",
        "$ARG1 , we need a validation $ARG2",
        "$ARG1 is to create fake data and add it to the training $ARG2"
    ],
    "set*****factors of variation": [
        "$ARG1 of underlying $ARG2"
    ],
    "factors of variation*****turn": [
        "$ARG1 that can in $ARG2"
    ],
    "turn*****other": [
        "$ARG1 be described in terms of $ARG2"
    ],
    "other*****factors of variation": [
        "$ARG1 , simpler underlying $ARG2"
    ],
    "computer vision*****convolutional network": [
        "$ARG1 called $ARG2"
    ],
    "gradient*****output layer": [
        "$ARG1 to \ufb02ow from $ARG2",
        "$ARG1 on the $ARG2",
        "$ARG1 estimation for the $ARG2"
    ],
    "deep feedforward network*****accuracy": [
        "$ARG1 Test $ARG2",
        "$ARG1 Test $ARG2"
    ],
    "accuracy*****number": [
        "$ARG1 ( percent ) $ARG2",
        "$ARG1 of the address $ARG2"
    ],
    "subset*****output layer": [
        "$ARG1 of units in the $ARG2"
    ],
    "convolutional network*****computer vision": [
        "$ARG1 , described in chapter 9 , use specialized patterns of sparse connections that are very e\ufb00ective for $ARG2"
    ],
    "number*****convolutional network": [
        "$ARG1 of parameters in layers of $ARG2"
    ],
    "convolutional network*****set": [
        "$ARG1 without increasing their depth is not nearly as e\ufb00ective at increasing test $ARG2"
    ],
    "learning*****turn": [
        "$ARG1 a representation that is composed in $ARG2"
    ],
    "turn*****learning": [
        "$ARG1 of simpler representations ( e.g. , corners de\ufb01ned in terms of edges ) or in $ARG2"
    ],
    "learning*****set": [
        "$ARG1 a program with sequentially dependent steps ( e.g. , \ufb01rst locate a $ARG2",
        "$ARG1 algorithms that learn to associate some input with some output , given a training $ARG2",
        "$ARG1 , we must brie\ufb02y introduce an important $ARG2",
        "$ARG1 to $ARG2",
        "$ARG1 a separate $ARG2",
        "$ARG1 a separate $ARG2",
        "$ARG1 curves where the validation $ARG2",
        "$ARG1 How does one decide whether to gather more data ? First , determine whether the performance on the training $ARG2"
    ],
    "set*****other": [
        "$ARG1 of objects , then segment them from each $ARG2",
        "$ARG1 could be identical copies of each $ARG2",
        "$ARG1 of basis functions that are all mutually di\ufb00erent from each $ARG2",
        "$ARG1 di\ufb00erent units to compute di\ufb00erent functions from each $ARG2",
        "$ARG1 of words into clusters or classes , based on their co-occurrence frequencies with $ARG2",
        "$ARG1 of inputs that are connected directly to the output , and not connected to any $ARG2",
        "$ARG1 of known true positive facts compared to $ARG2",
        "$ARG1 into k di\ufb00erent clusters of examples that are near each $ARG2",
        "$ARG1 of formal rules for determining the likelihood of a proposition being true given the likelihood of $ARG2",
        "$ARG1 of vertices that may be connected to each $ARG2",
        "$ARG1 of nodes that are all connected to each $ARG2"
    ],
    "back-propagation*****other": [
        "$ARG1 and $ARG2"
    ],
    "other*****feedforward neural network": [
        "$ARG1 Di\ufb00erentiation Algorithms When we use a $ARG2"
    ],
    "feedforward neural network*****information": [
        "$ARG1 to accept an input x and produce an output y\u0302 , $ARG2"
    ],
    "forward propagation*****scalar": [
        "$ARG1 can continue onward until it produces a $ARG2"
    ],
    "algorithm*****backprop": [
        "$ARG1 ( Rumelhart et al. , 1986a ) , often simply called $ARG2"
    ],
    "backprop*****information": [
        "$ARG1 , allows the $ARG2"
    ],
    "information*****gradient": [
        "$ARG1 from the cost to then \ufb02ow backwards through the network , in order to compute the $ARG2",
        "$ARG1 available in the $ARG2",
        "$ARG1 alone as we make a $ARG2"
    ],
    "back-propagation*****learning": [
        "$ARG1 is often misunderstood as meaning the whole $ARG2",
        "$ARG1 , and the $ARG2"
    ],
    "algorithm*****neural network": [
        "$ARG1 for multi-layer $ARG2",
        "$ARG1 6.4 Backward computation for the deep $ARG2",
        "$ARG1 to training $ARG2"
    ],
    "back-propagation*****method": [
        "$ARG1 refers only to the $ARG2"
    ],
    "gradient*****algorithm": [
        "$ARG1 , while another $ARG2",
        "$ARG1 computation directly ( $ARG2",
        "$ARG1 estimate : g\u0302 \u2190 + m1 \u2207 \u03b8 i L ( f ( x ( i ) ; \u03b8 ) , y ( i ) ) Apply update : \u03b8 \u2190 \u03b8 \u2212 \ue00fg\u0302 A crucial parameter for the SGD $ARG2",
        "$ARG1 , but the Newtonian scenario used by the momentum $ARG2"
    ],
    "algorithm*****stochastic gradient descent": [
        "$ARG1 , such as $ARG2",
        "$ARG1 called $ARG2",
        "$ARG1 8.1 $ARG2",
        "$ARG1 8.2 $ARG2",
        "$ARG1 8.3 $ARG2",
        "$ARG1 : $ARG2",
        "$ARG1 that makes small steps , such as $ARG2"
    ],
    "stochastic gradient descent*****learning": [
        "$ARG1 , is used to perform $ARG2"
    ],
    "learning*****gradient": [
        "$ARG1 using this $ARG2",
        "$ARG1 algorithms , the $ARG2",
        "$ARG1 becomes very slow despite the presence of a strong $ARG2",
        "$ARG1 the $ARG2",
        "$ARG1 , one must be able to compute the $ARG2"
    ],
    "neural network*****principle": [
        "$ARG1 , but in $ARG2"
    ],
    "principle*****function": [
        "$ARG1 it can compute derivatives of any $ARG2"
    ],
    "derivative*****function": [
        "$ARG1 of the $ARG2",
        "$ARG1 of the functional J with respect to the value of the $ARG2",
        "$ARG1 of another $ARG2",
        "$ARG1 of the reconstruction $ARG2",
        "$ARG1 of this $ARG2",
        "$ARG1 is therefore useful for minimizing a $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "machine learning*****other": [
        "$ARG1 tasks involve computing $ARG2"
    ],
    "neural network*****graph": [
        "$ARG1 with a relatively informal $ARG2"
    ],
    "algorithm*****computational graph": [
        "$ARG1 more precisely , it is helpful to have a more precise $ARG2",
        "$ARG1 as a symbolic speci\ufb01cation of the $ARG2"
    ],
    "graph*****variable": [
        "$ARG1 to indicate a $ARG2",
        "$ARG1 G corresponds to a $ARG2",
        "$ARG1 if and only if the $ARG2",
        "$ARG1 , nor can a $ARG2",
        "$ARG1 over the hidden units , so $ARG2"
    ],
    "variable*****scalar": [
        "$ARG1 may be a $ARG2"
    ],
    "scalar*****vector": [
        "$ARG1 , $ARG2",
        "$ARG1 y \u2208 R. The prediction is parametrized by the $ARG2",
        "$ARG1 value d\ue03e x ( i ) on the right of the $ARG2",
        "$ARG1 coe\ufb03cients on the left of $ARG2",
        "$ARG1 from the input $ARG2"
    ],
    "tensor*****variable": [
        "$ARG1 , or even a $ARG2"
    ],
    "operation*****function": [
        "$ARG1 is a simple $ARG2",
        "$ARG1 f ( i ) and is computed by evaluating the $ARG2",
        "$ARG1 , and the get_operation $ARG2",
        "$ARG1 , op.f is the mathematical $ARG2",
        "$ARG1 to be a $ARG2",
        "$ARG1 at every moment , we obtain a new $ARG2"
    ],
    "graph*****set": [
        "$ARG1 language is accompanied by a $ARG2",
        "$ARG1 , then return a $ARG2"
    ],
    "set*****loss": [
        "$ARG1 may be described by composing many operations Without $ARG2",
        "$ARG1 0-1 $ARG2",
        "$ARG1 0-1 $ARG2"
    ],
    "loss*****operation": [
        "$ARG1 of generality , we de\ufb01ne an $ARG2"
    ],
    "operation*****variable": [
        "$ARG1 to return only a single output $ARG2",
        "$ARG1 to a $ARG2",
        "$ARG1 to each $ARG2",
        "$ARG1 to create a $ARG2",
        "$ARG1 may be highly $ARG2"
    ],
    "variable*****multiple": [
        "$ARG1 can have $ARG2"
    ],
    "multiple*****vector": [
        "$ARG1 entries , such as a $ARG2"
    ],
    "back-propagation*****support": [
        "$ARG1 usually $ARG2"
    ],
    "support*****multiple": [
        "$ARG1 operations with $ARG2",
        "$ARG1 for $ARG2"
    ],
    "multiple*****variable": [
        "$ARG1 outputs , but we avoid this case in our description because it introduces many extra details that are not important to If a $ARG2"
    ],
    "operation*****other": [
        "$ARG1 applied , and $ARG2"
    ],
    "other*****operation": [
        "$ARG1 times omit this label when the $ARG2"
    ],
    "rule*****chain rule of probability": [
        "$ARG1 of calculus ( not to be confused with the $ARG2"
    ],
    "chain rule of probability*****other": [
        "$ARG1 ) is used to compute the derivatives of functions formed by composing $ARG2"
    ],
    "back-propagation*****algorithm": [
        "$ARG1 is an $ARG2",
        "$ARG1 as a table-\ufb01lling $ARG2"
    ],
    "algorithm*****rule": [
        "$ARG1 that computes the chain $ARG2",
        "$ARG1 6.1 for the associated forward computation ) , in the order it will actually be done and according to the recursive application of chain $ARG2"
    ],
    "number*****mapping": [
        "$ARG1 , and let f and g both be functions $ARG2",
        "$ARG1 of relatively complex layers , with each layer having many \u201c stages. \u201d In this terminology , there is a one-to-one $ARG2"
    ],
    "mapping*****number": [
        "$ARG1 from a real $ARG2"
    ],
    "rule*****scalar": [
        "$ARG1 states that We can generalize this beyond the $ARG2",
        "$ARG1 with respect to $ARG2"
    ],
    "1*****computational graph": [
        "$ARG1 ) Figure 6.8 : Examples of $ARG2",
        "$ARG1 ) U ( 3 ) u ( 4 ) Figure 6.11 : The $ARG2",
        "$ARG1 ) x ( 2 ) x ( 3 ) x ( 4 ) Figure 10.14 : A recursive network has a $ARG2"
    ],
    "graph*****logistic regression": [
        "$ARG1 for the $ARG2"
    ],
    "logistic regression*****\u03c3": [
        "$ARG1 prediction y\u0302 = $ARG2"
    ],
    "computational graph*****design matrix": [
        "$ARG1 for the expression H = max { 0 , XW + b } , which computes a $ARG2"
    ],
    "design matrix*****recti\ufb01ed linear unit": [
        "$ARG1 of $ARG2"
    ],
    "recti\ufb01ed linear unit*****design matrix": [
        "$ARG1 activations H given a $ARG2"
    ],
    "design matrix*****minibatch": [
        "$ARG1 containing a $ARG2"
    ],
    "operation*****weights": [
        "$ARG1 to the $ARG2"
    ],
    "weights*****linear regression": [
        "$ARG1 w of a $ARG2",
        "$ARG1 in our $ARG2"
    ],
    "weights*****weight decay": [
        "$ARG1 are used to make both the prediction y\u0302 and the $ARG2",
        "$ARG1 w. If this prior is given by N ( w ; 0 , 1\u03bb I 2 ) , then the log-prior term in equation 5.79 is proportional to the familiar \u03bbw \ue03ew $ARG2"
    ],
    "vector*****jacobian matrix": [
        "$ARG1 notation , this may be equivalently written as is the n \u00d7 m $ARG2"
    ],
    "gradient*****variable": [
        "$ARG1 of a $ARG2",
        "$ARG1 accumulation $ARG2",
        "$ARG1 ascent on latent $ARG2"
    ],
    "algorithm*****jacobian matrix": [
        "$ARG1 consists a $ARG2"
    ],
    "jacobian matrix*****gradient": [
        "$ARG1 \u2202y \u2202x by a $ARG2"
    ],
    "tensor*****vector": [
        "$ARG1 into a $ARG2",
        "$ARG1 X , we write \u2207 Xz , just as if X were a $ARG2",
        "$ARG1 into a \ufb02at $ARG2"
    ],
    "vector*****back-propagation": [
        "$ARG1 before we run $ARG2",
        "$ARG1 g , then after one step of $ARG2"
    ],
    "gradient*****tensor": [
        "$ARG1 back into a $ARG2",
        "$ARG1 of a value z with respect to a $ARG2",
        "$ARG1 with respect to R as a $ARG2"
    ],
    "view*****back-propagation": [
        "$ARG1 , $ARG2"
    ],
    "multiple*****example": [
        "$ARG1 coordinates\u2014for $ARG2",
        "$ARG1 models on each test $ARG2"
    ],
    "example*****tensor": [
        "$ARG1 , a 3-D $ARG2",
        "$ARG1 , if we wish to compute both the maximum value in a $ARG2"
    ],
    "rule*****backprop": [
        "$ARG1 to Obtain $ARG2"
    ],
    "backprop*****rule": [
        "$ARG1 Using the chain $ARG2"
    ],
    "rule*****gradient": [
        "$ARG1 , it is straightforward to write down an algebraic expression for the $ARG2",
        "$ARG1 to zero out components of the $ARG2"
    ],
    "gradient*****scalar": [
        "$ARG1 of a $ARG2",
        "$ARG1 of some $ARG2",
        "$ARG1 of a $ARG2",
        "$ARG1 of a $ARG2"
    ],
    "scalar*****computational graph": [
        "$ARG1 with respect to any node in the $ARG2"
    ],
    "computational graph*****scalar": [
        "$ARG1 that produced that $ARG2",
        "$ARG1 describing how to compute a single $ARG2"
    ],
    "other*****memory": [
        "$ARG1 cases , computing the same subexpression twice could be a valid way to reduce $ARG2"
    ],
    "computational graph*****back-propagation": [
        "$ARG1 for computing the $ARG2",
        "$ARG1 with a symbolic description of the Some approaches to $ARG2",
        "$ARG1 , it is possible to run $ARG2",
        "$ARG1 constructed by $ARG2"
    ],
    "graph*****gradient": [
        "$ARG1 that performs the $ARG2",
        "$ARG1 , we begin by observing that the $ARG2",
        "$ARG1 by multiplying the current $ARG2",
        "$ARG1 has a corresponding slot in a \u2202u ( i ) table to store the $ARG2",
        "$ARG1 , the naive implementation computes k gradients instead of a single $ARG2"
    ],
    "scalar*****loss": [
        "$ARG1 ( say the $ARG2"
    ],
    "loss*****example": [
        "$ARG1 on a training $ARG2",
        "$ARG1 L ( y\u0302 , y ) associated with a single ( input , target ) training $ARG2",
        "$ARG1 on a particular $ARG2",
        "$ARG1 L = \u2212 log pdecoder ( x | h = f ( x\u0303 ) ) , where x\u0303 is a corrupted version of the data $ARG2"
    ],
    "scalar*****gradient": [
        "$ARG1 is the quantity whose $ARG2"
    ],
    "gradient*****1": [
        "$ARG1 we want to obtain , with respect to the n i input nodes u ( $ARG2",
        "$ARG1 with respect to z is given by dz dz = $ARG2",
        "$ARG1 corresponding to entries of U ( $ARG2",
        "$ARG1 on W ( $ARG2",
        "$ARG1 is thus given by \u2202o ( t ) \u2207h ( t ) L = ( \u2207h ( t+1 ) L ) + ( \u2207 o ( t ) L ) \u2202h ( t ) = W ( \u2207h ( t+1 ) L ) diag $ARG2",
        "$ARG1 on the remaining parameters is given by : \u2207o ( t ) L = \u2207 o ( t ) L \u2202h ( t ) \u2207h ( t ) L = \u2207h ( t ) L ( 10.23 ) diag $ARG2",
        "$ARG1 : r \u2190 \u03c1r + ( $ARG2",
        "$ARG1 : r \u2190 \u03c1r + ( $ARG2",
        "$ARG1 : g \u2190 m1 \u2207\u03b8 i L ( f ( x ( i ) ; \u03b8 ) , y ( i ) ) Update biased \ufb01rst moment estimate : s \u2190 \u03c11 s + ( $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 is zero for the i-th term if the score of the observed word , a y , is greater than the score of the negative word ai by a margin of $ARG2",
        "$ARG1 is $ARG2"
    ],
    "back-propagation*****gradient descent": [
        "$ARG1 to computing gradients for $ARG2"
    ],
    "gradient descent*****example": [
        "$ARG1 over parameters , u ( n ) will be the cost associated with an $ARG2"
    ],
    "algorithm*****operation": [
        "$ARG1 6.1 , each node u ( i ) is associated with an $ARG2",
        "$ARG1 e\ufb03cient : we can optimally encode x just using a matrix-vector $ARG2"
    ],
    "algorithm*****forward propagation": [
        "$ARG1 speci\ufb01es the $ARG2",
        "$ARG1 6.3 \ufb01rst shows the $ARG2",
        "$ARG1 6.3 $ARG2",
        "$ARG1 by looking at the $ARG2"
    ],
    "forward propagation*****graph": [
        "$ARG1 computation , which we could put in a $ARG2",
        "$ARG1 stage will at worst execute all n nodes in the original $ARG2"
    ],
    "back-propagation*****computational graph": [
        "$ARG1 , we can construct a $ARG2",
        "$ARG1 take a $ARG2"
    ],
    "computational graph*****set": [
        "$ARG1 that depends on G and adds to it an extra $ARG2",
        "$ARG1 and a $ARG2",
        "$ARG1 is a way to formalize the structure of a $ARG2"
    ],
    "mapping*****1": [
        "$ARG1 n i inputs u ( $ARG2"
    ],
    "computational graph*****function": [
        "$ARG1 where each node computes numerical value u ( i ) by applying a $ARG2"
    ],
    "computational graph*****vector": [
        "$ARG1 is the $ARG2"
    ],
    "vector*****set": [
        "$ARG1 x , and is $ARG2",
        "$ARG1 c , then we can $ARG2",
        "$ARG1 lies in the $ARG2",
        "$ARG1 in the $ARG2",
        "$ARG1 to a $ARG2",
        "$ARG1 does not add any points to the $ARG2"
    ],
    "scalar*****algorithm": [
        "$ARG1 output u ( n ) : \u2202u ( n ) \u2202u ( j ) i : j \u2208P a ( u ( i ) ) \u2202u ( n ) \u2202u ( i ) \u2202u ( i ) \u2202u ( j ) as speci\ufb01ed by $ARG2"
    ],
    "dot product*****gradient": [
        "$ARG1 is performed for each node , between the $ARG2"
    ],
    "gradient*****vector": [
        "$ARG1 already computed with respect to nodes u ( i ) that are children of u ( j ) and the $ARG2",
        "$ARG1 can be written as follows : \u2202 log P ( y | C ) \u2202 log softmaxy ( a ) ( a y \u2212 log P ( y = i | C ) where a is the $ARG2",
        "$ARG1 of f is the $ARG2"
    ],
    "vector*****partial derivative": [
        "$ARG1 containing the $ARG2",
        "$ARG1 containing all of the $ARG2"
    ],
    "back-propagation*****number": [
        "$ARG1 scales linearly with the $ARG2",
        "$ARG1 guarantees that the $ARG2"
    ],
    "number*****partial derivative": [
        "$ARG1 of edges in G , where the computation for each edge corresponds to computing a $ARG2"
    ],
    "partial derivative*****multiplication": [
        "$ARG1 ( of one node with respect to one of its parents ) as well as performing one $ARG2",
        "$ARG1 \u2202u \u2202u ( j ) be computed only once along with an associated $ARG2"
    ],
    "scalar*****back-propagation": [
        "$ARG1 values in the same node and enable more The $ARG2",
        "$ARG1 output but $ARG2"
    ],
    "algorithm*****number": [
        "$ARG1 is designed to reduce the $ARG2",
        "$ARG1 is proportional to the $ARG2",
        "$ARG1 that increases the $ARG2",
        "$ARG1 , which is allowed to determine the $ARG2",
        "$ARG1 terminates when no parameters have improved over the best recorded validation error for some pre-speci\ufb01ed $ARG2"
    ],
    "number*****memory": [
        "$ARG1 of common subexpressions without regard to $ARG2",
        "$ARG1 of parameters that must be stored in $ARG2",
        "$ARG1 of models with a tractable amount of $ARG2"
    ],
    "backprop*****algorithm": [
        "$ARG1 ( $ARG2"
    ],
    "algorithm*****graph": [
        "$ARG1 6.2 ) visits each edge from node u ( j ) to node u ( i ) of \u2202u ( i ) the $ARG2",
        "$ARG1 for computing the derivatives of u ( n ) with respect to the variables in the $ARG2",
        "$ARG1 to this $ARG2",
        "$ARG1 , instructing it to construct the $ARG2",
        "$ARG1 applied to the unrolled $ARG2"
    ],
    "graph*****partial derivative": [
        "$ARG1 exactly once in order to obtain the associated $ARG2",
        "$ARG1 , assuming that the $ARG2",
        "$ARG1 G has a single output node and each $ARG2"
    ],
    "number*****graph": [
        "$ARG1 of edges in the $ARG2",
        "$ARG1 of outputs of the $ARG2",
        "$ARG1 of time steps \u03c4 , the $ARG2"
    ],
    "partial derivative*****constant": [
        "$ARG1 associated with each edge requires a $ARG2",
        "$ARG1 can be computed with a $ARG2"
    ],
    "number*****forward propagation": [
        "$ARG1 of computations for the $ARG2",
        "$ARG1 of such paths , can grow exponentially with the depth of the $ARG2"
    ],
    "function*****graph": [
        "$ARG1 of the parents u ( j ) of u ( i ) , thus \u2202u ( j ) linking the nodes of the forward $ARG2"
    ],
    "back-propagation*****forward propagation": [
        "$ARG1 Run $ARG2",
        "$ARG1 to retain unbounded dynamics even when $ARG2"
    ],
    "forward propagation*****algorithm": [
        "$ARG1 ( $ARG2"
    ],
    "example*****data structure": [
        "$ARG1 ) to obtain the activations of the network Initialize grad_table , a $ARG2"
    ],
    "other*****computational graph": [
        "$ARG1 algorithms may be able to avoid more subexpressions by performing simpli\ufb01cations on the $ARG2",
        "$ARG1 way to draw the RNN is as an unfolded $ARG2"
    ],
    "computational graph*****memory": [
        "$ARG1 , or may be able to conserve $ARG2"
    ],
    "back-propagation*****graph": [
        "$ARG1 computation , let us consider the speci\ufb01c $ARG2"
    ],
    "forward propagation*****loss": [
        "$ARG1 , which maps parameters to the supervised $ARG2"
    ],
    "deep feedforward network*****computational graph": [
        "$ARG1 Figure 6.9 : A $ARG2",
        "$ARG1 terms of constructing a $ARG2"
    ],
    "computational graph*****gradient": [
        "$ARG1 that results in repeated subexpressions when computing the $ARG2",
        "$ARG1 for the $ARG2",
        "$ARG1 to the $ARG2"
    ],
    "memory*****back-propagation": [
        "$ARG1 required to store the value of these expressions is low , the $ARG2",
        "$ARG1 consumption and increase statistical e\ufb03ciency , and also reduces the amount of computation needed to perform forward and $ARG2",
        "$ARG1 to store these binary numbers until the $ARG2"
    ],
    "rule*****memory": [
        "$ARG1 , and is useful when $ARG2"
    ],
    "deep feedforward network*****back-propagation": [
        "$ARG1 applying the $ARG2"
    ],
    "computational graph*****data structure": [
        "$ARG1 by explicitly manipulating a $ARG2",
        "$ARG1 are usually represented by explicit $ARG2"
    ],
    "data structure*****algorithm": [
        "$ARG1 for representing symbolic $ARG2"
    ],
    "forward propagation*****neural network": [
        "$ARG1 through a typical deep $ARG2"
    ],
    "loss*****loss function": [
        "$ARG1 L ( y\u0302 , y ) depends on the output y\u0302 and on the target y ( see section 6.2.1.1 for examples of $ARG2"
    ],
    "loss*****contains": [
        "$ARG1 may be added to a regularizer \u2126 ( \u03b8 ) , where \u03b8 $ARG2"
    ],
    "contains*****weights": [
        "$ARG1 all the parameters ( $ARG2"
    ],
    "bias parameter*****model": [
        "$ARG1 of the $ARG2",
        "$ARG1 , so \u03b8 is just w. Such a $ARG2"
    ],
    "neural network*****algorithm": [
        "$ARG1 of $ARG2",
        "$ARG1 have been proposed earlier , such as the scaled conjugate gradients $ARG2"
    ],
    "change*****gradient": [
        "$ARG1 to reduce error , one can obtain the $ARG2",
        "$ARG1 of h ( t ) propagates one step forward , or equivalently , how the $ARG2",
        "$ARG1 the direction of the $ARG2"
    ],
    "weights*****stochastic": [
        "$ARG1 and biases can be immediately used as part of a $ARG2",
        "$ARG1 is a practical , $ARG2"
    ],
    "other*****optimization": [
        "$ARG1 gradient-based $ARG2",
        "$ARG1 related architectures emulating algorithmic mechanisms in a way that still allows gradient-based $ARG2",
        "$ARG1 \ufb01elds , the dominant approach to $ARG2"
    ],
    "output layer*****1": [
        "$ARG1 : g \u2190 \u2207y\u0302J = \u2207y\u0302 L ( y\u0302 , y ) for k = l , l \u2212 $ARG2",
        "$ARG1 performs the following ai = b i + W ij hj \u2200i \u2208 { $ARG2"
    ],
    "1*****gradient": [
        "$ARG1 do Convert the $ARG2",
        "$ARG1 \u2212 h We do not need to compute the $ARG2",
        "$ARG1 while stopping criterion not met do Initialize the $ARG2",
        "$ARG1 , then we can make a step of size \ue00f along the negative $ARG2"
    ],
    "multiplication*****weights": [
        "$ARG1 if f is element-wise ) : g \u2190 \u2207 a ( k ) J = g \ue00c f \ue030 ( a ( k ) ) Compute gradients on $ARG2",
        "$ARG1 of several large $ARG2"
    ],
    "weights*****regularization": [
        "$ARG1 and biases ( including the $ARG2",
        "$ARG1 closer to the origin1 by adding a $ARG2",
        "$ARG1 can also be interpreted as equivalent ( under some assumptions ) to a more traditional form of $ARG2"
    ],
    "set*****graph": [
        "$ARG1 of numerical values for the inputs to the $ARG2",
        "$ARG1 of variables S if the $ARG2",
        "$ARG1 of variables S if the $ARG2"
    ],
    "set*****gradient": [
        "$ARG1 of numerical values describing the $ARG2",
        "$ARG1 are called batch or deterministic $ARG2",
        "$ARG1 of m examples while stopping criterion not met\ue050do Compute $ARG2",
        "$ARG1 size grows to billions of examples , the time to take a single $ARG2"
    ],
    "subset*****graph": [
        "$ARG1 of the $ARG2",
        "$ARG1 of some clique in the $ARG2"
    ],
    "scalar*****graph": [
        "$ARG1 z with respect to one of its ancestors x in the $ARG2",
        "$ARG1 nodes in the $ARG2",
        "$ARG1 internal node in the original forward $ARG2"
    ],
    "gradient*****graph": [
        "$ARG1 with respect to each parent of z in the $ARG2",
        "$ARG1 in a $ARG2",
        "$ARG1 this way , and will instead explicitly propagate gradients through all of the logarithm and exponentiation operations in the original $ARG2",
        "$ARG1 computed at nodes that follow it in the $ARG2"
    ],
    "variable*****tensor": [
        "$ARG1 as being a $ARG2"
    ],
    "tensor*****number": [
        "$ARG1 can in general have any $ARG2"
    ],
    "operation*****computational graph": [
        "$ARG1 that computes V , represented by the edges coming into V in the $ARG2",
        "$ARG1 as the fundamental unit of our $ARG2",
        "$ARG1 that maps a circuit as in the left side of the \ufb01gure to a $ARG2"
    ],
    "example*****python": [
        "$ARG1 , there may be a $ARG2"
    ],
    "python*****matrix multiplication": [
        "$ARG1 or C++ class representing the $ARG2"
    ],
    "variable*****matrix multiplication": [
        "$ARG1 that is created by $ARG2"
    ],
    "list*****computational graph": [
        "$ARG1 of variables that are children of V in the $ARG2",
        "$ARG1 of variables that are parents of V in the $ARG2"
    ],
    "deep feedforward network*****operation": [
        "$ARG1 Each $ARG2"
    ],
    "example*****matrix multiplication": [
        "$ARG1 , we might use a $ARG2",
        "$ARG1 , $ARG2"
    ],
    "scalar*****matrix multiplication": [
        "$ARG1 z with respect to C is given by G. The $ARG2"
    ],
    "operation*****back-propagation": [
        "$ARG1 is responsible for de\ufb01ning two $ARG2"
    ],
    "gradient*****method": [
        "$ARG1 on the output is G , then the bprop $ARG2",
        "$ARG1 , but it is a very practical $ARG2",
        "$ARG1 by H \u22121 , Newton \u2019 s $ARG2"
    ],
    "method*****matrix multiplication": [
        "$ARG1 of the $ARG2"
    ],
    "operation*****gradient": [
        "$ARG1 must state that the $ARG2",
        "$ARG1 implements , X is the input whose $ARG2",
        "$ARG1 to add X \ue03eG \ue030 to the $ARG2",
        "$ARG1 , this input $ARG2"
    ],
    "gradient*****matrix operation": [
        "$ARG1 with respect to B , then the $ARG2"
    ],
    "matrix operation*****method": [
        "$ARG1 is responsible for implementing the bprop $ARG2"
    ],
    "example*****method": [
        "$ARG1 , if the mul operator is passed two copies of x to compute x2 , the op.bprop $ARG2",
        "$ARG1 , Newton \u2019 s $ARG2",
        "$ARG1 , near a saddle point , then Newton \u2019 s $ARG2",
        "$ARG1 of such a $ARG2"
    ],
    "method*****derivative": [
        "$ARG1 should still return x as the $ARG2"
    ],
    "algorithm*****total derivative": [
        "$ARG1 will later add both of these arguments together to obtain 2x , which is the correct $ARG2"
    ],
    "back-propagation*****deep learning": [
        "$ARG1 usually provide both the operations and their bprop methods , so that users of $ARG2"
    ],
    "deep learning*****matrix multiplication": [
        "$ARG1 software libraries are able to back-propagate through graphs built using common operations like $ARG2"
    ],
    "back-propagation*****operation": [
        "$ARG1 or advanced users who need to add their own $ARG2",
        "$ARG1 rules to be developed for each $ARG2"
    ],
    "operation*****method": [
        "$ARG1 to an existing library must usually derive the op.bprop $ARG2",
        "$ARG1 to a di\ufb00erentiation library and must de\ufb01ne its bprop $ARG2"
    ],
    "computational graph*****variable": [
        "$ARG1 Require : z , the $ARG2",
        "$ARG1 , in which each component is represented by many di\ufb00erent variables , with one $ARG2"
    ],
    "data structure*****1": [
        "$ARG1 associating tensors to their gradients grad_table [ z ] \u2190 $ARG2"
    ],
    "1*****back-propagation": [
        "$ARG1 for V in T do build_grad ( V , G , G \ue030 , grad_table ) Return grad_table restricted to T In section 6.5.2 , we explained that $ARG2"
    ],
    "back-propagation*****rule": [
        "$ARG1 was developed in order to avoid computing the same subexpression in the chain $ARG2"
    ],
    "operation*****number": [
        "$ARG1 evaluation has roughly the same cost , then we may analyze the computational cost in terms of the $ARG2",
        "$ARG1 with variably-sized pools but a \ufb01xed $ARG2",
        "$ARG1 has the same $ARG2"
    ],
    "computational graph*****example": [
        "$ARG1 , which might actually consist of very many arithmetic operations ( for $ARG2",
        "$ARG1 used to compute the cost used to train our $ARG2"
    ],
    "graph*****matrix multiplication": [
        "$ARG1 that treats $ARG2"
    ],
    "matrix multiplication*****operation": [
        "$ARG1 as a single $ARG2"
    ],
    "computational graph*****operation": [
        "$ARG1 , not individual operations executed by the underlying hardware , so it is important to remember that the runtime of each $ARG2"
    ],
    "gradient*****forward propagation": [
        "$ARG1 requires as most O ( n 2 ) operations because the $ARG2",
        "$ARG1 computation involves performing a $ARG2",
        "$ARG1 step requires a complete run of $ARG2"
    ],
    "algorithm*****1": [
        "$ARG1 adds one Jacobian-vector product , which should be expressed with O ( $ARG2",
        "$ARG1 Require : Step size \ue00f ( Suggested default : 0.001 ) Require : Exponential decay rates for moment estimates , \u03c11 and \u03c1 2 in [ 0 , $ARG2",
        "$ARG1 will perform are its ability to : $ARG2",
        "$ARG1 works by initializing k di\ufb00erent centroids { \u00b5 ( $ARG2",
        "$ARG1 for \ufb01nding D\u2217 , we will start by considering the case where l = $ARG2"
    ],
    "1*****graph": [
        "$ARG1 ) nodes , per edge in the original $ARG2",
        "$ARG1 requires an edge connecting b and c. The $ARG2"
    ],
    "algorithm*****loop": [
        "$ARG1 6.6 The inner $ARG2"
    ],
    "loop*****back-propagation": [
        "$ARG1 subroutine build_grad ( V , G , G\ue030 , grad_table ) of the $ARG2"
    ],
    "mapping*****back-propagation": [
        "$ARG1 nodes to their gradients if V is in grad_table then Return grad_table [ V ] for C in get_consumers ( V , G \ue030 ) do op \u2190 get_operation ( C ) D \u2190 build_grad ( C , G , G \ue030 , grad_table ) G ( i ) \u2190 op.bprop ( get_inputs ( C , G \ue030 ) , V , D ) G \u2190 i G ( i ) grad_table [ V ] = G Insert G and the operations creating it into G roughly chain-structured , causing $ARG2"
    ],
    "rule*****path": [
        "$ARG1 ( equation 6.49 ) non-recursively : \u2202u ( n ) \u2202u ( j ) $ARG2"
    ],
    "path*****number": [
        "$ARG1 ( u ( \u03c01 ) , u ( \u03c02 ) , ... , u ( \u03c0 t ) ) , from \u03c0 1=j to \u03c0 t=n \u2202u ( \u03c0k ) Since the $ARG2"
    ],
    "back-propagation*****example": [
        "$ARG1 for MLP Training As an $ARG2"
    ],
    "algorithm*****multilayer perceptron": [
        "$ARG1 as it is used to train a $ARG2"
    ],
    "multilayer perception*****hidden layer": [
        "$ARG1 with a single $ARG2"
    ],
    "model*****minibatch": [
        "$ARG1 , we will use $ARG2"
    ],
    "gradient*****minibatch": [
        "$ARG1 of the cost on a single $ARG2",
        "$ARG1 from a $ARG2",
        "$ARG1 for an individual $ARG2",
        "$ARG1 or the $ARG2"
    ],
    "set*****design matrix": [
        "$ARG1 formatted as a $ARG2"
    ],
    "design matrix*****vector": [
        "$ARG1 X and a $ARG2",
        "$ARG1 of inputs as X ( test ) and the $ARG2"
    ],
    "operation*****cross-entropy": [
        "$ARG1 that computes the $ARG2"
    ],
    "cross-entropy*****probability distribution": [
        "$ARG1 between the targets y and the $ARG2"
    ],
    "example*****regularization": [
        "$ARG1 more realistic , we also include a $ARG2",
        "$ARG1 , to include \u03b8 or x as arguments , or to exclude y as arguments , in order to develop various forms of $ARG2",
        "$ARG1 , where \u03bbi \ue01d \u03b1 , the e\ufb00ect of $ARG2",
        "$ARG1 , L2 $ARG2"
    ],
    "1*****cross-entropy": [
        "$ARG1 ) \ue011 2 \ue058 \ue010 ( 2 ) \ue0112 J = JMLE + \u03bb \uf8ed W i , j Wi , j \uf8f8 consists of the $ARG2"
    ],
    "cross-entropy*****weight decay": [
        "$ARG1 and a $ARG2",
        "$ARG1 cost , and one through the $ARG2"
    ],
    "gradient*****example": [
        "$ARG1 of this $ARG2",
        "$ARG1 is NP-complete ( Naumann , 2008 ) , in the sense that it may require simplifying algebraic expressions into their least For $ARG2",
        "$ARG1 : optionally reset \u03b2t to zero , for $ARG2"
    ],
    "algorithm*****trace": [
        "$ARG1 , which is that it can automatically generate gradients that would be straightforward but tedious for a software engineer to We can roughly $ARG2"
    ],
    "trace*****back-propagation": [
        "$ARG1 out the behavior of the $ARG2"
    ],
    "weights*****cross-entropy": [
        "$ARG1 : one through the $ARG2"
    ],
    "weight decay*****gradient": [
        "$ARG1 cost is relatively simple ; it will always contribute 2\u03bbW ( i ) to the $ARG2"
    ],
    "deep feedforward network*****8": [
        "$ARG1 U ( 2 ) u ( $ARG2"
    ],
    "8*****1": [
        "$ARG1 ) W ( 2 ) U ( 5 ) u ( 6 ) u ( 7 ) U ( $ARG2"
    ],
    "example*****cross-entropy": [
        "$ARG1 of a single-layer MLP using the $ARG2"
    ],
    "loss*****weight decay": [
        "$ARG1 and $ARG2"
    ],
    "path*****cross-entropy": [
        "$ARG1 through the $ARG2"
    ],
    "rule*****matrix multiplication": [
        "$ARG1 for the second argument to the $ARG2",
        "$ARG1 for the \ufb01rst argument to the $ARG2"
    ],
    "operation*****rule": [
        "$ARG1 uses its backpropagation $ARG2"
    ],
    "rule*****operation": [
        "$ARG1 for the second argument of the matmul $ARG2"
    ],
    "algorithm*****optimization algorithm": [
        "$ARG1 , or another $ARG2",
        "$ARG1 by combining models , costs , and $ARG2"
    ],
    "matrix*****number": [
        "$ARG1 , resulting in O ( w ) multiply-adds , where w is the $ARG2"
    ],
    "transpose*****matrix": [
        "$ARG1 of each weight $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of a $ARG2",
        "$ARG1 of a $ARG2",
        "$ARG1 of the resulting $ARG2",
        "$ARG1 of the weight $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the weight $ARG2"
    ],
    "memory*****algorithm": [
        "$ARG1 cost of the $ARG2",
        "$ARG1 costs of the BFGS $ARG2",
        "$ARG1 cost of running the $ARG2"
    ],
    "memory*****number": [
        "$ARG1 cost is thus O ( mnh ) , where m is the $ARG2",
        "$ARG1 scales exponentially with the $ARG2"
    ],
    "minibatch*****number": [
        "$ARG1 and nh is the $ARG2",
        "$ARG1 size m \ue030 is typically chosen to be a relatively small $ARG2"
    ],
    "function*****tensor": [
        "$ARG1 that returns a single $ARG2"
    ],
    "support*****tensor": [
        "$ARG1 operations that can return more than one $ARG2"
    ],
    "tensor*****pass": [
        "$ARG1 and the index of that value , it is best to compute both in a single $ARG2"
    ],
    "pass*****memory": [
        "$ARG1 through $ARG2",
        "$ARG1 , so the $ARG2"
    ],
    "memory*****operation": [
        "$ARG1 , so it is most e\ufb03cient to implement this procedure as a single $ARG2",
        "$ARG1 cost of essentially every $ARG2"
    ],
    "track*****gradient": [
        "$ARG1 these cases and determine whether the $ARG2"
    ],
    "deep learning*****computer science": [
        "$ARG1 community has been somewhat isolated from the broader $ARG2"
    ],
    "other*****rule": [
        "$ARG1 approaches evaluate the subexpressions of the chain $ARG2"
    ],
    "softmax function*****cross-entropy": [
        "$ARG1 out of exponentiation , \ue050 operations , and construct a $ARG2"
    ],
    "human*****derivative": [
        "$ARG1 mathematician can observe that the $ARG2"
    ],
    "constant*****back-propagation": [
        "$ARG1 amount of computation , $ARG2"
    ],
    "number*****gradient": [
        "$ARG1 of computations for the $ARG2",
        "$ARG1 of updates ) if they are allowed to rapidly compute approximate estimates of the $ARG2"
    ],
    "gradient*****number": [
        "$ARG1 computation is of the same order as the $ARG2",
        "$ARG1 from a small $ARG2",
        "$ARG1 is bounded , restricting both the $ARG2"
    ],
    "number*****algorithm": [
        "$ARG1 of computations for the forward computation : this can be seen in $ARG2"
    ],
    "algorithm*****partial derivative": [
        "$ARG1 6.2 because each local $ARG2"
    ],
    "back-propagation*****task": [
        "$ARG1 , and this is an NP-complete $ARG2"
    ],
    "tensorflow*****matching": [
        "$ARG1 use heuristics based on $ARG2"
    ],
    "matching*****graph": [
        "$ARG1 known simpli\ufb01cation patterns in order to iteratively attempt to simplify the $ARG2"
    ],
    "back-propagation*****scalar": [
        "$ARG1 can be extended to compute a Jacobian ( either of k di\ufb00erent $ARG2"
    ],
    "deep feedforward network*****scalar": [
        "$ARG1 each $ARG2"
    ],
    "graph*****number": [
        "$ARG1 is larger than the $ARG2",
        "$ARG1 ( as in equation 10.6 ) might be very ine\ufb03cient , with an ever growing $ARG2",
        "$ARG1 requires adding moralizing edges between every pair of hidden units , thus introducing a quadratic $ARG2",
        "$ARG1 structure allows us to represent complicated , high-dimensional distributions with a reasonable $ARG2"
    ],
    "recurrent network*****example": [
        "$ARG1 , for $ARG2",
        "$ARG1 with recurrent connections between hidden units , that read an entire sequence and then produce a single output , illustrated in \ufb01gure 10.3 is a reasonably representative $ARG2"
    ],
    "graph*****memory": [
        "$ARG1 , trading o\ufb00 computational e\ufb03ciency for $ARG2"
    ],
    "vector*****graph": [
        "$ARG1 while A has many rows , this corresponds to a $ARG2"
    ],
    "machine learning*****python": [
        "$ARG1 , it is more common to implement di\ufb00erentiation software that acts directly on traditional programming language code , such as $ARG2"
    ],
    "deep learning*****computational graph": [
        "$ARG1 community , $ARG2",
        "$ARG1 have no latent variables or only one layer of latent variables , but use deep $ARG2"
    ],
    "method*****deep learning": [
        "$ARG1 that continues to serve the $ARG2"
    ],
    "deep feedforward network*****support": [
        "$ARG1 Some software frameworks $ARG2"
    ],
    "support*****higher-order derivative": [
        "$ARG1 the use of $ARG2"
    ],
    "deep learning*****tensorflow": [
        "$ARG1 software frameworks , this includes at least Theano and $ARG2"
    ],
    "data structure*****function": [
        "$ARG1 to describe the expressions for derivatives as they use to describe the original $ARG2"
    ],
    "deep learning*****second derivative": [
        "$ARG1 , it is rare to compute a single $ARG2"
    ],
    "second derivative*****scalar": [
        "$ARG1 of a $ARG2"
    ],
    "function*****hessian matrix": [
        "$ARG1 f : Rn \u2192 R , then the $ARG2",
        "$ARG1 f ( x ) whose $ARG2"
    ],
    "deep learning*****krylov methods": [
        "$ARG1 approach is to use $ARG2"
    ],
    "krylov methods*****set": [
        "$ARG1 are a $ARG2"
    ],
    "matrix*****operation": [
        "$ARG1 or \ufb01nding approximations to its eigenvectors or eigenvalues , without using any $ARG2"
    ],
    "krylov methods*****hessian matrix": [
        "$ARG1 on the Hessian , we only need to be able to compute the product between the $ARG2"
    ],
    "hessian matrix*****vector": [
        "$ARG1 H and an arbitrary $ARG2"
    ],
    "vector*****technique": [
        "$ARG1 v. A straightforward $ARG2"
    ],
    "vector*****computational graph": [
        "$ARG1 produced by a $ARG2"
    ],
    "gradient descent*****function": [
        "$ARG1 to minimize the error in a $ARG2",
        "$ARG1 to minimize a quadratic $ARG2"
    ],
    "deep feedforward network*****view": [
        "$ARG1 From this point of $ARG2"
    ],
    "view*****function": [
        "$ARG1 , the modern feedforward network is the culmination of centuries of progress on the general $ARG2",
        "$ARG1 this plot as a linear cross-section of a high-dimensional $ARG2",
        "$ARG1 the decision $ARG2",
        "$ARG1 of brain $ARG2"
    ],
    "function*****task": [
        "$ARG1 approximation $ARG2",
        "$ARG1 f ( x ) , but do so using many more parameters than are necessary for the $ARG2",
        "$ARG1 f : Rn \u2192 R. This type of $ARG2"
    ],
    "rule*****back-propagation": [
        "$ARG1 that underlies the $ARG2"
    ],
    "optimization problem*****gradient descent": [
        "$ARG1 in closed form , but $ARG2"
    ],
    "gradient descent*****technique": [
        "$ARG1 was not introduced as a $ARG2"
    ],
    "technique*****solution": [
        "$ARG1 for iteratively approximating the $ARG2"
    ],
    "solution*****optimization problem": [
        "$ARG1 to $ARG2"
    ],
    "machine learning*****perceptron": [
        "$ARG1 models such as the $ARG2"
    ],
    "learning*****multilayer perceptron": [
        "$ARG1 nonlinear functions required the development of a $ARG2"
    ],
    "rule*****sensitivity": [
        "$ARG1 based on dynamic programming began to appear in the 1960s and 1970s , mostly for control applications ( Kelley , 1960 ; Bryson and Denham , 1961 ; Dreyfus , 1962 ; Bryson and Ho , 1969 ; Dreyfus , 1973 ) but also for $ARG2"
    ],
    "parallel distributed processing*****back-propagation": [
        "$ARG1 presented the results of some of the \ufb01rst successful experiments with $ARG2"
    ],
    "cognition*****learning": [
        "$ARG1 and $ARG2"
    ],
    "learning*****connectionism": [
        "$ARG1 , which came under the name of \u201c $ARG2"
    ],
    "connectionism*****learning": [
        "$ARG1 \u201d because of the importance this school of thought places on the connections between neurons as the locus of $ARG2"
    ],
    "learning*****memory": [
        "$ARG1 and $ARG2"
    ],
    "deep feedforward network*****gradient descent": [
        "$ARG1 approaches to $ARG2"
    ],
    "generalization*****challenge": [
        "$ARG1 is a $ARG2"
    ],
    "challenge*****neural network": [
        "$ARG1 for $ARG2"
    ],
    "number*****mean squared error": [
        "$ARG1 of algorithmic changes have improved the performance of neural One of these algorithmic changes was the replacement of $ARG2"
    ],
    "mean squared error*****cross-entropy": [
        "$ARG1 with the $ARG2",
        "$ARG1 was popular in the 1980s and 1990s , but was gradually replaced by $ARG2",
        "$ARG1 is the $ARG2",
        "$ARG1 is the $ARG2"
    ],
    "cross-entropy*****loss function": [
        "$ARG1 family of $ARG2"
    ],
    "cross-entropy*****principle": [
        "$ARG1 losses and the $ARG2"
    ],
    "statistics*****machine learning": [
        "$ARG1 community and the $ARG2",
        "$ARG1 gives us many tools that can be used to achieve the $ARG2",
        "$ARG1 , but this term is more rarely used in $ARG2"
    ],
    "cross-entropy*****sigmoid": [
        "$ARG1 losses greatly improved the performance of models with $ARG2"
    ],
    "softmax*****learning": [
        "$ARG1 outputs , which had previously su\ufb00ered from saturation and slow $ARG2"
    ],
    "learning*****mean squared error": [
        "$ARG1 when using the $ARG2"
    ],
    "other*****change": [
        "$ARG1 major algorithmic $ARG2",
        "$ARG1 layers do not $ARG2",
        "$ARG1 words , it speci\ufb01es how to scale a small $ARG2"
    ],
    "change*****sigmoid": [
        "$ARG1 that has greatly improved the performance of feedforward networks was the replacement of $ARG2"
    ],
    "sigmoid*****recti\ufb01ed linear unit": [
        "$ARG1 hidden units with piecewise linear hidden units , such as $ARG2"
    ],
    "neural network*****neocognitron": [
        "$ARG1 models and dates back at least as far as the Cognitron and $ARG2"
    ],
    "system*****neural network": [
        "$ARG1 \u201d among several di\ufb00erent factors of $ARG2"
    ],
    "weights*****hidden layer": [
        "$ARG1 of the $ARG2",
        "$ARG1 to initialize a deep autoencoder with gradually smaller $ARG2"
    ],
    "weights*****information": [
        "$ARG1 are su\ufb03cient to propagate useful $ARG2",
        "$ARG1 should be large enough to propagate $ARG2"
    ],
    "information*****feature": [
        "$ARG1 through a recti\ufb01ed linear network , allowing the classi\ufb01er layer at the top to learn how to map di\ufb00erent $ARG2",
        "$ARG1 included in the representation of the patient is known as a $ARG2",
        "$ARG1 ( such as evidence for a particular $ARG2"
    ],
    "recti\ufb01ed linear unit*****neuroscience": [
        "$ARG1 are also of historical interest because they show that $ARG2"
    ],
    "neuroscience*****deep learning": [
        "$ARG1 has continued to have an in\ufb02uence on the development of $ARG2",
        "$ARG1 is regarded as an important source of inspiration for $ARG2",
        "$ARG1 in $ARG2",
        "$ARG1 has given us a reason to hope that a single $ARG2",
        "$ARG1 \u201d and is a separate \ufb01eld of study from $ARG2"
    ],
    "learning*****variational autoencoder": [
        "$ARG1 in feedforward networks is used as a tool to develop probabilistic models , such as the $ARG2"
    ],
    "variational autoencoder*****generative adversarial networks": [
        "$ARG1 and $ARG2"
    ],
    "unsupervised learning*****support": [
        "$ARG1 to $ARG2"
    ],
    "supervised learning*****support": [
        "$ARG1 to $ARG2"
    ],
    "optimization algorithm*****model": [
        "$ARG1 and $ARG2"
    ],
    "supervised learning*****vector": [
        "$ARG1 problems\u2014how to learn to map one $ARG2",
        "$ARG1 involves observing several examples of a random $ARG2"
    ],
    "learning*****accuracy": [
        "$ARG1 algorithms require large amounts of supervised data to achieve good $ARG2"
    ],
    "deep learning*****unsupervised learning": [
        "$ARG1 algorithms have been designed to tackle $ARG2",
        "$ARG1 based on $ARG2"
    ],
    "unsupervised learning*****problem": [
        "$ARG1 problems , but none have truly solved the $ARG2"
    ],
    "problem*****deep learning": [
        "$ARG1 in the same way that $ARG2",
        "$ARG1 is one of the simplest and most widely used tests in $ARG2",
        "$ARG1 of determining the capacity of a $ARG2",
        "$ARG1 , you may even want to begin without using $ARG2"
    ],
    "unsupervised learning*****random variable": [
        "$ARG1 is the high dimensionality of the $ARG2"
    ],
    "challenges*****challenge": [
        "$ARG1 : a statistical $ARG2"
    ],
    "challenge*****generalization": [
        "$ARG1 regards $ARG2",
        "$ARG1 of generalizing to new examples becomes exponentially more di\ufb03cult when working with high-dimensional data , and how the mechanisms used to achieve $ARG2"
    ],
    "generalization*****number": [
        "$ARG1 : the $ARG2",
        "$ARG1 error , as the $ARG2",
        "$ARG1 error can never increase as the $ARG2",
        "$ARG1 error for a \ufb01xed $ARG2"
    ],
    "challenge*****learning": [
        "$ARG1 associated with high-dimensional distributions arises because many algorithms for $ARG2",
        "$ARG1 of $ARG2"
    ],
    "challenge*****inference": [
        "$ARG1 arises from the need to perform intractable $ARG2",
        "$ARG1 of $ARG2"
    ],
    "distribution*****constant": [
        "$ARG1 over a , b and c. In order to even compute such conditional probabilities one needs to sum over the values of the variables c , as well as compute a normalization $ARG2"
    ],
    "probability*****inference": [
        "$ARG1 functions come up in $ARG2",
        "$ARG1 , while our optimization-based $ARG2"
    ],
    "markov chain*****partition function": [
        "$ARG1 ( MCMC ) methods ( chapter 17 ) are often used to deal with the $ARG2"
    ],
    "deep learning*****arti\ufb01cial intelligence": [
        "$ARG1 , and push the \ufb01eld forward towards true $ARG2",
        "$ARG1 and $ARG2"
    ],
    "mathematical concept*****deep learning": [
        "$ARG1 needed to understand $ARG2"
    ],
    "cost function*****algorithm": [
        "$ARG1 that measures how well those beliefs correspond with reality and using a training $ARG2"
    ],
    "algorithm*****cost function": [
        "$ARG1 to minimize that $ARG2",
        "$ARG1 to successfully minimize the $ARG2"
    ],
    "speech*****medicine": [
        "$ARG1 or images , make diagnoses in $ARG2"
    ],
    "medicine*****support": [
        "$ARG1 and $ARG2"
    ],
    "arti\ufb01cial intelligence*****human": [
        "$ARG1 , the \ufb01eld rapidly tackled and solved problems that are intellectually di\ufb03cult for $ARG2"
    ],
    "human*****list": [
        "$ARG1 beings but relatively straightforward for computers\u2014problems that can be described by a $ARG2"
    ],
    "challenge*****arti\ufb01cial intelligence": [
        "$ARG1 to $ARG2"
    ],
    "solution*****concept": [
        "$ARG1 is to allow computers to learn from experience and understand the world in terms of a hierarchy of concepts , with each $ARG2"
    ],
    "concept*****relation": [
        "$ARG1 de\ufb01ned in terms of its $ARG2",
        "$ARG1 de\ufb01ned in $ARG2",
        "$ARG1 analogous to a $ARG2"
    ],
    "graph*****1": [
        "$ARG1 showing how these CHAPTER $ARG2",
        "$ARG1 can be unfolded by applying the de\ufb01nition \u03c4 \u2212 $ARG2",
        "$ARG1 drawn in \ufb01gure 16.2 , p ( t0 , t $ARG2",
        "$ARG1 over t0 , t $ARG2"
    ],
    "other*****graph": [
        "$ARG1 , the $ARG2",
        "$ARG1 factors can not be encoded in a $ARG2",
        "$ARG1 variables that precede it in the ordering as its ancestors in the $ARG2",
        "$ARG1 variables , we often require that all the conditioning variables come earlier than the variables to be sampled in the ordered $ARG2"
    ],
    "example*****deep blue": [
        "$ARG1 , IBM \u2019 s $ARG2"
    ],
    "deep blue*****system": [
        "$ARG1 chess-playing $ARG2"
    ],
    "chess*****challenge": [
        "$ARG1 strategy is a tremendous accomplishment , but the $ARG2"
    ],
    "set*****chess": [
        "$ARG1 of $ARG2"
    ],
    "chess*****list": [
        "$ARG1 can be completely described by a very brief $ARG2"
    ],
    "chess*****matching": [
        "$ARG1 player , but are only recently $ARG2"
    ],
    "human*****speech": [
        "$ARG1 beings to recognize objects or $ARG2"
    ],
    "challenges*****arti\ufb01cial intelligence": [
        "$ARG1 in $ARG2",
        "$ARG1 needed to solve $ARG2"
    ],
    "base*****arti\ufb01cial intelligence": [
        "$ARG1 approach to $ARG2"
    ],
    "cyc*****inference": [
        "$ARG1 is an $ARG2"
    ],
    "inference*****database": [
        "$ARG1 engine and a $ARG2"
    ],
    "example*****cyc": [
        "$ARG1 , $ARG2"
    ],
    "inference*****inconsistency": [
        "$ARG1 engine detected an $ARG2"
    ],
    "capability*****machine learning": [
        "$ARG1 is known as $ARG2"
    ],
    "algorithm*****logistic regression": [
        "$ARG1 called $ARG2"
    ],
    "algorithm*****naive bayes": [
        "$ARG1 called $ARG2"
    ],
    "logistic regression*****system": [
        "$ARG1 is used to recommend cesarean delivery , the AI $ARG2"
    ],
    "system*****information": [
        "$ARG1 several pieces of relevant $ARG2",
        "$ARG1 \u2019 s understanding of the simpler concepts can be re\ufb01ned given $ARG2",
        "$ARG1 of gating units that controls the \ufb02ow of $ARG2",
        "$ARG1 learn how to \u201c align \u201d the acoustic-level $ARG2"
    ],
    "dependence*****phenomenon": [
        "$ARG1 on representations is a general $ARG2"
    ],
    "phenomenon*****computer science": [
        "$ARG1 that appears throughout $ARG2"
    ],
    "computer science*****collection": [
        "$ARG1 , operations such as searching a $ARG2"
    ],
    "arti\ufb01cial intelligence*****set": [
        "$ARG1 tasks can be solved by designing the right $ARG2"
    ],
    "set*****task": [
        "$ARG1 of features to extract for that $ARG2",
        "$ARG1 of features for a simple $ARG2",
        "$ARG1 size until the $ARG2"
    ],
    "task*****machine learning": [
        "$ARG1 , then providing these features to a simple $ARG2",
        "$ARG1 , T $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 , the $ARG2"
    ],
    "example*****feature": [
        "$ARG1 , a useful $ARG2",
        "$ARG1 of an in\ufb01nite-dimensional $ARG2"
    ],
    "problem*****machine learning": [
        "$ARG1 is to use $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 in $ARG2",
        "$ARG1 in $ARG2"
    ],
    "machine learning*****mapping": [
        "$ARG1 to discover not only the $ARG2",
        "$ARG1 Figure 14.1 : The general structure of an autoencoder , $ARG2"
    ],
    "task*****human": [
        "$ARG1 requires a great deal of $ARG2",
        "$ARG1 by trial and error , without any guidance from the $ARG2"
    ],
    "example*****representation learning": [
        "$ARG1 of a $ARG2",
        "$ARG1 of a simple $ARG2"
    ],
    "function*****decoder": [
        "$ARG1 that converts the input data into a di\ufb00erent representation , and a $ARG2",
        "$ARG1 h = f ( x ) and a $ARG2"
    ],
    "function*****format": [
        "$ARG1 that converts the new representation back into the original $ARG2"
    ],
    "information*****encoder": [
        "$ARG1 as possible when an input is run through the $ARG2"
    ],
    "speech*****factors of variation": [
        "$ARG1 recording , the $ARG2"
    ],
    "image*****factors of variation": [
        "$ARG1 of a car , the $ARG2"
    ],
    "arti\ufb01cial intelligence*****factors of variation": [
        "$ARG1 applications is that many of the $ARG2"
    ],
    "problem*****representation learning": [
        "$ARG1 , $ARG2",
        "$ARG1 in $ARG2"
    ],
    "deep learning*****problem": [
        "$ARG1 solves this central $ARG2",
        "$ARG1 A central $ARG2"
    ],
    "system*****concept": [
        "$ARG1 can represent the $ARG2"
    ],
    "concept*****image": [
        "$ARG1 of an $ARG2"
    ],
    "image*****turn": [
        "$ARG1 of a person by combining simpler concepts , such as corners and contours , which are in $ARG2"
    ],
    "example*****deep learning": [
        "$ARG1 of a $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 , $ARG2",
        "$ARG1 of neuroscienti\ufb01c principles in\ufb02uencing $ARG2",
        "$ARG1 of a software package that automatically detects and stabilizes many common numerically unstable expressions that arise in the context of $ARG2"
    ],
    "model*****multilayer perceptron": [
        "$ARG1 is the feedforward deep network or $ARG2"
    ],
    "multilayer perceptron*****function": [
        "$ARG1 is just a mathematical $ARG2"
    ],
    "mapping*****set": [
        "$ARG1 some $ARG2",
        "$ARG1 from a $ARG2",
        "$ARG1 from a rich $ARG2"
    ],
    "memory*****set": [
        "$ARG1 after executing another $ARG2",
        "$ARG1 requirements for storing the parameters will increase only by a factor of the size of this $ARG2"
    ],
    "identity*****illustration": [
        "$ARG1 ) ( object parts ) ( input pixels ) Figure 1.2 : $ARG2"
    ],
    "illustration*****deep learning": [
        "$ARG1 of a $ARG2"
    ],
    "image*****collection": [
        "$ARG1 represented as a $ARG2"
    ],
    "set*****identity": [
        "$ARG1 of pixels to an object $ARG2"
    ],
    "learning*****mapping": [
        "$ARG1 or evaluating this $ARG2"
    ],
    "deep learning*****mapping": [
        "$ARG1 resolves this di\ufb03culty by breaking the desired complicated $ARG2"
    ],
    "mapping*****series": [
        "$ARG1 into a $ARG2"
    ],
    "series*****model": [
        "$ARG1 of nested simple mappings , each described by a di\ufb00erent layer of the $ARG2"
    ],
    "visible layer*****contains": [
        "$ARG1 , so named because it $ARG2"
    ],
    "hidden layer*****image": [
        "$ARG1 extracts increasingly abstract features from the $ARG2",
        "$ARG1 \u2019 s description of the $ARG2"
    ],
    "hidden layer*****search": [
        "$ARG1 can easily $ARG2"
    ],
    "image*****hidden layer": [
        "$ARG1 in terms of corners and contours , the third $ARG2"
    ],
    "image*****contains": [
        "$ARG1 in terms of the object parts it $ARG2",
        "$ARG1 data : One channel $ARG2",
        "$ARG1 I ( x , y ) exp ( \u2212\u03b2x x\ue0302 \u2212 \u03b2yy \ue0302 ) $ARG2"
    ],
    "contains*****image": [
        "$ARG1 can be used to recognize the objects present in the $ARG2"
    ],
    "illustration*****computational graph": [
        "$ARG1 of $ARG2"
    ],
    "mapping*****operation": [
        "$ARG1 an input to an output where each node performs an $ARG2"
    ],
    "logistic regression model*****\u03c3": [
        "$ARG1 , $ARG2"
    ],
    "multiplication*****logistic sigmoid": [
        "$ARG1 and $ARG2"
    ],
    "logistic sigmoid*****model": [
        "$ARG1 as the elements of our computer language , then this $ARG2"
    ],
    "logistic regression*****element": [
        "$ARG1 as an $ARG2"
    ],
    "element*****model": [
        "$ARG1 itself , then this $ARG2"
    ],
    "view*****deep learning": [
        "$ARG1 of $ARG2",
        "$ARG1 uses $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of designing the $ARG2"
    ],
    "deep learning*****information": [
        "$ARG1 , not all of the $ARG2",
        "$ARG1 research today is that we simply do not have enough $ARG2",
        "$ARG1 approach is often to \ufb01gure out what the minimum amount of $ARG2"
    ],
    "information*****factors of variation": [
        "$ARG1 in a layer \u2019 s activations necessarily encodes $ARG2"
    ],
    "content*****model": [
        "$ARG1 of the input speci\ufb01cally , but it helps the $ARG2"
    ],
    "view*****number": [
        "$ARG1 is based on the $ARG2",
        "$ARG1 imagery using a two-step cascade that \ufb01rst locates the address $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 address $ARG2"
    ],
    "path*****model": [
        "$ARG1 through a \ufb02ow chart that describes how to compute each of the $ARG2"
    ],
    "model*****computational graph": [
        "$ARG1 as being not the depth of the $ARG2"
    ],
    "concept*****graph": [
        "$ARG1 may be much deeper than the $ARG2"
    ],
    "example*****system": [
        "$ARG1 , an AI $ARG2",
        "$ARG1 , while readers who just want to implement a working $ARG2",
        "$ARG1 , suppose we have a vision $ARG2",
        "$ARG1 , consider the classical form of a dynamical $ARG2",
        "$ARG1 , let us consider a dynamical $ARG2"
    ],
    "system*****image": [
        "$ARG1 observing an $ARG2",
        "$ARG1 would crop the $ARG2"
    ],
    "graph*****concept": [
        "$ARG1 of computations includes 2n layers if we re\ufb01ne our estimate of each $ARG2"
    ],
    "concept*****other": [
        "$ARG1 given the $ARG2"
    ],
    "model*****deep learning": [
        "$ARG1 requires to qualify as \u201c deep. \u201d However , $ARG2"
    ],
    "deep learning*****machine learning": [
        "$ARG1 can safely be regarded as the study of models that either involve a greater amount of composition of learned functions or learned concepts than traditional $ARG2",
        "$ARG1 is a particular kind of $ARG2",
        "$ARG1 \u201d goes beyond the neuroscienti\ufb01c perspective on the current breed of $ARG2",
        "$ARG1 is an approach to $ARG2",
        "$ARG1 is a speci\ufb01c kind of $ARG2",
        "$ARG1 well , one must have a solid understanding of the basic principles of $ARG2",
        "$ARG1 The simple $ARG2",
        "$ARG1 is to scale $ARG2",
        "$ARG1 Converting between Undirected and Directed Graphs We often refer to a speci\ufb01c $ARG2"
    ],
    "machine learning*****technique": [
        "$ARG1 , a $ARG2"
    ],
    "learning*****concept": [
        "$ARG1 to represent the world as a nested hierarchy of concepts , with each $ARG2"
    ],
    "learning*****machine learning": [
        "$ARG1 about $ARG2",
        "$ARG1 algorithms greatly a\ufb00ected the modern landscape of $ARG2",
        "$ARG1 theory claims that a $ARG2"
    ],
    "machine learning*****statistics": [
        "$ARG1 or $ARG2",
        "$ARG1 is essentially a form of applied $ARG2"
    ],
    "statistics*****deep learning": [
        "$ARG1 background , but want to rapidly acquire one and begin using $ARG2"
    ],
    "deep learning*****1": [
        "$ARG1 has already proven useful in CHAPTER $ARG2",
        "$ARG1 A state of $ARG2",
        "$ARG1 Assuming that w ( 0 ) = 0 and that \ue00f is chosen to be small enough to guarantee |1 \u2212 \ue00f\u03bb i| < $ARG2",
        "$ARG1 3 \u22121 2 \u22125 4 2 \u22123 \u22121 $ARG2"
    ],
    "deep learning*****representation learning": [
        "$ARG1 is a kind of $ARG2"
    ],
    "representation learning*****turn": [
        "$ARG1 , which is in $ARG2"
    ],
    "turn*****machine learning": [
        "$ARG1 a kind of $ARG2"
    ],
    "system*****other": [
        "$ARG1 relate to each $ARG2",
        "$ARG1 is integrated with many $ARG2"
    ],
    "computer vision*****speech": [
        "$ARG1 , $ARG2"
    ],
    "speech*****audio": [
        "$ARG1 and $ARG2"
    ],
    "audio*****natural language processing": [
        "$ARG1 processing , $ARG2"
    ],
    "natural language processing*****robotics": [
        "$ARG1 , $ARG2"
    ],
    "robotics*****video game": [
        "$ARG1 , bioinformatics and chemistry , $ARG2"
    ],
    "video game*****search": [
        "$ARG1 , $ARG2"
    ],
    "linear algebra*****probability": [
        "$ARG1 , $ARG2",
        "$ARG1 , $ARG2"
    ],
    "probability*****machine learning": [
        "$ARG1 , and fundamental $ARG2"
    ],
    "machine learning*****example": [
        "$ARG1 concepts can skip part I , for $ARG2",
        "$ARG1 applications to use each training $ARG2",
        "$ARG1 BASICS $ARG2",
        "$ARG1 BASICS $ARG2",
        "$ARG1 BASICS entire checkerboard right is to cover each of its cells with at least one $ARG2",
        "$ARG1 BASICS examples , with each $ARG2"
    ],
    "deep learning*****accuracy": [
        "$ARG1 has solved increasingly complicated applications with increasing $ARG2"
    ],
    "neural network*****deep learning": [
        "$ARG1 We expect that many readers of this book have heard of $ARG2",
        "$ARG1 research popularized the use of the term \u201c $ARG2",
        "$ARG1 continues to the time of this writing , though the focus of $ARG2",
        "$ARG1 is of paramount importance , $ARG2",
        "$ARG1 and this success spurred a new wave of research into $ARG2",
        "$ARG1 to solve important commercial applications and remain at the forefront of commercial applications of $ARG2"
    ],
    "deep learning*****connectionism": [
        "$ARG1 known as $ARG2",
        "$ARG1 is based on the philosophy of $ARG2"
    ],
    "connectionism*****deep learning": [
        "$ARG1 in the 1980s\u20131990s , and the current resurgence under the name $ARG2",
        "$ARG1 movement of the 1980s that remain central to today \u2019 s $ARG2"
    ],
    "deep learning*****arti\ufb01cial neural network": [
        "$ARG1 has gone by is $ARG2",
        "$ARG1 has only recently become recognized as a crucial technology though the \ufb01rst experiments with $ARG2"
    ],
    "deep learning*****human": [
        "$ARG1 models is that they are engineered systems inspired by the biological brain ( whether the $ARG2",
        "$ARG1 usually not very easy for a $ARG2"
    ],
    "machine learning*****function": [
        "$ARG1 have sometimes been used to understand brain $ARG2",
        "$ARG1 algorithms need to be guided by prior beliefs about what kind of $ARG2",
        "$ARG1 techniques are based on minimizing a $ARG2",
        "$ARG1 algorithms exploit this idea only insofar as that they learn a $ARG2"
    ],
    "example*****path": [
        "$ARG1 that intelligent behavior is possible , and a conceptually straightforward $ARG2",
        "$ARG1 , linear paths or skip connections between layers reduce the length of the shortest $ARG2",
        "$ARG1 , a large proportion of the resulting networks have no input units or no $ARG2"
    ],
    "human*****machine learning": [
        "$ARG1 intelligence , so $ARG2"
    ],
    "multiple*****machine learning": [
        "$ARG1 levels of composition , which can be applied in $ARG2"
    ],
    "connectionism*****neural network": [
        "$ARG1 + $ARG2",
        "$ARG1 \u201d or \u201c $ARG2"
    ],
    "neural network*****connectionism": [
        "$ARG1 ) Figure 1.7 : The \ufb01gure shows two of the three historical waves of arti\ufb01cial neural nets research , as measured by the frequency of the phrases \u201c cybernetics \u201d and \u201c $ARG2",
        "$ARG1 research emerged in great part via a movement called $ARG2"
    ],
    "learning*****perceptron": [
        "$ARG1 ( McCulloch and Pitts , 1943 ; Hebb , 1949 ) and implementations of the \ufb01rst models such as the $ARG2"
    ],
    "neural network*****hidden layer": [
        "$ARG1 with one or two $ARG2"
    ],
    "other*****activity": [
        "$ARG1 two waves similarly appeared in book form much later than the corresponding scienti\ufb01c $ARG2"
    ],
    "deep learning*****linear model": [
        "$ARG1 were simple $ARG2"
    ],
    "set*****weights": [
        "$ARG1 of $ARG2",
        "$ARG1 the recurrent $ARG2",
        "$ARG1 the input and recurrent $ARG2",
        "$ARG1 the input and recurrent $ARG2",
        "$ARG1 the $ARG2",
        "$ARG1 the biases for each unit to heuristically chosen constants , and initialize only the $ARG2",
        "$ARG1 all of the initial $ARG2",
        "$ARG1 while ignoring the e\ufb00ect of the $ARG2",
        "$ARG1 of probabilities or $ARG2",
        "$ARG1 of columns of $ARG2",
        "$ARG1 of numbers ( such as all the $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of y coordinates , Y , and applying $ARG2"
    ],
    "model*****weights": [
        "$ARG1 to correspond to the desired de\ufb01nition of the categories , the $ARG2",
        "$ARG1 that could learn the $ARG2",
        "$ARG1 with only $ARG2",
        "$ARG1 with a Gaussian prior on the $ARG2",
        "$ARG1 is relatively insensitive to small variations in the $ARG2",
        "$ARG1 with all units , but with the $ARG2"
    ],
    "weights*****set": [
        "$ARG1 needed to be $ARG2",
        "$ARG1 could be $ARG2",
        "$ARG1 so that a rich $ARG2",
        "$ARG1 are close enough to zero that the biases may be $ARG2",
        "$ARG1 , then $ARG2",
        "$ARG1 on a $ARG2",
        "$ARG1 chosen on a validation $ARG2",
        "$ARG1 but augment x with an extra entry that is always $ARG2",
        "$ARG1 at every spatial location , we learn a $ARG2"
    ],
    "set*****human": [
        "$ARG1 by the $ARG2"
    ],
    "adaptive linear element*****number": [
        "$ARG1 ( ADALINE ) , which dates from about the same time , simply returned the value of f ( x ) itself to predict a real $ARG2"
    ],
    "algorithm*****weights": [
        "$ARG1 used to adapt the $ARG2",
        "$ARG1 that will improve the $ARG2"
    ],
    "weights*****special case": [
        "$ARG1 of the ADALINE was a $ARG2"
    ],
    "special case*****algorithm": [
        "$ARG1 of an $ARG2",
        "$ARG1 of the EM $ARG2"
    ],
    "algorithm*****deep learning": [
        "$ARG1 remain the dominant training algorithms for $ARG2",
        "$ARG1 Nearly all $ARG2"
    ],
    "perceptron*****linear model": [
        "$ARG1 and ADALINE are called $ARG2"
    ],
    "linear model*****learning": [
        "$ARG1 caused a backlash against biologically inspired $ARG2"
    ],
    "machine learning*****natural language processing": [
        "$ARG1 research was more fragmented , with di\ufb00erent communities of researchers studying $ARG2"
    ],
    "natural language processing*****speech recognition": [
        "$ARG1 , vision , motion planning and $ARG2"
    ],
    "neocognitron*****model": [
        "$ARG1 ( Fukushima , 1980 ) introduced a powerful $ARG2",
        "$ARG1 ( Fukushima , 1980 ) incorporated most of the $ARG2"
    ],
    "system*****convolutional network": [
        "$ARG1 and later became the basis for the modern $ARG2"
    ],
    "model*****recti\ufb01ed linear unit": [
        "$ARG1 neuron called the $ARG2",
        "$ARG1 innovations like the LSTM , $ARG2"
    ],
    "recti\ufb01ed linear unit*****machine learning": [
        "$ARG1 , but greater neural realism has not yet led to an improvement in $ARG2"
    ],
    "neuroscience*****neural network": [
        "$ARG1 has successfully inspired several $ARG2"
    ],
    "neural network*****learning": [
        "$ARG1 architectures , we do not yet know enough about biological $ARG2",
        "$ARG1 , the $ARG2"
    ],
    "learning*****neuroscience": [
        "$ARG1 for $ARG2"
    ],
    "neuroscience*****learning": [
        "$ARG1 to o\ufb00er much guidance for the $ARG2"
    ],
    "similarity*****deep learning": [
        "$ARG1 of $ARG2"
    ],
    "deep learning*****other": [
        "$ARG1 researchers are more likely to cite the brain as an in\ufb02uence than researchers working in $ARG2",
        "$ARG1 has also made contributions back to $ARG2",
        "$ARG1 has been applied to many $ARG2",
        "$ARG1 was designed to overcome these and $ARG2",
        "$ARG1 practitioners generally use the same basic computational tools as $ARG2"
    ],
    "machine learning*****kernel": [
        "$ARG1 \ufb01elds such as $ARG2",
        "$ARG1 applications , the input is usually a multidimensional array of data and the $ARG2"
    ],
    "kernel*****bayesian statistics": [
        "$ARG1 machines or $ARG2"
    ],
    "bayesian statistics*****view": [
        "$ARG1 , one should not $ARG2"
    ],
    "deep learning*****linear algebra": [
        "$ARG1 draws inspiration from many \ufb01elds , especially applied math fundamentals like $ARG2",
        "$ARG1 with a focused presentation of the key $ARG2"
    ],
    "probability*****information": [
        "$ARG1 , $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2",
        "$ARG1 AND $ARG2"
    ],
    "information*****optimization": [
        "$ARG1 theory , and numerical $ARG2"
    ],
    "deep learning*****neuroscience": [
        "$ARG1 researchers cite $ARG2",
        "$ARG1 is primarily concerned with how to build computer systems that are able to successfully solve tasks requiring intelligence , while the \ufb01eld of computational $ARG2"
    ],
    "neuroscience*****1": [
        "$ARG1 as an important source of inspiration , others are not concerned with CHAPTER $ARG2"
    ],
    "connectionism*****parallel distributed processing": [
        "$ARG1 or $ARG2"
    ],
    "connectionism*****number": [
        "$ARG1 is that a large $ARG2"
    ],
    "system*****feature": [
        "$ARG1 should be represented by many features , and each $ARG2"
    ],
    "concept*****identity": [
        "$ARG1 of color and object $ARG2"
    ],
    "distributed representation*****identity": [
        "$ARG1 , with three neurons describing the color and three neurons describing the object $ARG2"
    ],
    "neural network*****back-propagation": [
        "$ARG1 with internal representations and the popularization of the $ARG2",
        "$ARG1 with $ARG2"
    ],
    "kernel*****graphical model": [
        "$ARG1 machines ( Boser et al. , 1992 ; Cortes and Vapnik , 1995 ; Sch\u00f6lkopf et al. , 1999 ) and $ARG2"
    ],
    "machine learning*****yann lecun": [
        "$ARG1 research groups led by Geo\ufb00rey Hinton at University of Toronto , Yoshua Bengio at University of Montreal , and $ARG2"
    ],
    "human*****computer vision": [
        "$ARG1 and $ARG2"
    ],
    "neural network*****deep belief network": [
        "$ARG1 called a $ARG2"
    ],
    "deep belief network*****pretraining": [
        "$ARG1 could be e\ufb03ciently trained using a strategy called greedy layer-wise $ARG2"
    ],
    "other*****generalization": [
        "$ARG1 kinds of deep networks ( Bengio et al. , 2007 ; Ranzato et al. , 2007a ) and systematically helped to improve $ARG2",
        "$ARG1 way to improve $ARG2"
    ],
    "deep learning*****neural network": [
        "$ARG1 \u201d to emphasize that researchers were now able to train deeper $ARG2",
        "$ARG1 , the most di\ufb03cult is $ARG2",
        "$ARG1 and paved the way to the acceptance of $ARG2"
    ],
    "neural network*****attention": [
        "$ARG1 than had been possible before , and to focus $ARG2",
        "$ARG1 was introduced even earlier , in the context of handwriting generation ( Graves , 2013 ) , with an $ARG2"
    ],
    "learning*****human": [
        "$ARG1 algorithms reaching $ARG2"
    ],
    "human*****learning": [
        "$ARG1 performance on complex tasks today are nearly identical to the $ARG2"
    ],
    "dataset*****machine learning": [
        "$ARG1 appropriate for $ARG2",
        "$ARG1 to build a $ARG2"
    ],
    "rule*****deep learning": [
        "$ARG1 of thumb is that a supervised $ARG2"
    ],
    "algorithm*****human": [
        "$ARG1 will generally achieve acceptable performance with around 5,000 labeled examples per category , and will match or exceed $ARG2",
        "$ARG1 can estimate how con\ufb01dent it should be about a decision , especially if a wrong decision can be harmful and if a $ARG2"
    ],
    "human*****dataset": [
        "$ARG1 performance when trained with a $ARG2"
    ],
    "machine learning*****number": [
        "$ARG1 models have had a $ARG2",
        "$ARG1 problems become exceedingly di\ufb03cult when the $ARG2",
        "$ARG1 BASICS Figure 5.9 : As the $ARG2"
    ],
    "number*****magnitude": [
        "$ARG1 of connections per neuron that was within an order of $ARG2",
        "$ARG1 of inputs m without making the $ARG2"
    ],
    "number*****neural network": [
        "$ARG1 of neurons , $ARG2",
        "$ARG1 of parameters , so with k parameters ( and for even very small $ARG2",
        "$ARG1 of $ARG2"
    ],
    "arti\ufb01cial neural network*****number": [
        "$ARG1 will not have the same $ARG2"
    ],
    "neural network*****arti\ufb01cial intelligence": [
        "$ARG1 with fewer neurons than a leech were unable to solve sophisticated $ARG2"
    ],
    "view*****system": [
        "$ARG1 , are smaller than the nervous $ARG2",
        "$ARG1 the recurrent net as a dynamical $ARG2",
        "$ARG1 of the visual $ARG2"
    ],
    "dataset*****number": [
        "$ARG1 size ( $ARG2",
        "$ARG1 has the same $ARG2"
    ],
    "machine learning*****neural network": [
        "$ARG1 often worked with small , synthetic datasets , such as low-resolution bitmaps of letters , that were designed to incur low computational cost and demonstrate that $ARG2"
    ],
    "machine learning*****dataset": [
        "$ARG1 became more statistical in nature and began to leverage larger datasets containing tens of thousands of examples such as the MNIST $ARG2",
        "$ARG1 researchers is the Iris $ARG2",
        "$ARG1 algorithms do not just experience a \ufb01xed $ARG2",
        "$ARG1 algorithms simply experience a $ARG2",
        "$ARG1 BASICS Figure 5.4 : The e\ufb00ect of the training $ARG2",
        "$ARG1 BASICS Dividing the $ARG2",
        "$ARG1 BASICS Figure 5.13 : Training examples from the QMUL Multiview Face $ARG2"
    ],
    "view*****dataset": [
        "$ARG1 House Numbers $ARG2"
    ],
    "graph*****dataset": [
        "$ARG1 , we see that datasets of translated sentences , such as IBM \u2019 s $ARG2"
    ],
    "dataset*****other": [
        "$ARG1 ( Schwenk , 2014 ) are typically far ahead of $ARG2",
        "$ARG1 are independent from each $ARG2",
        "$ARG1 curation , including using $ARG2"
    ],
    "dataset*****image": [
        "$ARG1 consists of scans of handwritten digits and associated labels describing which digit 0\u20139 is contained in each $ARG2"
    ],
    "machine learning*****conditions": [
        "$ARG1 researchers to study their algorithms in controlled laboratory $ARG2"
    ],
    "accuracy*****deep learning": [
        "$ARG1 , Complexity and Real-World Impact Since the 1980s , $ARG2"
    ],
    "object recognition*****process": [
        "$ARG1 networks $ARG2"
    ],
    "object recognition*****challenge": [
        "$ARG1 is the ImageNet Large Scale Visual Recognition $ARG2"
    ],
    "convolutional network*****challenge": [
        "$ARG1 won this $ARG2"
    ],
    "challenge*****rate": [
        "$ARG1 for the \ufb01rst time and by a wide margin , bringing down the state-of-the-art top-5 error $ARG2"
    ],
    "rate*****convolutional network": [
        "$ARG1 from 26.1 % to 15.3 % ( Krizhevsky et al. , 2012 ) , meaning that the $ARG2"
    ],
    "convolutional network*****list": [
        "$ARG1 produces a ranked $ARG2"
    ],
    "list*****image": [
        "$ARG1 of possible categories for each $ARG2"
    ],
    "image*****list": [
        "$ARG1 and the correct category appeared in the \ufb01rst \ufb01ve entries of this $ARG2"
    ],
    "deep learning*****rate": [
        "$ARG1 have brought the latest top-5 error $ARG2"
    ],
    "deep learning*****speech recognition": [
        "$ARG1 has also had a dramatic impact on $ARG2",
        "$ARG1 ( Dahl et al. , 2010 ; Deng et al. , 2010b ; Seide et al. , 2011 ; Hinton et al. , 2012a ) to $ARG2",
        "$ARG1 is a crucial component of modern $ARG2"
    ],
    "pedestrian detection*****image": [
        "$ARG1 and $ARG2"
    ],
    "image*****classi\ufb01cation": [
        "$ARG1 segmentation ( Sermanet et al. , 2013 ; Farabet et al. , 2013 ; Couprie et al. , 2013 ) and yielded superhuman performance in tra\ufb03c sign $ARG2"
    ],
    "classi\ufb01cation*****1": [
        "$ARG1 ( Ciresan CHAPTER $ARG2",
        "$ARG1 targets with targets of k\u22121 and $ARG2"
    ],
    "number*****arti\ufb01cial neural network": [
        "$ARG1 of connections between neurons in $ARG2"
    ],
    "arti\ufb01cial neural network*****other": [
        "$ARG1 have nearly as many connections per neuron as a cat , and it is quite common for $ARG2"
    ],
    "neural network*****image": [
        "$ARG1 could learn to output an entire sequence of characters transcribed from an $ARG2",
        "$ARG1 that is specialized for processing a grid of values X such as an $ARG2"
    ],
    "recurrent neural network*****model": [
        "$ARG1 , such as the LSTM sequence $ARG2"
    ],
    "learning*****machine translation": [
        "$ARG1 seems to be on the cusp of revolutionizing another application : $ARG2"
    ],
    "machine translation*****neural turing machine": [
        "$ARG1 ( Sutskever et al. , 2014 ; Bahdanau et al. , This trend of increasing complexity has been pushed to its logical conclusion with the introduction of $ARG2"
    ],
    "neural turing machine*****memory": [
        "$ARG1 ( Graves et al. , 2014a ) that learn to read from $ARG2"
    ],
    "memory*****content": [
        "$ARG1 cells and write arbitrary $ARG2"
    ],
    "content*****memory": [
        "$ARG1 to $ARG2",
        "$ARG1 to $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of a $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of one $ARG2"
    ],
    "principle*****task": [
        "$ARG1 be applied to nearly any $ARG2",
        "$ARG1 , there was no requirement that these pixel values actually be based on a rendering $ARG2"
    ],
    "achievement*****deep learning": [
        "$ARG1 of $ARG2"
    ],
    "deep learning*****reinforcement learning": [
        "$ARG1 is its extension to the domain of $ARG2",
        "$ARG1 has also signi\ufb01cantly improved the performance of $ARG2",
        "$ARG1 approach to $ARG2"
    ],
    "reinforcement learning*****task": [
        "$ARG1 , an autonomous agent must learn to perform a $ARG2"
    ],
    "system*****deep learning": [
        "$ARG1 based on $ARG2"
    ],
    "learning*****video game": [
        "$ARG1 to play Atari $ARG2"
    ],
    "reinforcement learning*****robotics": [
        "$ARG1 for $ARG2"
    ],
    "tensorflow*****deep learning": [
        "$ARG1 ( Abadi et al. , 2015 ) have all supported important research projects or $ARG2"
    ],
    "search*****human": [
        "$ARG1 for subatomic particles ( Baldi et al. , 2014 ) , and to automatically parse microscope images used to construct a 3-D map of the $ARG2",
        "$ARG1 by a $ARG2"
    ],
    "machine learning*****human": [
        "$ARG1 that has drawn heavily on our knowledge of the $ARG2",
        "$ARG1 allows us to tackle tasks that are too di\ufb03cult to solve with \ufb01xed programs written and designed by $ARG2"
    ],
    "human*****statistics": [
        "$ARG1 brain , $ARG2"
    ],
    "challenges*****deep learning": [
        "$ARG1 and opportunities to improve $ARG2",
        "$ARG1 in $ARG2",
        "$ARG1 have motivated the development of $ARG2",
        "$ARG1 Motivating $ARG2"
    ],
    "recurrent neural network*****speech recognition": [
        "$ARG1 for $ARG2"
    ],
    "multilayer perceptron*****speech recognition": [
        "$ARG1 for $ARG2"
    ],
    "sigmoid belief network*****8": [
        "$ARG1 ( Saul et al. , 1996 ) $ARG2"
    ],
    "classi\ufb01cation*****rate": [
        "$ARG1 error $ARG2"
    ],
    "convolutional network*****neural network": [
        "$ARG1 is a $ARG2",
        "$ARG1 are simply $ARG2",
        "$ARG1 were also some of the \ufb01rst $ARG2",
        "$ARG1 provide a way to specialize $ARG2"
    ],
    "image*****recurrent neural network": [
        "$ARG1 , a $ARG2"
    ],
    "convolutional network*****process": [
        "$ARG1 can $ARG2",
        "$ARG1 Once a prediction for each pixel is made , various methods can be used to further $ARG2",
        "$ARG1 is that they can also $ARG2"
    ],
    "process*****variable": [
        "$ARG1 images of $ARG2",
        "$ARG1 sequences of $ARG2"
    ],
    "variable*****recurrent network": [
        "$ARG1 size , $ARG2",
        "$ARG1 To go from multi-layer networks to $ARG2"
    ],
    "recurrent network*****process": [
        "$ARG1 can also $ARG2"
    ],
    "recurrent network*****machine learning": [
        "$ARG1 , we need to take advantage of one of the early ideas found in $ARG2"
    ],
    "machine learning*****statistical models": [
        "$ARG1 and $ARG2"
    ],
    "statistical models*****model": [
        "$ARG1 of the 1980s : sharing parameters across di\ufb00erent parts of a $ARG2"
    ],
    "parameter sharing*****model": [
        "$ARG1 makes it possible to extend and apply the $ARG2",
        "$ARG1 , because we interpret the various models or $ARG2"
    ],
    "information*****multiple": [
        "$ARG1 can occur at $ARG2"
    ],
    "example*****machine learning": [
        "$ARG1 , consider the two sentences \u201c I went to Nepal in 2009 \u201d and \u201c In 2009 , I went to Nepal. \u201d If we ask a $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 of what $ARG2",
        "$ARG1 of a simple $ARG2",
        "$ARG1 repeatedly as we introduce more $ARG2",
        "$ARG1 to obtain a polynomial of degree 9 : y\u0302 = b + $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 of a successful application of insights obtained by studying the brain to $ARG2",
        "$ARG1 , $ARG2",
        "$ARG1 of a general strategy in $ARG2"
    ],
    "recurrent neural network*****weights": [
        "$ARG1 shares the same $ARG2"
    ],
    "convolution*****function": [
        "$ARG1 is a sequence where each member of the output is a $ARG2",
        "$ARG1 , if we let g be any $ARG2"
    ],
    "parameter sharing*****convolution": [
        "$ARG1 manifests in the application of the same $ARG2",
        "$ARG1 used by the $ARG2"
    ],
    "recurrent network*****minibatch": [
        "$ARG1 usually operate on minibatches of such sequences , with a di\ufb00erent sequence length \u03c4 for each member of the $ARG2"
    ],
    "computational graph*****recurrent neural network": [
        "$ARG1 allow us to de\ufb01ne $ARG2",
        "$ARG1 , where each node is now associated with one particular Some examples of important design patterns for $ARG2"
    ],
    "information*****recurrent neural network": [
        "$ARG1 on $ARG2"
    ],
    "set*****mapping": [
        "$ARG1 of computations , such as those involved in $ARG2"
    ],
    "mapping*****loss": [
        "$ARG1 inputs and parameters to outputs and $ARG2"
    ],
    "graph*****example": [
        "$ARG1 results in the sharing of parameters across a deep network For $ARG2",
        "$ARG1 has one node per training $ARG2"
    ],
    "computational graph*****classical dynamical system": [
        "$ARG1 of equation 10.1 and equation 10.3 is illustrated in s ( ... ) s ( t\u22121 ) s ( t ) s ( t+1 ) s ( ... ) Figure 10.1 : The $ARG2"
    ],
    "classical dynamical system*****computational graph": [
        "$ARG1 described by equation 10.1 , illustrated as an unfolded $ARG2"
    ],
    "function*****feedforward neural network": [
        "$ARG1 can be considered a $ARG2"
    ],
    "feedforward neural network*****function": [
        "$ARG1 , essentially any $ARG2"
    ],
    "function*****recurrent neural network": [
        "$ARG1 involving recurrence can be considered a $ARG2",
        "$ARG1 composition employed by $ARG2"
    ],
    "variable*****output layer": [
        "$ARG1 h to represent h ( t ) = f ( h ( t\u22121 ) , x ( t ) ; \u03b8 ) , illustrated in \ufb01gure 10.2 , typical RNNs will add extra architectural features such as $ARG2"
    ],
    "output layer*****information": [
        "$ARG1 that read $ARG2"
    ],
    "recurrent network*****task": [
        "$ARG1 is trained to perform a $ARG2"
    ],
    "precision*****other": [
        "$ARG1 than $ARG2"
    ],
    "example*****information": [
        "$ARG1 , if the RNN is used in statistical language modeling , typically to predict the next word given previous words , it may not be necessary to store all of the $ARG2",
        "$ARG1 , good CPU-based code is usually designed to read $ARG2",
        "$ARG1 , this extra $ARG2",
        "$ARG1 conveys less $ARG2",
        "$ARG1 of reading factorization $ARG2",
        "$ARG1 , \ufb01nding out that a tossed coin has come up as heads twice should convey twice as much $ARG2",
        "$ARG1 , an event with unit density still has zero $ARG2"
    ],
    "recurrent network*****information": [
        "$ARG1 just processes $ARG2",
        "$ARG1 we have considered up to now have a \u201c causal \u201d structure , meaning that the state at time t only captures $ARG2"
    ],
    "interaction*****1": [
        "$ARG1 takes place with a delay of a single time step , from the state at time t to the state at time t + $ARG2"
    ],
    "variable*****point in time": [
        "$ARG1 per time step , representing the state of the component at that $ARG2"
    ],
    "variable*****computational graph": [
        "$ARG1 for each time step is drawn as a separate node of the $ARG2"
    ],
    "model*****parameter sharing": [
        "$ARG1 to be estimated with far fewer training examples than would be required without $ARG2",
        "$ARG1 has no $ARG2",
        "$ARG1 structures with $ARG2"
    ],
    "information*****path": [
        "$ARG1 \ufb02ow forward in time ( computing outputs and losses ) and backward in time ( computing gradients ) by explicitly showing the $ARG2"
    ],
    "path*****graph": [
        "$ARG1 along which this Armed with the $ARG2"
    ],
    "graph*****parameter sharing": [
        "$ARG1 unrolling and $ARG2"
    ],
    "parameter sharing*****recurrent neural network": [
        "$ARG1 ideas of section 10.1 , we can design a wide variety of $ARG2"
    ],
    "computational graph*****loss": [
        "$ARG1 to compute the training $ARG2"
    ],
    "loss*****recurrent network": [
        "$ARG1 of a $ARG2",
        "$ARG1 with a $ARG2"
    ],
    "loss*****softmax": [
        "$ARG1 L internally computes y\u0302 = $ARG2",
        "$ARG1 applied to a traditional $ARG2"
    ],
    "recurrent neural network*****function": [
        "$ARG1 of \ufb01gure 10.3 and equation 10.8 is universal in the sense that any $ARG2"
    ],
    "function*****recurrent network": [
        "$ARG1 computable by a Turing machine can be computed by such a $ARG2"
    ],
    "operation*****vector": [
        "$ARG1 as a post-processing step to obtain a $ARG2"
    ],
    "connection*****hidden layer": [
        "$ARG1 from the output to the $ARG2"
    ],
    "hidden layer*****loss": [
        "$ARG1 activations are h ( t ) , the outputs are o ( t ) , the targets are y ( t ) and the $ARG2"
    ],
    "softmax*****bias": [
        "$ARG1 ( o ( t ) ) where the parameters are the $ARG2"
    ],
    "loss function*****operation": [
        "$ARG1 with respect to the parameters is an expensive $ARG2"
    ],
    "pass*****illustration": [
        "$ARG1 moving left to right through our $ARG2"
    ],
    "illustration*****graph": [
        "$ARG1 of the unrolled $ARG2"
    ],
    "graph*****pass": [
        "$ARG1 in \ufb01gure 10.3 , followed by a backward propagation $ARG2"
    ],
    "pass*****graph": [
        "$ARG1 moving right to left through the $ARG2"
    ],
    "graph*****back-propagation through time": [
        "$ARG1 with O ( \u03c4 ) cost is called $ARG2"
    ],
    "information*****system": [
        "$ARG1 about the past history of the input , unless the user knows how to describe the full state of the $ARG2"
    ],
    "system*****set": [
        "$ARG1 and provides it as part of the training $ARG2",
        "$ARG1 , and $ARG2"
    ],
    "model*****teacher forcing": [
        "$ARG1 may be trained with $ARG2"
    ],
    "teacher forcing*****maximum likelihood": [
        "$ARG1 is a procedure that emerges from the $ARG2"
    ],
    "illustration*****teacher forcing": [
        "$ARG1 of $ARG2"
    ],
    "teacher forcing*****technique": [
        "$ARG1 is a training $ARG2"
    ],
    "model*****conditional probability": [
        "$ARG1 is trained to maximize the $ARG2",
        "$ARG1 to estimate the $ARG2"
    ],
    "conditional probability*****set": [
        "$ARG1 of y ( 2 ) given both the x sequence so far and the previous y value from the training $ARG2"
    ],
    "teacher forcing*****back-propagation through time": [
        "$ARG1 as allowing us to avoid $ARG2"
    ],
    "teacher forcing*****distribution": [
        "$ARG1 arises if the network is going to be later used in an open-loop mode , with the network outputs ( or samples from the output $ARG2"
    ],
    "problem*****example": [
        "$ARG1 is to train with both teacher-forced inputs and with free-running inputs , for $ARG2"
    ],
    "gradient*****recurrent neural network": [
        "$ARG1 in a $ARG2",
        "$ARG1 through a $ARG2"
    ],
    "recurrent neural network*****gradient": [
        "$ARG1 Computing the $ARG2"
    ],
    "loss*****derivation": [
        "$ARG1 \u2202L ( t ) In this $ARG2"
    ],
    "derivation*****softmax function": [
        "$ARG1 we assume that the outputs o ( t ) are used as the argument to the $ARG2"
    ],
    "softmax function*****vector": [
        "$ARG1 to obtain the $ARG2"
    ],
    "1*****diagonal matrix": [
        "$ARG1 \u2212 \u2202h ( t+1 ) \u2202h ( t ) h ( t+1 ) indicates the $ARG2"
    ],
    "diagonal matrix*****1": [
        "$ARG1 containing the elements $ARG2"
    ],
    "method*****computational graph": [
        "$ARG1 of section 6.5.6 , that computes the contribution of a single edge in the $ARG2"
    ],
    "ambiguity*****weights": [
        "$ARG1 , we introduce dummy variables W ( t ) that are de\ufb01ned to be copies of W but with each W ( t ) used only at time step t. We may then use \u2207 W ( t ) to denote the contribution of the $ARG2"
    ],
    "weights*****gradient": [
        "$ARG1 at time step t to the $ARG2",
        "$ARG1 and tends to halt in areas that are nearer to the initial parameters ( whether due to getting stuck in a region of low $ARG2"
    ],
    "gradient*****computational graph": [
        "$ARG1 with respect to x ( t ) for training because it does not have any parameters as ancestors in the $ARG2"
    ],
    "recurrent network*****directed graphical model": [
        "$ARG1 as $ARG2"
    ],
    "directed graphical model*****example": [
        "$ARG1 In the $ARG2",
        "$ARG1 depicting the relay race $ARG2"
    ],
    "principle*****loss": [
        "$ARG1 possible to use almost any $ARG2"
    ],
    "loss*****task": [
        "$ARG1 should be chosen based on the $ARG2",
        "$ARG1 more directly linked to the $ARG2"
    ],
    "probability distribution*****cross-entropy": [
        "$ARG1 , and we usually use the $ARG2"
    ],
    "distribution*****loss": [
        "$ARG1 to de\ufb01ne the $ARG2"
    ],
    "distribution*****element": [
        "$ARG1 of the next sequence $ARG2"
    ],
    "joint probability*****series": [
        "$ARG1 over the sequence of y values as a $ARG2"
    ],
    "series*****distribution": [
        "$ARG1 of one-step probabilistic predictions is one way to capture the full joint $ARG2"
    ],
    "random variable*****1": [
        "$ARG1 Y = { y ( $ARG2",
        "$ARG1 x ( $ARG2",
        "$ARG1 being equal to $ARG2",
        "$ARG1 being equal to $ARG2",
        "$ARG1 is nearly always $ARG2"
    ],
    "1*****graphical model": [
        "$ARG1 ) y ( 2 ) y ( 3 ) y ( 4 ) y ( 5 ) y ( ... ) Figure 10.7 : Fully connected $ARG2"
    ],
    "graphical model*****1": [
        "$ARG1 for a sequence y ( $ARG2"
    ],
    "graphical model*****graph": [
        "$ARG1 directly according to this $ARG2",
        "$ARG1 whose structure is the complete $ARG2",
        "$ARG1 over the y values with the complete $ARG2",
        "$ARG1 is understanding which variables need to be able to interact directly , i.e. , which $ARG2",
        "$ARG1 is that the $ARG2",
        "$ARG1 use a $ARG2"
    ],
    "number*****element": [
        "$ARG1 of inputs and parameters for each $ARG2"
    ],
    "1*****variable": [
        "$ARG1 ) y ( 2 ) y ( 3 ) y ( 4 ) y ( 5 ) y ( ... ) Figure 10.8 : Introducing the state $ARG2"
    ],
    "variable*****graphical model": [
        "$ARG1 in the $ARG2"
    ],
    "graphical model*****function": [
        "$ARG1 of the RNN , even though it is a deterministic $ARG2"
    ],
    "number*****other": [
        "$ARG1 of inputs for each node ) and can share the same parameters with the $ARG2",
        "$ARG1 of samples , our estimate of the true underlying parameter is uncertain , in the sense that we could have obtained $ARG2"
    ],
    "graphical model*****other": [
        "$ARG1 indicate which variables depend directly on $ARG2",
        "$ARG1 typically have large groups of units that are all connected to $ARG2"
    ],
    "example*****assumption": [
        "$ARG1 , it is common to make the Markov $ARG2"
    ],
    "assumption*****graphical model": [
        "$ARG1 that the $ARG2"
    ],
    "graphical model*****view": [
        "$ARG1 is to $ARG2"
    ],
    "view*****graphical model": [
        "$ARG1 the RNN as de\ufb01ning a $ARG2",
        "$ARG1 of an RNN as a $ARG2",
        "$ARG1 of $ARG2"
    ],
    "interpretation*****model": [
        "$ARG1 of the RNN is based on ignoring the hidden units h ( t ) by marginalizing them out of the $ARG2",
        "$ARG1 of the sparsity penalty as corresponding to log p $ARG2"
    ],
    "graphical model*****random variable": [
        "$ARG1 structure of RNNs that results from regarding the hidden units h ( t ) as $ARG2"
    ],
    "random variable*****graphical model": [
        "$ARG1 Including the hidden units in the $ARG2"
    ],
    "graphical model*****distribution": [
        "$ARG1 reveals that the RNN provides a very e\ufb03cient parametrization of the joint $ARG2"
    ],
    "parameter sharing*****number": [
        "$ARG1 , the $ARG2",
        "$ARG1 has allowed CNNs to dramatically lower the $ARG2",
        "$ARG1 makes it possible to represent an exponential $ARG2"
    ],
    "graph*****model": [
        "$ARG1 shows that the $ARG2",
        "$ARG1 is the simplest directed $ARG2",
        "$ARG1 ) or requires introducing so many edges that the resulting directed $ARG2"
    ],
    "model*****conditional probability distribution": [
        "$ARG1 can be e\ufb03ciently parametrized by using the same $ARG2"
    ],
    "conditional probability distribution*****probability": [
        "$ARG1 at each time step , and that when the variables are all observed , the $ARG2"
    ],
    "recurrent network*****number": [
        "$ARG1 pay for their reduced $ARG2"
    ],
    "parameter sharing*****recurrent network": [
        "$ARG1 used in $ARG2"
    ],
    "recurrent network*****assumption": [
        "$ARG1 relies on the $ARG2"
    ],
    "assumption*****conditional probability distribution": [
        "$ARG1 is that the $ARG2"
    ],
    "conditional probability distribution*****1": [
        "$ARG1 over the variables at time t+ $ARG2"
    ],
    "1*****principle": [
        "$ARG1 given the variables at time t is stationary , meaning that the relationship between the previous time step and the next time step does not depend on t. In $ARG2",
        "$ARG1 likes item D , then this should be a strong cue that user 2 will also like D. Algorithms based on this $ARG2"
    ],
    "graphical model*****model": [
        "$ARG1 , we must describe how to draw samples from the $ARG2",
        "$ARG1 , we can de\ufb01ne the depth of a $ARG2"
    ],
    "operation*****distribution": [
        "$ARG1 that we need to perform is simply to sample from the conditional $ARG2"
    ],
    "option*****model": [
        "$ARG1 is to introduce an extra Bernoulli output to the $ARG2"
    ],
    "sigmoid*****cross-entropy": [
        "$ARG1 unit trained with the $ARG2"
    ],
    "directed graphical model*****random variable": [
        "$ARG1 over a sequence of $ARG2",
        "$ARG1 over $ARG2"
    ],
    "view*****distribution": [
        "$ARG1 to represent not only a joint $ARG2"
    ],
    "option*****vector": [
        "$ARG1 is to take only a single $ARG2"
    ],
    "interaction*****unit vector": [
        "$ARG1 between the input x and each hidden $ARG2"
    ],
    "unit vector*****matrix": [
        "$ARG1 h ( t ) is parametrized by a newly introduced weight $ARG2"
    ],
    "vector*****distribution": [
        "$ARG1 x into a $ARG2",
        "$ARG1 x \u2208 Rn , the joint $ARG2",
        "$ARG1 w , we \ufb01rst need to specify a prior $ARG2",
        "$ARG1 w have been omitted ; they are implied by the fact that the $ARG2",
        "$ARG1 \ufb01eld ( g ( f ( x ) ) \u2212 x ) that estimates the score of the data $ARG2"
    ],
    "assumption*****distribution": [
        "$ARG1 that this $ARG2"
    ],
    "recurrent neural network*****loss": [
        "$ARG1 , meant to learn to map input sequences x to target sequences y , with $ARG2"
    ],
    "information*****1": [
        "$ARG1 from the past , x ( $ARG2"
    ],
    "speech recognition*****interpretation": [
        "$ARG1 , the correct $ARG2"
    ],
    "convolutional network*****feature": [
        "$ARG1 , RNNs applied to images are typically more expensive but allow for long-range lateral interactions between features in the same $ARG2",
        "$ARG1 attending to di\ufb00erent spatial locations of each $ARG2"
    ],
    "forward propagation*****convolution": [
        "$ARG1 equations for such RNNs may be written in a form that shows they use a $ARG2"
    ],
    "convolution*****feature": [
        "$ARG1 that computes the bottom-up input to each layer , prior to the recurrent propagation across the $ARG2",
        "$ARG1 , and full Locally connected layers are useful when we know that each $ARG2"
    ],
    "learning*****1": [
        "$ARG1 to generate an output sequence ( y ( $ARG2"
    ],
    "decoder*****probability": [
        "$ARG1 RNN that generates the output sequence ( or computes the $ARG2"
    ],
    "encoder*****variable": [
        "$ARG1 RNN is used to compute a generally \ufb01xed-size context $ARG2"
    ],
    "variable*****decoder": [
        "$ARG1 C which represents a semantic summary of the input sequence and is given as input to the $ARG2"
    ],
    "speech recognition*****machine translation": [
        "$ARG1 , $ARG2",
        "$ARG1 ( Graves et al. , 2013 ; Graves and Jaitly , 2014 ) , handwriting generation ( Graves , 2013 ) , $ARG2",
        "$ARG1 systems used at major companies including Microsoft , IBM and Google ( Hinton et al. , \u2022 $ARG2",
        "$ARG1 , $ARG2"
    ],
    "system*****machine translation": [
        "$ARG1 is based on scoring proposals generated by another $ARG2"
    ],
    "system*****recurrent network": [
        "$ARG1 , while the latter uses a standalone $ARG2",
        "$ARG1 is a $ARG2"
    ],
    "encoder*****function": [
        "$ARG1 emits the context C , usually as a simple $ARG2",
        "$ARG1 and could not learn $ARG2",
        "$ARG1 , or representation $ARG2"
    ],
    "decoder*****vector": [
        "$ARG1 or writer or output RNN is conditioned on that \ufb01xed-length $ARG2"
    ],
    "average*****1": [
        "$ARG1 of log P ( y ( $ARG2",
        "$ARG1 \u00b5 ( t ) of some value v ( t ) by applying the update \u00b5 ( t ) \u2190 \u03b1\u00b5 ( t\u22121 ) + ( $ARG2",
        "$ARG1 : \u03b8\u0302 ( t ) = \u03b1\u03b8\u0302 ( t\u22121 ) + ( $ARG2",
        "$ARG1 over one-hot code vectors c with cy = $ARG2"
    ],
    "encoder*****vector": [
        "$ARG1 RNN is typically used as a representation C of the input sequence that is provided as input to the If the context C is a $ARG2"
    ],
    "vector*****decoder": [
        "$ARG1 , then the $ARG2"
    ],
    "constraint*****encoder": [
        "$ARG1 that the $ARG2"
    ],
    "encoder*****hidden layer": [
        "$ARG1 must have the same size of $ARG2"
    ],
    "hidden layer*****decoder": [
        "$ARG1 as the $ARG2"
    ],
    "encoder*****dimension": [
        "$ARG1 RNN has a $ARG2"
    ],
    "representational capacity*****learning": [
        "$ARG1 suggest to allocate enough capacity in each of these three steps , but doing so by adding depth may hurt $ARG2"
    ],
    "learning*****optimization": [
        "$ARG1 by making $ARG2",
        "$ARG1 Di\ufb00ers from Pure $ARG2",
        "$ARG1 trajectory spends most of its time tracing out a wide arc around a Much of research into the di\ufb03culties of $ARG2"
    ],
    "path*****variable": [
        "$ARG1 from a $ARG2",
        "$ARG1 exists between them , or all paths contain an observed $ARG2",
        "$ARG1 from h i to an observed $ARG2"
    ],
    "hidden layer*****path": [
        "$ARG1 is used for the state-to-state transition , we have doubled the length of the shortest $ARG2"
    ],
    "computational graph*****recurrent network": [
        "$ARG1 that generalizes that of the $ARG2"
    ],
    "recursive neural network*****generalization": [
        "$ARG1 2 represent yet another $ARG2"
    ],
    "generalization*****recurrent network": [
        "$ARG1 of $ARG2"
    ],
    "recurrent network*****computational graph": [
        "$ARG1 , with a di\ufb00erent kind of $ARG2",
        "$ARG1 , described in chapter 10 , which construct very deep $ARG2"
    ],
    "recursive neural network*****recurrent neural network": [
        "$ARG1 \u201d as \u201c RNN \u201d to avoid confusion with \u201c $ARG2"
    ],
    "potential*****learning": [
        "$ARG1 use for $ARG2"
    ],
    "data structure*****natural language processing": [
        "$ARG1 as input to neural nets ( Frasconi et al. , 1997 , 1998 ) , in $ARG2"
    ],
    "natural language processing*****computer vision": [
        "$ARG1 ( Socher et al. , 2011a , c , 2013a ) as well as in $ARG2"
    ],
    "option*****tree structure": [
        "$ARG1 is to have a $ARG2"
    ],
    "example*****tree structure": [
        "$ARG1 , when processing natural language sentences , the $ARG2"
    ],
    "tensor*****model": [
        "$ARG1 operations and bilinear forms , which have previously been found useful to $ARG2"
    ],
    "learning*****recurrent network": [
        "$ARG1 long-term dependencies in $ARG2"
    ],
    "weights*****multiplication": [
        "$ARG1 given to long-term interactions ( involving the $ARG2"
    ],
    "recurrent network*****function": [
        "$ARG1 involve the composition of the same $ARG2"
    ],
    "recurrent neural network*****matrix multiplication": [
        "$ARG1 somewhat resembles $ARG2"
    ],
    "relation*****recurrent neural network": [
        "$ARG1 h ( t ) = W \ue03e h ( t\u22121 ) as a very simple $ARG2"
    ],
    "recurrent neural network*****activation function": [
        "$ARG1 lacking a nonlinear $ARG2"
    ],
    "relation*****method": [
        "$ARG1 essentially describes the power $ARG2"
    ],
    "problem*****recurrent network": [
        "$ARG1 is particular to $ARG2"
    ],
    "weights*****variance": [
        "$ARG1 with $ARG2"
    ],
    "deep feedforward network*****gradient": [
        "$ARG1 with carefully chosen scaling can thus avoid the vanishing and exploding $ARG2",
        "$ARG1 can largely avoid the vanishing and exploding $ARG2"
    ],
    "gradient*****interaction": [
        "$ARG1 of a long term $ARG2",
        "$ARG1 of a short term $ARG2"
    ],
    "interaction*****magnitude": [
        "$ARG1 has exponentially smaller $ARG2"
    ],
    "magnitude*****gradient": [
        "$ARG1 than the $ARG2"
    ],
    "optimization*****probability": [
        "$ARG1 becomes increasingly di\ufb03cult , with the $ARG2"
    ],
    "learning*****challenges": [
        "$ARG1 long-term dependencies remains one of the main $ARG2"
    ],
    "mapping*****weights": [
        "$ARG1 from h ( t\u22121 ) to h ( t ) and the input $ARG2"
    ],
    "mapping*****recurrent network": [
        "$ARG1 from x ( t ) to h ( t ) are some of the most di\ufb03cult parameters to learn in a $ARG2"
    ],
    "echo state network*****liquid state machine": [
        "$ARG1 or ESNs ( Jaeger and Haas , 2004 ; Jaeger , 2007b ) and $ARG2"
    ],
    "recurrent network*****kernel": [
        "$ARG1 is that they are similar to $ARG2"
    ],
    "kernel*****vector": [
        "$ARG1 machines : they map an arbitrary length sequence ( the history of inputs up to time t ) into a \ufb01xed-length $ARG2",
        "$ARG1 can be expressed as the outer product of d vectors , one $ARG2"
    ],
    "vector*****linear regression": [
        "$ARG1 ( the recurrent state h ( t ) ) , on which a linear predictor ( typically a $ARG2"
    ],
    "linear regression*****problem": [
        "$ARG1 ) can be applied to solve the $ARG2",
        "$ARG1 solves a regression $ARG2"
    ],
    "function*****weights": [
        "$ARG1 of the output $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 learned by the network because the $ARG2",
        "$ARG1 with $ARG2"
    ],
    "example*****linear regression": [
        "$ARG1 , if the output consists of $ARG2",
        "$ARG1 , we can perform $ARG2",
        "$ARG1 : the $ARG2",
        "$ARG1 : $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 , $ARG2",
        "$ARG1 , we can modify the training criterion for $ARG2",
        "$ARG1 : $ARG2",
        "$ARG1 : Bayesian $ARG2",
        "$ARG1 , consider a $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of a sparsely parametrized $ARG2"
    ],
    "linear regression*****mean squared error": [
        "$ARG1 from the hidden units to the output targets , and the training criterion is $ARG2"
    ],
    "mean squared error*****learning": [
        "$ARG1 , then it is convex and may be solved reliably with simple $ARG2"
    ],
    "set*****recurrent neural network": [
        "$ARG1 of histories can be represented in the $ARG2"
    ],
    "recurrent neural network*****view": [
        "$ARG1 state ? The answer proposed in the reservoir computing literature is to $ARG2"
    ],
    "weights*****system": [
        "$ARG1 such that the dynamical $ARG2"
    ],
    "recurrent network*****eigenvalue": [
        "$ARG1 is the $ARG2"
    ],
    "spectral radius*****absolute value": [
        "$ARG1 of J ( t ) , de\ufb01ned to be the maximum of the $ARG2"
    ],
    "spectral radius*****back-propagation": [
        "$ARG1 , consider the simple case of $ARG2"
    ],
    "back-propagation*****jacobian matrix": [
        "$ARG1 with a $ARG2"
    ],
    "jacobian matrix*****change": [
        "$ARG1 J that does not $ARG2",
        "$ARG1 tells us how a small $ARG2"
    ],
    "change*****example": [
        "$ARG1 with t. This case happens , for $ARG2"
    ],
    "eigenvalue*****multiplication": [
        "$ARG1 \u03bb , then $ARG2"
    ],
    "back-propagation*****distance": [
        "$ARG1 are separated by a $ARG2"
    ],
    "derivative*****spectral radius": [
        "$ARG1 of the nonlinearity will approach zero on many time steps , and help to prevent the explosion resulting from a large $ARG2"
    ],
    "echo state network*****spectral radius": [
        "$ARG1 advocates using a $ARG2"
    ],
    "back-propagation*****matrix multiplication": [
        "$ARG1 via repeated $ARG2"
    ],
    "matrix multiplication*****forward propagation": [
        "$ARG1 applies equally to $ARG2"
    ],
    "linear map*****norm": [
        "$ARG1 W \ue03e always shrinks h as measured by the L2 $ARG2"
    ],
    "spectral radius*****mapping": [
        "$ARG1 is less than one , the $ARG2"
    ],
    "mapping*****change": [
        "$ARG1 from h ( t ) to h ( t+1 ) is contractive , so a small $ARG2"
    ],
    "information*****precision": [
        "$ARG1 about the past when we use a \ufb01nite level of $ARG2"
    ],
    "precision*****vector": [
        "$ARG1 ( such as 32 bit integers ) to store the state $ARG2"
    ],
    "magnitude*****absolute value": [
        "$ARG1 ( complex $ARG2"
    ],
    "absolute value*****matrix": [
        "$ARG1 ) of these possibly complex-valued basis coe\ufb03cients , when we multiply the $ARG2"
    ],
    "eigenvalue*****magnitude": [
        "$ARG1 with $ARG2"
    ],
    "forward propagation*****example": [
        "$ARG1 has bounded dynamics , for $ARG2"
    ],
    "example*****spectral radius": [
        "$ARG1 , when a sequence of tanh units are all in the middle of their linear regime and are connected by weight matrices with $ARG2"
    ],
    "spectral radius*****1": [
        "$ARG1 greater than $ARG2"
    ],
    "echo state network*****weights": [
        "$ARG1 is simply to \ufb01x the $ARG2"
    ],
    "weights*****spectral radius": [
        "$ARG1 to have some $ARG2"
    ],
    "spectral radius*****information": [
        "$ARG1 such as 3 , where $ARG2"
    ],
    "weights*****recurrent network": [
        "$ARG1 in a fully trainable $ARG2",
        "$ARG1 shared across layers a particular kind of $ARG2"
    ],
    "weights*****back-propagation through time": [
        "$ARG1 trained using $ARG2"
    ],
    "spectral radius*****sparse initialization": [
        "$ARG1 of 1.2 performs well , combined with the $ARG2"
    ],
    "leaky units*****other": [
        "$ARG1 and $ARG2"
    ],
    "other*****multiple": [
        "$ARG1 Strategies for $ARG2"
    ],
    "multiple*****model": [
        "$ARG1 One way to deal with long-term dependencies is to design a $ARG2",
        "$ARG1 time scales , so that some parts of the $ARG2",
        "$ARG1 machines work together on a single datapoint , with each machine running a di\ufb00erent part of the $ARG2",
        "$ARG1 paths to identify a single word in order to better $ARG2",
        "$ARG1 output values and a good $ARG2"
    ],
    "model*****multiple": [
        "$ARG1 that operates at $ARG2",
        "$ARG1 words that have $ARG2",
        "$ARG1 of V1 involves $ARG2"
    ],
    "other*****information": [
        "$ARG1 parts operate at coarse time scales and transfer $ARG2",
        "$ARG1 , obtaining a more con\ufb01dent and reliable classi\ufb01er , thus extracting more $ARG2",
        "$ARG1 texts use base-2 logarithms and units called bits or shannons ; $ARG2"
    ],
    "leaky units*****removal": [
        "$ARG1 \u201d that integrate signals with di\ufb00erent time constants , and the $ARG2"
    ],
    "recurrent network*****connection": [
        "$ARG1 , a recurrent $ARG2"
    ],
    "connection*****1": [
        "$ARG1 goes from a unit at time t to a unit at time t + $ARG2"
    ],
    "average*****information": [
        "$ARG1 remembers $ARG2"
    ],
    "leaky units*****echo state network": [
        "$ARG1 were also found to be useful in the context of $ARG2"
    ],
    "constant*****example": [
        "$ARG1 , for $ARG2"
    ],
    "distribution*****initialization": [
        "$ARG1 once at $ARG2"
    ],
    "multiple*****information": [
        "$ARG1 time-scales ( El Hihi and Bengio , 1996 ) , with $ARG2"
    ],
    "long short-term memory*****other": [
        "$ARG1 and $ARG2"
    ],
    "long short-term memory*****gated recurrent unit": [
        "$ARG1 and networks based on the $ARG2"
    ],
    "leaky units*****connection": [
        "$ARG1 did this with $ARG2"
    ],
    "recurrent network*****other": [
        "$ARG1 \u201c cell. \u201d Cells are connected recurrently to each $ARG2"
    ],
    "other*****recurrent network": [
        "$ARG1 , replacing the usual hidden units of ordinary $ARG2"
    ],
    "leaky units*****information": [
        "$ARG1 allow the network to accumulate $ARG2"
    ],
    "information*****neural network": [
        "$ARG1 has been used , it might be useful for the $ARG2"
    ],
    "gradient*****long short-term memory": [
        "$ARG1 can \ufb02ow for long durations is a core contribution of the initial $ARG2"
    ],
    "long short-term memory*****model": [
        "$ARG1 ( LSTM ) $ARG2"
    ],
    "machine translation*****image": [
        "$ARG1 ( Sutskever et al. , 2014 ) , $ARG2"
    ],
    "forward propagation*****recurrent network": [
        "$ARG1 equations are given below , in the case of a shallow $ARG2"
    ],
    "a\ufb03ne*****recurrent network": [
        "$ARG1 transformation of inputs and recurrent units , LSTM $ARG2"
    ],
    "recurrent network*****system": [
        "$ARG1 , but has more parameters and a $ARG2"
    ],
    "constant*****forget gate": [
        "$ARG1 ) is controlled by a $ARG2"
    ],
    "forget gate*****1": [
        "$ARG1 unit fi ( for time step t and cell i ) , that sets this weight to a value between 0 and $ARG2"
    ],
    "1*****sigmoid": [
        "$ARG1 via a $ARG2",
        "$ARG1 ) = $ARG2"
    ],
    "\u03c3*****vector": [
        "$ARG1 \uf8edb fi + Ui , j xj + Wi , j hj where x ( t ) is the current input $ARG2",
        "$ARG1 \uf8edb ri + U i , j r ( t ) \uf8f8 The reset and updates gates can individually \u201c ignore \u201d parts of the state $ARG2",
        "$ARG1 is a $ARG2"
    ],
    "weights*****forget gate": [
        "$ARG1 for the $ARG2"
    ],
    "\u03c3*****weights": [
        "$ARG1 \uf8edb i + U i , jx j + W i , j hj where b , U and W respectively denote the biases , input $ARG2",
        "$ARG1 \uf8edboi + which has parameters bo , U o , W o for its biases , input $ARG2"
    ],
    "forget gate*****sigmoid": [
        "$ARG1 ( with a $ARG2"
    ],
    "\u03c3*****sigmoid": [
        "$ARG1 \uf8edb gi + U i , j The output hi of the LSTM cell can also be shut o\ufb00 , via the output gate q i , which also uses a $ARG2",
        "$ARG1 ( zi ) = \u2212\u03b6 ( \u2212zi ) relating the $ARG2"
    ],
    "dimension*****sigmoid": [
        "$ARG1 , thus choosing to copy it ( at one extreme of the $ARG2"
    ],
    "sigmoid*****other": [
        "$ARG1 ) or completely ignore it ( at the $ARG2"
    ],
    "other*****integrator": [
        "$ARG1 extreme ) by replacing it by the new \u201c target state \u201d value ( towards which the leaky $ARG2"
    ],
    "example*****forget gate": [
        "$ARG1 the reset gate ( or $ARG2"
    ],
    "forget gate*****multiple": [
        "$ARG1 ) output could be shared across $ARG2"
    ],
    "bias*****1": [
        "$ARG1 of $ARG2",
        "$ARG1 for h so that h \u2248 $ARG2",
        "$ARG1 to $ARG2",
        "$ARG1 ( \u03b8\u0302m ) = E [ \u03b8\u0302 m ] \u2212 \u03b8 x ( i ) \u2212 \u03b8 $ARG2",
        "$ARG1 ( \u00b5\u0302 m ) = E [ \u02c6 \u00b5m ] \u2212 \u00b5 $ARG2"
    ],
    "1*****forget gate": [
        "$ARG1 to the LSTM $ARG2",
        "$ARG1 for the $ARG2"
    ],
    "optimization*****gradient": [
        "$ARG1 for Long-Term Dependencies Section 8.2.5 and section 10.7 have described the vanishing and exploding $ARG2",
        "$ARG1 FOR TRAINING DEEP MODELS that the $ARG2"
    ],
    "optimization algorithm*****derivative": [
        "$ARG1 may roughly be understood as dividing the \ufb01rst $ARG2"
    ],
    "derivative*****second derivative": [
        "$ARG1 by the $ARG2",
        "$ARG1 , then the ratio of \ufb01rst and $ARG2"
    ],
    "second derivative*****dimension": [
        "$ARG1 ( in higher $ARG2"
    ],
    "dimension*****gradient": [
        "$ARG1 , multiplying the $ARG2"
    ],
    "second derivative*****rate": [
        "$ARG1 shrinks at a similar $ARG2"
    ],
    "rate*****derivative": [
        "$ARG1 to the \ufb01rst $ARG2"
    ],
    "second derivative*****constant": [
        "$ARG1 may remain relatively $ARG2"
    ],
    "minibatch*****saddle points": [
        "$ARG1 , and a tendency to be attracted to $ARG2"
    ],
    "nesterov momentum*****initialization": [
        "$ARG1 with careful $ARG2"
    ],
    "objective function*****function": [
        "$ARG1 ( as a $ARG2",
        "$ARG1 as a $ARG2"
    ],
    "function*****objective function": [
        "$ARG1 of the parameters ) has a \u201c landscape \u201d in which one \ufb01nds \u201c cli\ufb00s \u201d : wide and rather \ufb02at regions separated by tiny regions where the $ARG2",
        "$ARG1 we want to minimize or maximize is called the $ARG2"
    ],
    "gradient*****gradient descent": [
        "$ARG1 is very large , a $ARG2",
        "$ARG1 , while $ARG2",
        "$ARG1 , such as $ARG2"
    ],
    "objective function*****solution": [
        "$ARG1 is larger , undoing much of the work that had been done to reach the current $ARG2"
    ],
    "gradient*****steepest descent": [
        "$ARG1 tells us the direction that corresponds to the $ARG2",
        "$ARG1 indicates the direction of approximately $ARG2"
    ],
    "example*****gradient clipping": [
        "$ARG1 of the e\ufb00ect of $ARG2"
    ],
    "gradient clipping*****recurrent network": [
        "$ARG1 in a $ARG2"
    ],
    "recurrent network*****gradient clipping": [
        "$ARG1 with two parameters w and b. $ARG2"
    ],
    "gradient clipping*****gradient descent": [
        "$ARG1 can make $ARG2",
        "$ARG1 ( thresholding the values of the gradients before performing a $ARG2"
    ],
    "number*****matrix": [
        "$ARG1 of time steps because the weight $ARG2",
        "$ARG1 is large , $ARG2"
    ],
    "gradient descent*****gradient clipping": [
        "$ARG1 without $ARG2",
        "$ARG1 with $ARG2"
    ],
    "gradient clipping*****gradient": [
        "$ARG1 overshoots the bottom of this small ravine , then receives a very large $ARG2",
        "$ARG1 , $ARG2",
        "$ARG1 heuristic intervenes to reduce the step size to be small enough that it is less likely to go outside the region where the $ARG2"
    ],
    "solution*****gradient": [
        "$ARG1 has been in use by practitioners for many years : clipping the $ARG2"
    ],
    "option*****gradient": [
        "$ARG1 is to clip the parameter $ARG2"
    ],
    "norm*****gradient": [
        "$ARG1 ||g|| of the $ARG2",
        "$ARG1 of the true $ARG2",
        "$ARG1 clipping heuristic ( which handles $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "weights*****method": [
        "$ARG1 and biases ) is renormalized jointly with a single scaling factor , the latter $ARG2"
    ],
    "norm*****vector": [
        "$ARG1 clipping , the parameter update $ARG2",
        "$ARG1 of a $ARG2",
        "$ARG1 depend on the entire $ARG2",
        "$ARG1 , because scaling the $ARG2",
        "$ARG1 of a $ARG2",
        "$ARG1 of the 2-D $ARG2",
        "$ARG1 penalty , while the $ARG2"
    ],
    "norm*****change": [
        "$ARG1 per-minibatch will not $ARG2"
    ],
    "average*****gradient": [
        "$ARG1 of the norm-clipped $ARG2",
        "$ARG1 of all $ARG2",
        "$ARG1 of the negative $ARG2"
    ],
    "gradient*****norm": [
        "$ARG1 from many minibatches is not equivalent to clipping the $ARG2"
    ],
    "norm*****minibatch": [
        "$ARG1 , as well as examples that appear in the same $ARG2"
    ],
    "contrast*****minibatch": [
        "$ARG1 to traditional $ARG2"
    ],
    "gradient descent*****gradient": [
        "$ARG1 , where the true $ARG2",
        "$ARG1 , every $ARG2",
        "$ARG1 , while others may use analytical solutions for where the $ARG2"
    ],
    "gradient*****average": [
        "$ARG1 direction is equal to the $ARG2",
        "$ARG1 accumulation into an exponentially weighted moving $ARG2"
    ],
    "average*****minibatch": [
        "$ARG1 over all $ARG2"
    ],
    "stochastic gradient descent*****unbiased": [
        "$ARG1 uses an $ARG2"
    ],
    "unbiased*****gradient": [
        "$ARG1 estimate of the $ARG2",
        "$ARG1 estimate of the $ARG2",
        "$ARG1 estimate of its $ARG2"
    ],
    "gradient descent*****norm": [
        "$ARG1 with $ARG2"
    ],
    "norm*****bias": [
        "$ARG1 clipping introduces a heuristic $ARG2"
    ],
    "information*****gradient clipping": [
        "$ARG1 Flow $ARG2"
    ],
    "computational graph*****1": [
        "$ARG1 of the unfolded recurrent architecture along which the product of gradients associated with arcs is near $ARG2"
    ],
    "magnitude*****loss function": [
        "$ARG1 , even if the $ARG2"
    ],
    "other*****neural network": [
        "$ARG1 knowledge can be explicit , declarative , and relatively straightforward to put into words\u2014every day commonsense knowledge , like \u201c a cat is a kind of animal , \u201d or very speci\ufb01c facts that you need to know to accomplish your current goals , like \u201c the meeting with the sales team is at 3:00 PM in room $ARG2",
        "$ARG1 settings besides $ARG2",
        "$ARG1 contexts are less applicable to $ARG2",
        "$ARG1 neurons in the same layer , $ARG2",
        "$ARG1 applications discussed in this chapter , very generic $ARG2",
        "$ARG1 \ufb01elds , some of the key design principles of $ARG2"
    ],
    "example*****memory": [
        "$ARG1 of a network with an explicit $ARG2",
        "$ARG1 , $ARG2"
    ],
    "memory*****neural turing machine": [
        "$ARG1 , capturing some of the key design elements of the $ARG2"
    ],
    "task*****memory": [
        "$ARG1 network , \u201d here a recurrent net in the bottom ) from the \u201c $ARG2",
        "$ARG1 network learns to \u201c control \u201d the $ARG2",
        "$ARG1 network can choose to read from or write to speci\ufb01c $ARG2"
    ],
    "memory*****model": [
        "$ARG1 \u201d part of the $ARG2",
        "$ARG1 cost of storing the $ARG2",
        "$ARG1 is choosing to not $ARG2",
        "$ARG1 requirements of the $ARG2",
        "$ARG1 footprint of the $ARG2"
    ],
    "neural network*****memory": [
        "$ARG1 lack the equivalent of the working $ARG2",
        "$ARG1 \u201d is coupled with a $ARG2",
        "$ARG1 , since training and evaluating such networks is costly in terms of runtime and $ARG2"
    ],
    "system*****human": [
        "$ARG1 that allows $ARG2",
        "$ARG1 thinks that it is less likely than a $ARG2",
        "$ARG1 is only useful if it is able to dramatically reduce the amount of photos that the $ARG2"
    ],
    "human*****information": [
        "$ARG1 beings to explicitly hold and manipulate pieces of $ARG2"
    ],
    "memory network*****set": [
        "$ARG1 that include a $ARG2"
    ],
    "set*****memory": [
        "$ARG1 of $ARG2",
        "$ARG1 ) need to be stored in $ARG2"
    ],
    "memory network*****memory": [
        "$ARG1 originally required a supervision signal instructing them how to use their $ARG2"
    ],
    "neural turing machine*****content": [
        "$ARG1 , which is able to learn to read from and write arbitrary $ARG2"
    ],
    "memory*****attention": [
        "$ARG1 cells without explicit supervision about which actions to undertake , and allowed end-to-end training without this supervision signal , via the use of a content-based soft $ARG2"
    ],
    "standard*****other": [
        "$ARG1 with $ARG2"
    ],
    "problem*****memory": [
        "$ARG1 , NTMs actually read to or write from many $ARG2"
    ],
    "number*****example": [
        "$ARG1 of cells , for $ARG2",
        "$ARG1 of input features ( pixels ) per $ARG2",
        "$ARG1 of training examples , because the i-th $ARG2",
        "$ARG1 of examples , a typical grid cell has no training $ARG2",
        "$ARG1 of hidden units in a layer is one such $ARG2"
    ],
    "example*****softmax function": [
        "$ARG1 , by producing them via a $ARG2"
    ],
    "weights*****memory": [
        "$ARG1 with non-zero derivatives allows the functions controlling access to the $ARG2"
    ],
    "memory*****gradient descent": [
        "$ARG1 to be optimized using $ARG2"
    ],
    "gradient*****memory": [
        "$ARG1 will typically be large only for those $ARG2",
        "$ARG1 is unavailable , either due to its high computation and $ARG2"
    ],
    "memory*****vector": [
        "$ARG1 cells are typically augmented to contain a $ARG2"
    ],
    "scalar*****memory": [
        "$ARG1 stored by an LSTM or GRU $ARG2"
    ],
    "memory*****content-based addressing": [
        "$ARG1 cells is that they allow for $ARG2"
    ],
    "content-based addressing*****function": [
        "$ARG1 , where the weight used to read to or write from a cell is a $ARG2"
    ],
    "content-based addressing*****memory": [
        "$ARG1 is more useful when we make the objects to be retrieved large\u2014if every letter of the song was stored in a separate $ARG2"
    ],
    "memory*****information": [
        "$ARG1 cell is copied ( not forgotten ) at most time steps , then the $ARG2"
    ],
    "information*****contains": [
        "$ARG1 it $ARG2"
    ],
    "memory*****task": [
        "$ARG1 approach is illustrated in \ufb01gure 10.18 , where we see that a \u201c $ARG2",
        "$ARG1 to sequentially perform a $ARG2"
    ],
    "neural network*****system": [
        "$ARG1 could be feedforward or recurrent , the overall $ARG2"
    ],
    "back-propagation*****memory": [
        "$ARG1 through weighted averages of $ARG2"
    ],
    "stochastic*****back-propagation": [
        "$ARG1 architectures that make discrete decisions remains harder than training deterministic algorithms that make soft Whether it is soft ( allowing $ARG2"
    ],
    "back-propagation*****stochastic": [
        "$ARG1 ) or $ARG2"
    ],
    "attention*****machine translation": [
        "$ARG1 mechanism which had been previously introduced in the context of $ARG2"
    ],
    "attention*****neural network": [
        "$ARG1 mechanisms for $ARG2"
    ],
    "machine translation*****memory network": [
        "$ARG1 and $ARG2"
    ],
    "memory network*****attention": [
        "$ARG1 , at each step , the focus of $ARG2"
    ],
    "recurrent neural network*****deep learning": [
        "$ARG1 provide a way to extend $ARG2"
    ],
    "vector*****deep learning": [
        "$ARG1 , and that are easy for a person to do rapidly , can be accomplished via $ARG2"
    ],
    "other*****vector": [
        "$ARG1 tasks , that can not be described as associating one $ARG2",
        "$ARG1 in one-hot $ARG2",
        "$ARG1 words , the $ARG2"
    ],
    "vector*****task": [
        "$ARG1 to another , or that are di\ufb03cult enough that a person would require time to think and re\ufb02ect in order to accomplish the $ARG2"
    ],
    "task*****deep learning": [
        "$ARG1 , remain beyond the scope of $ARG2"
    ],
    "function*****deep learning": [
        "$ARG1 approximation technology that is behind nearly all modern practical applications of $ARG2"
    ],
    "regularization*****optimization": [
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2"
    ],
    "convolutional network*****recurrent neural network": [
        "$ARG1 for scaling to large images and the $ARG2"
    ],
    "optimization*****deep learning": [
        "$ARG1 for Training Deep $ARG2",
        "$ARG1 methods being employed routinely by $ARG2"
    ],
    "deep learning*****optimization": [
        "$ARG1 algorithms involve $ARG2",
        "$ARG1 algorithms involve $ARG2"
    ],
    "inference*****pca": [
        "$ARG1 in models such as $ARG2"
    ],
    "pca*****optimization problem": [
        "$ARG1 involves solving an $ARG2"
    ],
    "optimization problem*****deep learning": [
        "$ARG1 involved in $ARG2",
        "$ARG1 involved in $ARG2",
        "$ARG1 in $ARG2"
    ],
    "neural network*****problem": [
        "$ARG1 training $ARG2"
    ],
    "set*****optimization": [
        "$ARG1 of $ARG2"
    ],
    "optimization*****neural network": [
        "$ARG1 techniques for $ARG2",
        "$ARG1 : \ufb01nding the parameters \u03b8 of a $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 has focused on whether training arrives at a global minimum , a local minimum , or a saddle point , but in practice $ARG2",
        "$ARG1 methods for training $ARG2"
    ],
    "cost function*****measure": [
        "$ARG1 J ( \u03b8 ) , which typically includes a performance $ARG2"
    ],
    "measure*****set": [
        "$ARG1 evaluated on the entire training $ARG2",
        "$ARG1 P , that is de\ufb01ned with respect to the test $ARG2",
        "$ARG1 on the training $ARG2",
        "$ARG1 on the validation $ARG2"
    ],
    "optimization*****algorithm": [
        "$ARG1 used as a training $ARG2",
        "$ARG1 FOR TRAINING DEEP MODELS $ARG2",
        "$ARG1 FOR TRAINING DEEP MODELS $ARG2",
        "$ARG1 FOR TRAINING DEEP MODELS $ARG2",
        "$ARG1 FOR TRAINING DEEP MODELS $ARG2",
        "$ARG1 procedure over a restricted family of The \ufb01rst $ARG2"
    ],
    "task*****optimization": [
        "$ARG1 di\ufb00ers from pure $ARG2"
    ],
    "challenges*****optimization": [
        "$ARG1 that make $ARG2",
        "$ARG1 involved in $ARG2",
        "$ARG1 in $ARG2"
    ],
    "learning rate*****information": [
        "$ARG1 during training or leverage $ARG2"
    ],
    "information*****8": [
        "$ARG1 contained in CHAPTER $ARG2"
    ],
    "optimization*****second derivative": [
        "$ARG1 FOR TRAINING DEEP MODELS the $ARG2"
    ],
    "second derivative*****cost function": [
        "$ARG1 of the $ARG2"
    ],
    "optimization*****optimization algorithm": [
        "$ARG1 strategies that are formed by combining simple $ARG2",
        "$ARG1 FOR TRAINING DEEP MODELS solvable by an $ARG2",
        "$ARG1 setting , where an $ARG2",
        "$ARG1 FOR TRAINING DEEP MODELS for networks of practical interest and whether $ARG2",
        "$ARG1 Several theoretical results show that there are limits on the performance of any $ARG2",
        "$ARG1 FOR TRAINING DEEP MODELS and Bousquet ( 2008 ) argue that it therefore may not be worthwhile to pursue an $ARG2",
        "$ARG1 FOR TRAINING DEEP MODELS Some $ARG2",
        "$ARG1 , the best strategy is not always to improve the $ARG2",
        "$ARG1 is to design $ARG2"
    ],
    "machine learning*****measure": [
        "$ARG1 scenarios , we care about some performance $ARG2",
        "$ARG1 , we usually $ARG2"
    ],
    "contrast*****optimization": [
        "$ARG1 to pure $ARG2"
    ],
    "optimization algorithm*****machine learning": [
        "$ARG1 for training deep models also typically include some specialization on the speci\ufb01c structure of $ARG2",
        "$ARG1 for $ARG2",
        "$ARG1 therefore remains an important goal for $ARG2",
        "$ARG1 for $ARG2",
        "$ARG1 that converges faster than O ( k1 ) for $ARG2"
    ],
    "cost function*****average": [
        "$ARG1 can be written as an $ARG2"
    ],
    "average*****set": [
        "$ARG1 over the training $ARG2",
        "$ARG1 of the corresponding y values in the training $ARG2"
    ],
    "set*****loss function": [
        "$ARG1 , J ( \u03b8 ) = E ( x , y ) \u223cp\u02c6data L ( f ( x ; \u03b8 ) , y ) , where L is the per-example $ARG2"
    ],
    "loss function*****empirical distribution": [
        "$ARG1 , f ( x ; \u03b8 ) is the predicted output when the input is x , p\u0302data is the $ARG2"
    ],
    "regularization*****objective function": [
        "$ARG1 or Equation 8.1 de\ufb01nes an $ARG2",
        "$ARG1 consists of adding an extra term to the $ARG2",
        "$ARG1 term \u2126 ( \u03b8 ) = 12 \ue06bw\ue06b22 to the $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 strategies that modify the $ARG2"
    ],
    "objective function*****set": [
        "$ARG1 with respect to the training $ARG2",
        "$ARG1 value and $ARG2",
        "$ARG1 plus a $ARG2",
        "$ARG1 , or the $ARG2"
    ],
    "objective function*****expectation": [
        "$ARG1 where the $ARG2"
    ],
    "expectation*****data generating distribution": [
        "$ARG1 is taken across the $ARG2",
        "$ARG1 is over m training samples from the $ARG2"
    ],
    "data generating distribution*****set": [
        "$ARG1 p data rather than just over the \ufb01nite training $ARG2"
    ],
    "algorithm*****expected": [
        "$ARG1 is to reduce the $ARG2",
        "$ARG1 \u2019 s $ARG2"
    ],
    "distribution*****optimization": [
        "$ARG1 pdata ( x , y ) , risk minimization would be an $ARG2",
        "$ARG1 , however , does have a large e\ufb00ect on both the outcome of the $ARG2"
    ],
    "task*****8": [
        "$ARG1 CHAPTER $ARG2"
    ],
    "set*****machine learning": [
        "$ARG1 of samples , we have a $ARG2"
    ],
    "problem*****optimization problem": [
        "$ARG1 back into an $ARG2",
        "$ARG1 , a wide , \ufb02at region must consist entirely of global minima , but in a general $ARG2"
    ],
    "optimization problem*****expected": [
        "$ARG1 is to minimize the $ARG2"
    ],
    "loss*****set": [
        "$ARG1 on the training $ARG2",
        "$ARG1 often continues to decrease for a long time after the training $ARG2",
        "$ARG1 on the training $ARG2",
        "$ARG1 measured on a validation $ARG2",
        "$ARG1 L on a small test $ARG2"
    ],
    "distribution*****empirical distribution": [
        "$ARG1 p ( x , y ) with the $ARG2",
        "$ARG1 match the $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 is only necessary to de\ufb01ne the $ARG2"
    ],
    "empirical distribution*****set": [
        "$ARG1 p\u0302 ( x , y ) de\ufb01ned by the training $ARG2",
        "$ARG1 de\ufb01ned by the training $ARG2",
        "$ARG1 p\u0302data de\ufb01ned by the training $ARG2",
        "$ARG1 de\ufb01ned by the training $ARG2"
    ],
    "empirical risk*****number": [
        "$ARG1 L ( f ( x ( i ) ; \u03b8 ) , y ( i ) ) Ex , y\u223cp\u0302data ( x , y ) [ L ( f ( x ; \u03b8 ) , y ) ] = where m is the $ARG2"
    ],
    "process*****average": [
        "$ARG1 based on minimizing this $ARG2"
    ],
    "average*****empirical risk minimization": [
        "$ARG1 training error is known as $ARG2"
    ],
    "conditions*****expected": [
        "$ARG1 under which the true risk can be $ARG2"
    ],
    "optimization algorithm*****gradient descent": [
        "$ARG1 are based on $ARG2",
        "$ARG1 , such as $ARG2"
    ],
    "gradient descent*****loss function": [
        "$ARG1 , but many useful $ARG2"
    ],
    "loss function*****loss": [
        "$ARG1 , such as 0-1 $ARG2",
        "$ARG1 , such as 0-1 $ARG2",
        "$ARG1 that is easier to approximate than the true $ARG2"
    ],
    "loss*****derivative": [
        "$ARG1 , have no useful derivatives ( the $ARG2"
    ],
    "deep learning*****empirical risk minimization": [
        "$ARG1 , we rarely use $ARG2"
    ],
    "loss function*****early stopping": [
        "$ARG1 and $ARG2"
    ],
    "early stopping*****loss function": [
        "$ARG1 Sometimes , the $ARG2",
        "$ARG1 criterion is based on the true underlying $ARG2"
    ],
    "loss function*****classi\ufb01cation": [
        "$ARG1 we actually care about ( say $ARG2"
    ],
    "example*****expected": [
        "$ARG1 , exactly minimizing $ARG2"
    ],
    "expected*****loss": [
        "$ARG1 0-1 $ARG2",
        "$ARG1 0-1 $ARG2",
        "$ARG1 0-1 $ARG2"
    ],
    "loss*****dimension": [
        "$ARG1 is typically intractable ( exponential in the input $ARG2"
    ],
    "conditional probability*****model": [
        "$ARG1 of the classes , given the input , and if the $ARG2"
    ],
    "classi\ufb01cation*****8": [
        "$ARG1 error in CHAPTER $ARG2"
    ],
    "optimization*****loss function": [
        "$ARG1 FOR TRAINING DEEP MODELS In some cases , a surrogate $ARG2"
    ],
    "loss*****other": [
        "$ARG1 is zero , one can improve the robustness of the classi\ufb01er by further pushing the classes apart from each $ARG2"
    ],
    "information*****average": [
        "$ARG1 from the training data than would have been possible by simply minimizing the $ARG2"
    ],
    "average*****loss": [
        "$ARG1 0-1 $ARG2"
    ],
    "algorithm*****loss function": [
        "$ARG1 usually minimizes a surrogate $ARG2",
        "$ARG1 often decomposes as a sum over training examples of some per-example $ARG2"
    ],
    "convergence*****early stopping": [
        "$ARG1 criterion based on $ARG2"
    ],
    "set*****algorithm": [
        "$ARG1 , and is designed to cause the $ARG2",
        "$ARG1 of examples that the training $ARG2"
    ],
    "loss function*****optimization": [
        "$ARG1 still has large derivatives , which is very di\ufb00erent from the pure $ARG2"
    ],
    "optimization algorithm*****gradient": [
        "$ARG1 is considered to have converged when the $ARG2",
        "$ARG1 that use only $ARG2",
        "$ARG1 are designed to account for imperfections in the $ARG2",
        "$ARG1 de\ufb01ned by solving for where the $ARG2",
        "$ARG1 that use only the $ARG2"
    ],
    "minibatch*****machine learning": [
        "$ARG1 Algorithms One aspect of $ARG2"
    ],
    "machine learning*****optimization algorithm": [
        "$ARG1 algorithms that separates them from general $ARG2",
        "$ARG1 BASICS an $ARG2",
        "$ARG1 BASICS the imperfection of the $ARG2"
    ],
    "optimization algorithm*****objective function": [
        "$ARG1 is that the $ARG2",
        "$ARG1 o\ufb00er include that the $ARG2"
    ],
    "machine learning*****expected value": [
        "$ARG1 typically compute each update to the parameters based on an $ARG2"
    ],
    "expected value*****cost function": [
        "$ARG1 of the $ARG2"
    ],
    "cost function*****subset": [
        "$ARG1 estimated using only a $ARG2"
    ],
    "subset*****cost function": [
        "$ARG1 of the terms of the full $ARG2"
    ],
    "example*****maximum likelihood estimation": [
        "$ARG1 , $ARG2"
    ],
    "maximum likelihood estimation*****example": [
        "$ARG1 problems , when viewed in log space , decompose into a sum over each $ARG2"
    ],
    "expectation*****empirical distribution": [
        "$ARG1 over the $ARG2",
        "$ARG1 with respect to the $ARG2"
    ],
    "objective function*****optimization algorithm": [
        "$ARG1 J used by most of our $ARG2"
    ],
    "optimization algorithm*****set": [
        "$ARG1 are also expectations over the training $ARG2",
        "$ARG1 that use the entire training $ARG2"
    ],
    "example*****8": [
        "$ARG1 , the CHAPTER $ARG2"
    ],
    "optimization*****property": [
        "$ARG1 FOR TRAINING DEEP MODELS most commonly used $ARG2"
    ],
    "property*****gradient": [
        "$ARG1 is the $ARG2",
        "$ARG1 ; the log-likelihood $ARG2"
    ],
    "expectation*****model": [
        "$ARG1 exactly is very expensive because it requires evaluating the $ARG2"
    ],
    "number*****dataset": [
        "$ARG1 of examples from the $ARG2",
        "$ARG1 of examples in the $ARG2",
        "$ARG1 of data points m in our $ARG2",
        "$ARG1 of training iterations over the $ARG2",
        "$ARG1 of passes through the $ARG2",
        "$ARG1 of examples as the original $ARG2"
    ],
    "dataset*****average": [
        "$ARG1 , then taking the $ARG2"
    ],
    "recall*****standard error of the mean": [
        "$ARG1 that the $ARG2"
    ],
    "standard error of the mean*****\u03c3": [
        "$ARG1 ( equation 5.46 ) estimated from n $ARG2"
    ],
    "\u03c3*****standard deviation": [
        "$ARG1 is the true $ARG2"
    ],
    "optimization algorithm*****number": [
        "$ARG1 converge much faster ( in terms of total computation , not in terms of $ARG2"
    ],
    "number*****set": [
        "$ARG1 of samples is redundancy in the training $ARG2",
        "$ARG1 of iterations required to make a few hundred passes through the training $ARG2",
        "$ARG1 of examples in the test $ARG2",
        "$ARG1 of possible distinct con\ufb01gurations of a $ARG2",
        "$ARG1 of parameters , it will require an astronomically large training $ARG2",
        "$ARG1 of complete graphs for every $ARG2",
        "$ARG1 of times to observe worsening validation $ARG2",
        "$ARG1 of hidden units taken with the $ARG2"
    ],
    "gradient*****process": [
        "$ARG1 methods , because they $ARG2",
        "$ARG1 , the training $ARG2"
    ],
    "optimization algorithm*****example": [
        "$ARG1 that use only a single $ARG2"
    ],
    "deep learning*****8": [
        "$ARG1 fall somewhere in between , using more CHAPTER $ARG2"
    ],
    "method*****stochastic gradient descent": [
        "$ARG1 is $ARG2"
    ],
    "minibatch*****gradient": [
        "$ARG1 sizes are generally driven by the following factors : \u2022 Larger batches provide a more accurate estimate of the $ARG2",
        "$ARG1 of training examples to compute the $ARG2"
    ],
    "reduction*****process": [
        "$ARG1 in the time to $ARG2"
    ],
    "process*****minibatch": [
        "$ARG1 a $ARG2"
    ],
    "generalization*****1": [
        "$ARG1 error is often best for a batch size of $ARG2"
    ],
    "learning rate*****variance": [
        "$ARG1 to maintain stability due to the high $ARG2"
    ],
    "learning rate*****set": [
        "$ARG1 and because it takes more steps to observe the entire training $ARG2",
        "$ARG1 taken within the $ARG2"
    ],
    "information*****minibatch": [
        "$ARG1 from the $ARG2"
    ],
    "multiplication*****8": [
        "$ARG1 by CHAPTER $ARG2"
    ],
    "unbiased*****expected": [
        "$ARG1 estimate of the $ARG2"
    ],
    "gradient*****set": [
        "$ARG1 from a $ARG2",
        "$ARG1 of an entire training $ARG2",
        "$ARG1 suggests we could $ARG2"
    ],
    "dataset*****list": [
        "$ARG1 of medical data with a long $ARG2"
    ],
    "list*****dataset": [
        "$ARG1 , then each of our minibatches would be extremely biased , because it would represent primarily one patient out of the many patients in the $ARG2"
    ],
    "optimization problem*****machine learning": [
        "$ARG1 in $ARG2"
    ],
    "other*****minibatch": [
        "$ARG1 words , we can compute the update that minimizes J ( X ) for one $ARG2"
    ],
    "minibatch*****other": [
        "$ARG1 of examples X at the same time that we compute the update for several $ARG2"
    ],
    "gradient*****generalization": [
        "$ARG1 of the true $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "gradient*****8": [
        "$ARG1 CHAPTER $ARG2",
        "$ARG1 ( at interim point ) : g \u2190 m1 \u2207\u03b8\u0303 i L ( f ( x ( i ) ; \u03b8\u0303 ) , y ( i ) ) Compute velocity update : v \u2190 \u03b1v \u2212 \ue00fg Apply update : \u03b8 \u2190 \u03b8 + v CHAPTER $ARG2",
        "$ARG1 , or CHAPTER $ARG2",
        "$ARG1 and may CHAPTER $ARG2"
    ],
    "optimization*****dataset": [
        "$ARG1 FOR TRAINING DEEP MODELS descent shu\ufb04e the $ARG2"
    ],
    "dataset*****pass": [
        "$ARG1 once and then $ARG2"
    ],
    "pass*****multiple": [
        "$ARG1 through it $ARG2",
        "$ARG1 through $ARG2"
    ],
    "pass*****minibatch": [
        "$ARG1 , each $ARG2"
    ],
    "minibatch*****unbiased": [
        "$ARG1 is used to compute an $ARG2"
    ],
    "unbiased*****generalization": [
        "$ARG1 estimate of the true $ARG2"
    ],
    "pass*****data generating distribution": [
        "$ARG1 , the estimate becomes biased because it is formed by re-sampling values that have already been used , rather than obtaining new fair samples from the $ARG2"
    ],
    "stochastic gradient descent*****generalization": [
        "$ARG1 minimizes $ARG2"
    ],
    "generalization*****learning": [
        "$ARG1 error is easiest to see in the online $ARG2",
        "$ARG1 error of a learner , after the $ARG2",
        "$ARG1 error of a $ARG2",
        "$ARG1 error is to improve the $ARG2"
    ],
    "other*****set": [
        "$ARG1 words , instead of receiving a \ufb01xed-size training $ARG2",
        "$ARG1 , and that the train $ARG2",
        "$ARG1 vectors in the $ARG2",
        "$ARG1 to \ufb01t the training $ARG2"
    ],
    "example*****data generating distribution": [
        "$ARG1 ( x , y ) coming from the $ARG2"
    ],
    "generalization*****gradient": [
        "$ARG1 error ( equation 8.2 ) can be written as a sum J \u2217 ( \u03b8 ) = pdata ( x , y ) L ( f ( x ; \u03b8 ) , y ) , with the exact $ARG2"
    ],
    "generalization*****minibatch": [
        "$ARG1 error by sampling a $ARG2"
    ],
    "data generating distribution*****gradient": [
        "$ARG1 pdata , and computing the $ARG2"
    ],
    "loss*****minibatch": [
        "$ARG1 with respect to the parameters for that $ARG2"
    ],
    "multiple*****epoch": [
        "$ARG1 such epochs are used , only the \ufb01rst $ARG2"
    ],
    "epoch*****unbiased": [
        "$ARG1 follows the $ARG2"
    ],
    "generalization*****8": [
        "$ARG1 error , but CHAPTER $ARG2"
    ],
    "example*****pass": [
        "$ARG1 only once or even to make an incomplete $ARG2"
    ],
    "pass*****set": [
        "$ARG1 through the training $ARG2"
    ],
    "challenges*****neural network": [
        "$ARG1 in $ARG2"
    ],
    "optimization*****task": [
        "$ARG1 in general is an extremely di\ufb03cult $ARG2",
        "$ARG1 refers to the $ARG2"
    ],
    "optimization*****objective function": [
        "$ARG1 by carefully designing the $ARG2",
        "$ARG1 FOR TRAINING DEEP MODELS Figure 8.3 : The $ARG2"
    ],
    "objective function*****optimization problem": [
        "$ARG1 and constraints to ensure that the $ARG2"
    ],
    "hessian matrix*****problem": [
        "$ARG1 H. This is a very general $ARG2"
    ],
    "recall*****series": [
        "$ARG1 from equation 4.9 that a second-order Taylor $ARG2"
    ],
    "series*****cost function": [
        "$ARG1 expansion of the $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 approximating the $ARG2"
    ],
    "cost function*****gradient descent": [
        "$ARG1 predicts that a $ARG2",
        "$ARG1 becomes small and then 0 when we approach and reach a minimum using batch $ARG2",
        "$ARG1 , $ARG2"
    ],
    "neural network*****task": [
        "$ARG1 training $ARG2",
        "$ARG1 to replace GMMs for the $ARG2"
    ],
    "task*****gradient": [
        "$ARG1 , one can monitor the squared $ARG2",
        "$ARG1 as the primary output at the top of the network in order to ensure that the lower layers receive a large $ARG2"
    ],
    "norm*****8": [
        "$ARG1 g \ue03eg and CHAPTER $ARG2"
    ],
    "optimization*****gradient descent": [
        "$ARG1 FOR TRAINING DEEP MODELS 50 100 150 200 250 Training time ( epochs ) Training time ( epochs ) Figure 8.1 : $ARG2",
        "$ARG1 FOR TRAINING DEEP MODELS $ARG2",
        "$ARG1 FOR TRAINING DEEP MODELS We have previously introduced the $ARG2",
        "$ARG1 procedure , such as $ARG2"
    ],
    "norm*****convolutional network": [
        "$ARG1 increases throughout training of a $ARG2"
    ],
    "convolutional network*****object detection": [
        "$ARG1 used for $ARG2"
    ],
    "norm*****epoch": [
        "$ARG1 is plotted per $ARG2"
    ],
    "norm*****process": [
        "$ARG1 clearly increases over time , rather than decreasing as we would expect if the training $ARG2"
    ],
    "norm*****learning": [
        "$ARG1 does not shrink signi\ufb01cantly throughout $ARG2"
    ],
    "learning*****magnitude": [
        "$ARG1 , but the g\ue03eHg term grows by more than an order of $ARG2"
    ],
    "method*****neural network": [
        "$ARG1 requires signi\ufb01cant modi\ufb01cation before it can be applied to $ARG2",
        "$ARG1 in more detail , with emphasis on its application to $ARG2",
        "$ARG1 for training large $ARG2"
    ],
    "function*****solution": [
        "$ARG1 , we know that we have reached a good $ARG2"
    ],
    "multiple*****model identi\ufb01ability": [
        "$ARG1 local minima because of the $ARG2"
    ],
    "set*****rule": [
        "$ARG1 can $ARG2"
    ],
    "rule*****model": [
        "$ARG1 out all but one setting of the $ARG2"
    ],
    "weight space symmetry*****neural network": [
        "$ARG1 , many kinds of $ARG2"
    ],
    "example*****maxout": [
        "$ARG1 , in any recti\ufb01ed linear or $ARG2",
        "$ARG1 , we train a $ARG2"
    ],
    "maxout*****weights": [
        "$ARG1 network , we can scale all of the incoming $ARG2"
    ],
    "cost function*****weight decay": [
        "$ARG1 does not include terms such as $ARG2"
    ],
    "weights*****maxout": [
        "$ARG1 rather than the models \u2019 outputs\u2014every local minimum of a recti\ufb01ed linear or $ARG2"
    ],
    "maxout*****model identi\ufb01ability": [
        "$ARG1 network lies on an ( m \u00d7 n ) -dimensional hyperbola of equivalent local These $ARG2"
    ],
    "model identi\ufb01ability*****neural network": [
        "$ARG1 issues mean that there can be an extremely large or even uncountably in\ufb01nite amount of local minima in a $ARG2"
    ],
    "other*****cost function": [
        "$ARG1 in $ARG2"
    ],
    "problem*****8": [
        "$ARG1 for gradient-based It remains an open question whether there are many local minima of high cost CHAPTER $ARG2"
    ],
    "rule*****problem": [
        "$ARG1 out local minima as the $ARG2"
    ],
    "problem*****norm": [
        "$ARG1 is to plot the $ARG2"
    ],
    "problem*****other": [
        "$ARG1 is neither local minima nor any $ARG2",
        "$ARG1 , we add one $ARG2"
    ],
    "saddle points*****other": [
        "$ARG1 and $ARG2"
    ],
    "other*****gradient": [
        "$ARG1 Flat Regions For many high-dimensional non-convex functions , local minima ( and maxima ) are in fact rare compared to another kind of point with zero $ARG2",
        "$ARG1 kinds of points with zero $ARG2",
        "$ARG1 words , modern neural nets have been designed so that their local $ARG2",
        "$ARG1 words , the $ARG2"
    ],
    "expected*****number": [
        "$ARG1 ratio of the $ARG2"
    ],
    "number*****saddle points": [
        "$ARG1 of $ARG2"
    ],
    "saddle points*****hessian matrix": [
        "$ARG1 to local minima grows exponentially with n. To understand the intuition behind this behavior , observe that the $ARG2"
    ],
    "neural network*****saddle points": [
        "$ARG1 ? Baldi and Hornik ( 1989 ) showed theoretically that shallow autoencoders ( feedforward networks trained to copy their input to their output , described in chapter 14 ) with no nonlinearities have global minima and $ARG2"
    ],
    "loss function*****function": [
        "$ARG1 is a non-convex $ARG2",
        "$ARG1 , seen as a $ARG2"
    ],
    "loss function*****saddle points": [
        "$ARG1 that contain very many high-cost $ARG2"
    ],
    "saddle points*****optimization algorithm": [
        "$ARG1 for training algorithms ? For \ufb01rst-order $ARG2"
    ],
    "other*****gradient descent": [
        "$ARG1 hand , $ARG2"
    ],
    "gradient descent*****saddle points": [
        "$ARG1 empirically seems to be able to escape $ARG2",
        "$ARG1 is not attracted to $ARG2"
    ],
    "cost function*****weights": [
        "$ARG1 near a prominent saddle point where the $ARG2",
        "$ARG1 J with a quadratic approximation in the neighborhood of the empirically optimal value of the $ARG2"
    ],
    "weights*****gradient descent": [
        "$ARG1 are all zero , but they also show the $ARG2"
    ],
    "method*****saddle points": [
        "$ARG1 , it is clear that $ARG2"
    ],
    "saddle points*****problem": [
        "$ARG1 constitute a $ARG2"
    ],
    "feedforward neural network*****convolutional network": [
        "$ARG1 , $ARG2"
    ],
    "convolutional network*****recurrent network": [
        "$ARG1 , and $ARG2"
    ],
    "recurrent network*****object recognition": [
        "$ARG1 applied to real $ARG2"
    ],
    "object recognition*****natural language processing": [
        "$ARG1 and $ARG2"
    ],
    "hessian matrix*****path": [
        "$ARG1 in this region , or simply the need to circumnavigate the tall \u201c mountain \u201d visible in the \ufb01gure via an indirect arcing $ARG2"
    ],
    "saddle points*****gradient descent": [
        "$ARG1 in high dimensional spaces presumably explains why second-order methods have not succeeded in replacing $ARG2"
    ],
    "method*****optimization": [
        "$ARG1 for second-order $ARG2",
        "$ARG1 is an $ARG2"
    ],
    "gradient*****saddle points": [
        "$ARG1 besides minima and $ARG2"
    ],
    "saddle points*****method": [
        "$ARG1 from the perspective of optimization\u2014many algorithms are not attracted to them , but unmodi\ufb01ed Newton \u2019 s $ARG2",
        "$ARG1 , that are problematic for Newton \u2019 s $ARG2",
        "$ARG1 , the application of Newton \u2019 s $ARG2"
    ],
    "optimization problem*****objective function": [
        "$ARG1 , such a region could correspond to a high value of the $ARG2"
    ],
    "objective function*****neural network": [
        "$ARG1 for highly nonlinear deep $ARG2"
    ],
    "neural network*****recurrent neural network": [
        "$ARG1 or for $ARG2"
    ],
    "recurrent neural network*****contains": [
        "$ARG1 often $ARG2"
    ],
    "contains*****multiplication": [
        "$ARG1 sharp nonlinearities in parameter space resulting from the $ARG2"
    ],
    "gradient descent*****optimization": [
        "$ARG1 update can catapult the parameters very far , possibly losing most of the $ARG2",
        "$ARG1 is limited to $ARG2"
    ],
    "recall*****gradient": [
        "$ARG1 that the $ARG2"
    ],
    "algorithm*****gradient clipping": [
        "$ARG1 proposes to make a very large step , the $ARG2"
    ],
    "cost function*****recurrent neural network": [
        "$ARG1 for $ARG2"
    ],
    "recurrent neural network*****multiplication": [
        "$ARG1 , because such models involve a $ARG2"
    ],
    "optimization algorithm*****computational graph": [
        "$ARG1 must overcome arises when the $ARG2"
    ],
    "computational graph*****8": [
        "$ARG1 CHAPTER $ARG2"
    ],
    "optimization*****operation": [
        "$ARG1 FOR TRAINING DEEP MODELS by repeatedly applying the same $ARG2"
    ],
    "example*****computational graph": [
        "$ARG1 , suppose that a $ARG2"
    ],
    "contains*****path": [
        "$ARG1 a $ARG2",
        "$ARG1 many steps , so following the $ARG2",
        "$ARG1 an active $ARG2"
    ],
    "path*****matrix": [
        "$ARG1 that consists of repeatedly multiplying by a $ARG2"
    ],
    "absolute value*****1": [
        "$ARG1 of $ARG2"
    ],
    "1*****magnitude": [
        "$ARG1 in $ARG2",
        "$ARG1 in $ARG2",
        "$ARG1 in $ARG2",
        "$ARG1 % of the $ARG2"
    ],
    "problem*****graph": [
        "$ARG1 refers to the fact that gradients through such a $ARG2"
    ],
    "graph*****cost function": [
        "$ARG1 are also scaled according to diag ( \u03bb ) t. Vanishing gradients make it di\ufb03cult to know which direction the parameters should move to improve the $ARG2"
    ],
    "gradient clipping*****example": [
        "$ARG1 are an $ARG2"
    ],
    "multiplication*****method": [
        "$ARG1 by W at each time step described here is very similar to the power $ARG2"
    ],
    "algorithm*****eigenvalue": [
        "$ARG1 used to \ufb01nd the largest $ARG2"
    ],
    "eigenvalue*****matrix": [
        "$ARG1 of a $ARG2"
    ],
    "matrix*****eigenvector": [
        "$ARG1 W and the corresponding $ARG2",
        "$ARG1 V with one $ARG2"
    ],
    "view*****eigenvector": [
        "$ARG1 it is not surprising that x\ue03e W t will eventually discard all components of x that are orthogonal to the principal $ARG2"
    ],
    "recurrent network*****matrix": [
        "$ARG1 use the same $ARG2"
    ],
    "matrix*****deep feedforward network": [
        "$ARG1 W at each time step , but feedforward networks do not , so even very $ARG2"
    ],
    "challenges*****recurrent network": [
        "$ARG1 of training $ARG2"
    ],
    "optimization algorithm*****assumption": [
        "$ARG1 are designed with the $ARG2"
    ],
    "assumption*****gradient": [
        "$ARG1 that we have access to the exact $ARG2"
    ],
    "algorithm*****minibatch": [
        "$ARG1 relies on sampling-based estimates at least insofar as using a $ARG2",
        "$ARG1 , we can sample a $ARG2"
    ],
    "other*****objective function": [
        "$ARG1 cases , the $ARG2"
    ],
    "objective function*****gradient": [
        "$ARG1 is intractable , typically its $ARG2",
        "$ARG1 , such as its $ARG2",
        "$ARG1 may have issues such as poor conditioning or discontinuous gradients , causing the region where the $ARG2",
        "$ARG1 : J\u02dc ( w ; X , y ) = w\ue03e w + J ( w ; X , y ) , with the corresponding parameter $ARG2",
        "$ARG1 J\u02dc ( w ; X , y ) is given by J\u02dc ( w ; X , y ) = \u03b1||w||1 + J ( w ; X , y ) , with the corresponding $ARG2"
    ],
    "contrastive divergence*****technique": [
        "$ARG1 gives a $ARG2"
    ],
    "gradient*****boltzmann machine": [
        "$ARG1 of the intractable log-likelihood of a $ARG2"
    ],
    "correspondence*****loss function": [
        "$ARG1 between Local and Global Structure Many of the problems we have discussed so far correspond to properties of the $ARG2"
    ],
    "loss function*****gradient": [
        "$ARG1 at a single point\u2014it can be di\ufb03cult to make a single step if J ( \u03b8 ) is poorly conditioned at the current point \u03b8 , or if \u03b8 lies on a cli\ufb00 , or if \u03b8 is a saddle point hiding the opportunity to make progress downhill from the $ARG2",
        "$ARG1 , the $ARG2"
    ],
    "loss function*****model": [
        "$ARG1 \u2212 log p ( y | x ; \u03b8 ) can lack a global minimum point and instead asymptotically approach some value as the $ARG2",
        "$ARG1 is J ( w ) = Ex\u223cp\u0302data ||x \u2212 r ( x ; w ) ||22 while our $ARG2",
        "$ARG1 that encourages the $ARG2"
    ],
    "set*****learning": [
        "$ARG1 y targets , the $ARG2",
        "$ARG1 back to any arbitrary value \u03b2 ? The answer is that the new parametrization can represent the same family of functions of the input as the old parametrization , but the new parametrization has di\ufb00erent $ARG2",
        "$ARG1 of functions that the $ARG2",
        "$ARG1 of preferences into the $ARG2",
        "$ARG1 to zero\u2014-we are not allowed to begin the Bayesian $ARG2",
        "$ARG1 of mathematical tools used in variational $ARG2",
        "$ARG1 is poor , the $ARG2"
    ],
    "example*****optimization": [
        "$ARG1 of a failure of local $ARG2",
        "$ARG1 , there is a software defect preventing successful $ARG2"
    ],
    "learning*****8": [
        "$ARG1 trajectory and better characterize the outcome J ( \u03b8 ) CHAPTER $ARG2",
        "$ARG1 CHAPTER $ARG2"
    ],
    "optimization*****solution": [
        "$ARG1 based on local downhill moves can fail if the local surface does not point toward the global $ARG2"
    ],
    "example*****saddle points": [
        "$ARG1 of how this can occur , even if there are no $ARG2"
    ],
    "gradient descent*****learning": [
        "$ARG1 and essentially all $ARG2"
    ],
    "path*****solution": [
        "$ARG1 to a valid $ARG2",
        "$ARG1 to the $ARG2",
        "$ARG1 to the $ARG2",
        "$ARG1 to a $ARG2",
        "$ARG1 that moves downhill but away from any $ARG2"
    ],
    "solution*****path": [
        "$ARG1 , but we are not actually able to follow the local descent $ARG2",
        "$ARG1 , but the $ARG2",
        "$ARG1 by a $ARG2",
        "$ARG1 by a short $ARG2"
    ],
    "path*****8": [
        "$ARG1 incurs a CHAPTER $ARG2"
    ],
    "function*****method": [
        "$ARG1 has a wide \ufb02at region , or if we manage to land exactly on a critical point ( usually this latter scenario only happens to methods that solve explicitly for critical points , such as Newton \u2019 s $ARG2",
        "$ARG1 , Newton \u2019 s $ARG2",
        "$ARG1 is quadratic , the quadratic approximation employed by Newton \u2019 s $ARG2"
    ],
    "other*****path": [
        "$ARG1 cases , local moves can be too greedy and lead us along a $ARG2"
    ],
    "path*****learning": [
        "$ARG1 that local descent can follow , and if we are able to initialize $ARG2",
        "$ARG1 followed by the momentum $ARG2"
    ],
    "view*****optimization algorithm": [
        "$ARG1 suggests research into choosing good initial points for traditional $ARG2"
    ],
    "optimization algorithm*****neural network": [
        "$ARG1 we might design for $ARG2",
        "$ARG1 for deep $ARG2"
    ],
    "neural network*****optimization": [
        "$ARG1 units output smoothly increasing values that make $ARG2",
        "$ARG1 , though it is often bene\ufb01cial to initialize the $ARG2"
    ],
    "optimization*****local search": [
        "$ARG1 via $ARG2"
    ],
    "gradient descent*****algorithm": [
        "$ARG1 ( section 4.3 ) $ARG2",
        "$ARG1 visit points output of the Polyak averaging $ARG2",
        "$ARG1 is as a completely sequential $ARG2"
    ],
    "stochastic gradient descent*****optimization algorithm": [
        "$ARG1 ( SGD ) and its variants are probably the most used $ARG2"
    ],
    "gradient*****average gradient": [
        "$ARG1 by taking the $ARG2"
    ],
    "average gradient*****minibatch": [
        "$ARG1 on a $ARG2"
    ],
    "minibatch*****data generating distribution": [
        "$ARG1 of m examples drawn i.i.d from the $ARG2"
    ],
    "stochastic gradient descent*****learning rate": [
        "$ARG1 ( SGD ) update at training iteration k Require : $ARG2",
        "$ARG1 ( SGD ) with momentum Require : $ARG2"
    ],
    "gradient descent*****learning rate": [
        "$ARG1 can use a \ufb01xed $ARG2",
        "$ARG1 with a properly chosen $ARG2"
    ],
    "convergence*****8": [
        "$ARG1 of SGD is that \ue00fk = \u221e , CHAPTER $ARG2"
    ],
    "learning rate*****1": [
        "$ARG1 linearly until iteration \u03c4 : \ue00fk = ( $ARG2"
    ],
    "learning rate*****learning": [
        "$ARG1 may be chosen by trial and error , but it is usually best to choose it by monitoring $ARG2",
        "$ARG1 is too low , $ARG2",
        "$ARG1 is too low , $ARG2",
        "$ARG1 throughout the course of $ARG2"
    ],
    "learning*****objective function": [
        "$ARG1 curves that plot the $ARG2"
    ],
    "cost function*****dropout": [
        "$ARG1 arising from the use of $ARG2"
    ],
    "learning*****learning rate": [
        "$ARG1 proceeds slowly , and if the initial $ARG2"
    ],
    "property*****minibatch": [
        "$ARG1 of SGD and related $ARG2"
    ],
    "minibatch*****optimization": [
        "$ARG1 or online gradientbased $ARG2",
        "$ARG1 used for a single SGD step , but usually we get less than linear returns in terms of $ARG2"
    ],
    "convergence*****number": [
        "$ARG1 even when the $ARG2"
    ],
    "dataset*****set": [
        "$ARG1 , SGD may converge to within some \ufb01xed tolerance of its \ufb01nal test $ARG2",
        "$ARG1 will only be used for evaluation , we call it the test $ARG2",
        "$ARG1 into a \ufb01xed training $ARG2",
        "$ARG1 will require more parameter updates because the training $ARG2",
        "$ARG1 are found in the resulting training $ARG2",
        "$ARG1 : If you have high error on the training $ARG2"
    ],
    "rate*****optimization algorithm": [
        "$ARG1 of an $ARG2"
    ],
    "optimization algorithm*****measure": [
        "$ARG1 it is common to $ARG2"
    ],
    "measure*****cost function": [
        "$ARG1 the excess error J ( \u03b8 ) \u2212 min\u03b8 J ( \u03b8 ) , which is the amount that the current $ARG2"
    ],
    "gradient descent*****convergence": [
        "$ARG1 enjoys better $ARG2",
        "$ARG1 applied to convex problems , this approach has strong $ARG2"
    ],
    "convergence*****stochastic gradient descent": [
        "$ARG1 rates than $ARG2"
    ],
    "machine learning*****convergence": [
        "$ARG1 tasks\u2014faster $ARG2"
    ],
    "stochastic gradient descent*****number": [
        "$ARG1 has after a small $ARG2"
    ],
    "gradient*****convergence": [
        "$ARG1 for only very few examples outweighs its slow asymptotic $ARG2",
        "$ARG1 with respect to all variables will be zero at $ARG2"
    ],
    "matter*****constant": [
        "$ARG1 in practice but are lost in the $ARG2"
    ],
    "stochastic gradient descent*****minibatch": [
        "$ARG1 by gradually increasing the $ARG2"
    ],
    "minibatch*****learning": [
        "$ARG1 size during the course of $ARG2"
    ],
    "stochastic gradient descent*****optimization": [
        "$ARG1 remains a very popular $ARG2"
    ],
    "optimization*****learning": [
        "$ARG1 strategy , $ARG2"
    ],
    "method*****learning": [
        "$ARG1 of momentum ( Polyak , 1964 ) is designed to accelerate $ARG2",
        "$ARG1 of modifying a $ARG2",
        "$ARG1 for $ARG2"
    ],
    "algorithm*****average": [
        "$ARG1 accumulates an exponentially decaying moving $ARG2",
        "$ARG1 we can conceive of has the same $ARG2"
    ],
    "algorithm*****variable": [
        "$ARG1 introduces a $ARG2"
    ],
    "set*****average": [
        "$ARG1 to an exponentially decaying $ARG2",
        "$ARG1 implies statistical uncertainty around the estimated $ARG2"
    ],
    "algorithm*****vector": [
        "$ARG1 , we assume unit mass , so the velocity $ARG2",
        "$ARG1 returns the $ARG2",
        "$ARG1 as providing a k-dimensional one-hot code $ARG2"
    ],
    "hyperparameter*****1": [
        "$ARG1 \u03b1 \u2208 [ 0 , $ARG2",
        "$ARG1 \u03b1 using values of { \u22121 , 0 , $ARG2"
    ],
    "optimization*****hessian matrix": [
        "$ARG1 FOR TRAINING DEEP MODELS Figure 8.5 : Momentum aims primarily to solve two problems : poor conditioning of the $ARG2"
    ],
    "hessian matrix*****variance": [
        "$ARG1 and $ARG2"
    ],
    "variance*****stochastic": [
        "$ARG1 in the $ARG2"
    ],
    "loss function*****hessian matrix": [
        "$ARG1 with a poorly conditioned $ARG2"
    ],
    "rule*****function": [
        "$ARG1 as it minimizes this $ARG2",
        "$ARG1 : \u03b8\u2217 = \u03b80 \u2212 H\u22121 \u2207 \u03b8 J ( \u03b8 0 ) Thus for a locally quadratic $ARG2"
    ],
    "optimization*****norm": [
        "$ARG1 FOR TRAINING DEEP MODELS Previously , the size of the step was simply the $ARG2"
    ],
    "gradient*****hyperparameter": [
        "$ARG1 g , then it will accelerate in the direction of \u2212g , until reaching a terminal velocity where the size of each step is It is thus helpful to think of the momentum $ARG2"
    ],
    "hyperparameter*****example": [
        "$ARG1 in terms of 1\u2212\u03b1 $ARG2"
    ],
    "example*****gradient descent": [
        "$ARG1 , \u03b1 = .9 corresponds to multiplying the maximum speed by 10 relative to the $ARG2",
        "$ARG1 , using $ARG2"
    ],
    "gradient*****view": [
        "$ARG1 estimate : g \u2190 m1 \u2207 \u03b8 i L ( f ( x ( i ) ; \u03b8 ) , y ( i ) ) Compute velocity update : v \u2190 \u03b1v \u2212 \ue00fg Apply update : \u03b8 \u2190 \u03b8 + v We can $ARG2"
    ],
    "variable*****8": [
        "$ARG1 v ( t ) representing the velocity of the particle at time t and rewrite the Newtonian dynamics as a \ufb01rst-order di\ufb00erential equation : v ( t ) = \u03b8 ( t ) , CHAPTER $ARG2"
    ],
    "other*****drag": [
        "$ARG1 kinds of $ARG2"
    ],
    "drag*****other": [
        "$ARG1 based on $ARG2"
    ],
    "example*****drag": [
        "$ARG1 , a particle traveling through the air experiences turbulent $ARG2"
    ],
    "drag*****constant": [
        "$ARG1 , with force proportional to the square of the velocity , while a particle moving along the ground experiences dry friction , with a force of $ARG2"
    ],
    "drag*****distance": [
        "$ARG1 will move away from its initial position forever , with the $ARG2"
    ],
    "cost function*****constant": [
        "$ARG1 is small but non-zero , the $ARG2"
    ],
    "drag*****8": [
        "$ARG1 avoids both of these problems\u2014it is weak enough CHAPTER $ARG2"
    ],
    "standard*****method": [
        "$ARG1 momentum $ARG2"
    ],
    "nesterov momentum*****standard": [
        "$ARG1 and $ARG2",
        "$ARG1 as attempting to add a correction factor to the $ARG2"
    ],
    "standard*****gradient": [
        "$ARG1 momentum is where the $ARG2"
    ],
    "nesterov momentum*****gradient": [
        "$ARG1 the $ARG2"
    ],
    "gradient*****nesterov momentum": [
        "$ARG1 case , $ARG2",
        "$ARG1 case , $ARG2"
    ],
    "nesterov momentum*****rate": [
        "$ARG1 brings the $ARG2",
        "$ARG1 does not improve the $ARG2"
    ],
    "rate*****convergence": [
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2"
    ],
    "stochastic gradient descent*****nesterov momentum": [
        "$ARG1 ( SGD ) with $ARG2"
    ],
    "nesterov momentum*****learning rate": [
        "$ARG1 Require : $ARG2",
        "$ARG1 Require : Global $ARG2"
    ],
    "optimization algorithm*****solution": [
        "$ARG1 are not iterative by nature and simply solve for a $ARG2",
        "$ARG1 searches for an optimal $ARG2",
        "$ARG1 with a greedy $ARG2"
    ],
    "optimization problem*****initialization": [
        "$ARG1 , converge to acceptable solutions in an acceptable amount of time regardless of $ARG2"
    ],
    "task*****initialization": [
        "$ARG1 that most algorithms are strongly a\ufb00ected by the choice of $ARG2",
        "$ARG1 can sometimes yield an $ARG2"
    ],
    "initialization*****task": [
        "$ARG1 strategies is a di\ufb03cult $ARG2"
    ],
    "optimization*****generalization": [
        "$ARG1 but detrimental from the viewpoint of $ARG2",
        "$ARG1 but inadvertently increase $ARG2",
        "$ARG1 and in terms of $ARG2",
        "$ARG1 is that we want the $ARG2",
        "$ARG1 can , to some degree , reduce or prevent over\ufb01tting , and even points with equivalent training error can have di\ufb00erent $ARG2"
    ],
    "property*****symmetry": [
        "$ARG1 known with complete certainty is that the initial parameters need to \u201c break $ARG2"
    ],
    "model*****algorithm": [
        "$ARG1 or training $ARG2",
        "$ARG1 design is on freeing the training $ARG2",
        "$ARG1 , while they introduce a specialized $ARG2",
        "$ARG1 of the brain just based on the features that it learns ( though it can certainly be a bad sign if an $ARG2",
        "$ARG1 using a di\ufb00erent $ARG2",
        "$ARG1 , training $ARG2",
        "$ARG1 and many more iterations of the training $ARG2",
        "$ARG1 and $ARG2"
    ],
    "example*****dropout": [
        "$ARG1 , if one trains with $ARG2",
        "$ARG1 , if it is regularized with $ARG2",
        "$ARG1 by using $ARG2"
    ],
    "dropout*****function": [
        "$ARG1 ) , it is usually best to initialize each unit to compute a di\ufb00erent $ARG2"
    ],
    "forward propagation*****gradient": [
        "$ARG1 and no $ARG2"
    ],
    "function*****8": [
        "$ARG1 CHAPTER $ARG2"
    ],
    "optimization*****initialization": [
        "$ARG1 FOR TRAINING DEEP MODELS motivates random $ARG2"
    ],
    "search*****set": [
        "$ARG1 for a large $ARG2"
    ],
    "example*****matrix": [
        "$ARG1 , if we have at most as many outputs as inputs , we could use Gram-Schmidt orthogonalization on an initial weight $ARG2",
        "$ARG1 , each vertex in the same rigid object will be multiplied by the same $ARG2",
        "$ARG1 , consider a 3 \u00d7 2 $ARG2",
        "$ARG1 , if A is a $ARG2",
        "$ARG1 , if a $ARG2"
    ],
    "matrix*****function": [
        "$ARG1 , and be guaranteed that each unit computes a very di\ufb00erent $ARG2",
        "$ARG1 computed by applying the $ARG2",
        "$ARG1 involved is a $ARG2"
    ],
    "initialization*****distribution": [
        "$ARG1 from a high-entropy $ARG2"
    ],
    "variance*****set": [
        "$ARG1 of a prediction , are usually $ARG2",
        "$ARG1 of the output in the training $ARG2"
    ],
    "model*****uniform distribution": [
        "$ARG1 to values drawn randomly from a Gaussian or $ARG2"
    ],
    "uniform distribution*****matter": [
        "$ARG1 does not seem to $ARG2"
    ],
    "weights*****symmetry": [
        "$ARG1 will yield a stronger $ARG2"
    ],
    "back-propagation*****matrix": [
        "$ARG1 through the linear component of each layer\u2014larger values in the $ARG2"
    ],
    "matrix*****matrix multiplication": [
        "$ARG1 result in larger outputs of $ARG2",
        "$ARG1 , then both $ARG2"
    ],
    "weights*****forward propagation": [
        "$ARG1 that are too large may , however , result in exploding values during $ARG2"
    ],
    "forward propagation*****back-propagation": [
        "$ARG1 or $ARG2",
        "$ARG1 , $ARG2"
    ],
    "weights*****sensitivity": [
        "$ARG1 can also result in chaos ( such extreme $ARG2"
    ],
    "sensitivity*****forward propagation": [
        "$ARG1 to small perturbations of the input that the behavior of the deterministic $ARG2"
    ],
    "problem*****gradient clipping": [
        "$ARG1 can be mitigated by $ARG2"
    ],
    "weights*****activation function": [
        "$ARG1 may also result in extreme values that cause the $ARG2"
    ],
    "activation function*****loss": [
        "$ARG1 to saturate , causing complete $ARG2"
    ],
    "loss*****gradient": [
        "$ARG1 of $ARG2"
    ],
    "optimization*****weights": [
        "$ARG1 perspective suggests that the $ARG2",
        "$ARG1 FOR TRAINING DEEP MODELS increasing its $ARG2",
        "$ARG1 FOR TRAINING DEEP MODELS of one of the lower $ARG2"
    ],
    "information*****regularization": [
        "$ARG1 successfully , but some $ARG2",
        "$ARG1 through $ARG2"
    ],
    "optimization algorithm*****stochastic gradient descent": [
        "$ARG1 such as $ARG2",
        "$ARG1 called $ARG2"
    ],
    "stochastic gradient descent*****weights": [
        "$ARG1 that makes small incremental changes to the $ARG2"
    ],
    "optimization*****early stopping": [
        "$ARG1 FOR TRAINING DEEP MODELS due to triggering some $ARG2"
    ],
    "recall*****gradient descent": [
        "$ARG1 from section 7.8 that $ARG2"
    ],
    "gradient descent*****early stopping": [
        "$ARG1 with $ARG2",
        "$ARG1 with $ARG2"
    ],
    "early stopping*****weight decay": [
        "$ARG1 is equivalent to $ARG2",
        "$ARG1 is not the same as $ARG2",
        "$ARG1 therefore has the advantage over $ARG2"
    ],
    "weight decay*****thinking": [
        "$ARG1 , but does provide a loose analogy for $ARG2"
    ],
    "thinking*****initialization": [
        "$ARG1 about the e\ufb00ect of $ARG2"
    ],
    "weights*****normalized initialization": [
        "$ARG1 of a fully connected layer with m inputs and n outputs by sampling each weight from U ( \u2212 \u221a1m , \u221a1m ) , while Glorot and Bengio ( 2010 ) suggest using the $ARG2"
    ],
    "normalized initialization*****variance": [
        "$ARG1 Wi , j \u223c U \u2212 This latter heuristic is designed to compromise between the goal of initializing all layers to have the same activation $ARG2"
    ],
    "assumption*****matrix multiplication": [
        "$ARG1 that the network consists only of a chain of $ARG2"
    ],
    "neural network*****assumption": [
        "$ARG1 obviously violate this $ARG2"
    ],
    "assumption*****linear model": [
        "$ARG1 , but many strategies designed for the $ARG2"
    ],
    "initialization*****model": [
        "$ARG1 scheme is also motivated by a $ARG2"
    ],
    "model*****matrix": [
        "$ARG1 of a deep network as a sequence of $ARG2",
        "$ARG1 such inputs with a weight $ARG2"
    ],
    "model*****initialization": [
        "$ARG1 , this $ARG2"
    ],
    "initialization*****number": [
        "$ARG1 scheme guarantees that the total $ARG2"
    ],
    "number*****convergence": [
        "$ARG1 of training iterations required to reach $ARG2",
        "$ARG1 of updates required to reach $ARG2"
    ],
    "problem*****matrix": [
        "$ARG1 that arises when the same weight $ARG2"
    ],
    "weights*****hyperparameter": [
        "$ARG1 as a $ARG2",
        "$ARG1 for each layer as a $ARG2"
    ],
    "weights*****standard deviation": [
        "$ARG1 to have the same $ARG2"
    ],
    "initialization*****sparse initialization": [
        "$ARG1 scheme called $ARG2"
    ],
    "sparse initialization*****weights": [
        "$ARG1 in which each unit is initialized to have exactly k non-zero $ARG2"
    ],
    "magnitude*****sparse initialization": [
        "$ARG1 of individual weight elements shrink with m. $ARG2"
    ],
    "sparse initialization*****initialization": [
        "$ARG1 helps to achieve more diversity among the units at $ARG2"
    ],
    "gradient descent*****initialization": [
        "$ARG1 to shrink \u201c incorrect \u201d large values , this $ARG2"
    ],
    "initialization*****maxout": [
        "$ARG1 scheme can cause problems for units such as $ARG2"
    ],
    "algorithm*****random search": [
        "$ARG1 described in section 11.4.2 , such as $ARG2"
    ],
    "sparse initialization*****hyperparameter": [
        "$ARG1 can also be made a $ARG2"
    ],
    "rule*****standard deviation": [
        "$ARG1 of thumb for choosing the initial scales is to look at the range or $ARG2"
    ],
    "standard deviation*****minibatch": [
        "$ARG1 of activations or gradients on a single $ARG2"
    ],
    "weights*****minibatch": [
        "$ARG1 are too small , the range of activations across the $ARG2"
    ],
    "learning*****standard deviation": [
        "$ARG1 is still too slow at this point , it can be useful to look at the range or $ARG2"
    ],
    "principle*****hyperparameter optimization": [
        "$ARG1 be automated and is generally less computationally costly than $ARG2",
        "$ARG1 , to develop $ARG2"
    ],
    "hyperparameter optimization*****set": [
        "$ARG1 based on validation $ARG2"
    ],
    "initialization*****weights": [
        "$ARG1 of the $ARG2"
    ],
    "set*****bias": [
        "$ARG1 some biases to \u2022 If a $ARG2",
        "$ARG1 the $ARG2",
        "$ARG1 the $ARG2",
        "$ARG1 the $ARG2"
    ],
    "bias*****statistics": [
        "$ARG1 to obtain the right marginal $ARG2"
    ],
    "bias*****activation function": [
        "$ARG1 to the inverse of the $ARG2"
    ],
    "activation function*****statistics": [
        "$ARG1 applied to the marginal $ARG2"
    ],
    "distribution*****marginal probability": [
        "$ARG1 with the $ARG2"
    ],
    "marginal probability*****element": [
        "$ARG1 of class i given by $ARG2"
    ],
    "vector*****softmax": [
        "$ARG1 b by solving the equation $ARG2",
        "$ARG1 v : P ( y = y | v ) = $ARG2",
        "$ARG1 d : P ( y = y | v ; d ) = $ARG2"
    ],
    "softmax*****boltzmann machine": [
        "$ARG1 ( b ) = c. This applies not only to classi\ufb01ers but also to models we will encounter in Part III , such as autoencoders and $ARG2"
    ],
    "bias*****initialization": [
        "$ARG1 to avoid causing too much saturation at $ARG2",
        "$ARG1 of a ReLU hidden unit to 0.1 rather than 0 to avoid saturating the ReLU at $ARG2",
        "$ARG1 corrections to the estimates of both the \ufb01rst-order moments ( the momentum term ) and the ( uncentered ) second-order moments to account for their $ARG2"
    ],
    "example*****initialization": [
        "$ARG1 , it is not recommended for use with random walk $ARG2"
    ],
    "optimization*****view": [
        "$ARG1 FOR TRAINING DEEP MODELS can $ARG2",
        "$ARG1 that infers the most likely h. From this point of $ARG2"
    ],
    "1*****initialization": [
        "$ARG1 most of the time at $ARG2"
    ],
    "forget gate*****model": [
        "$ARG1 of the LSTM $ARG2"
    ],
    "linear regression*****variance": [
        "$ARG1 with a conditional $ARG2"
    ],
    "precision*****1": [
        "$ARG1 parameters to $ARG2"
    ],
    "set*****variance": [
        "$ARG1 the $ARG2",
        "$ARG1 may have too high $ARG2"
    ],
    "constant*****model": [
        "$ARG1 or random methods of initializing $ARG2"
    ],
    "model*****machine learning": [
        "$ARG1 parameters using $ARG2"
    ],
    "initialization*****convergence": [
        "$ARG1 that o\ufb00ers faster $ARG2",
        "$ARG1 strategies may yield faster $ARG2"
    ],
    "convergence*****initialization": [
        "$ARG1 than a random $ARG2"
    ],
    "convergence*****generalization": [
        "$ARG1 and better $ARG2"
    ],
    "generalization*****information": [
        "$ARG1 because they encode $ARG2",
        "$ARG1 can happen , transferring $ARG2"
    ],
    "information*****distribution": [
        "$ARG1 about the $ARG2",
        "$ARG1 from a $ARG2",
        "$ARG1 in the de\ufb01nition of the conditional $ARG2",
        "$ARG1 about the $ARG2",
        "$ARG1 THEORY The parameter \u00b5 still gives the mean of the $ARG2",
        "$ARG1 in an event drawn from that $ARG2"
    ],
    "neural network*****learning rate": [
        "$ARG1 researchers have long realized that the $ARG2",
        "$ARG1 models\u2014the accumulation of squared gradients from the beginning of training can result in a premature and excessive decrease in the e\ufb00ective $ARG2"
    ],
    "learning rate*****hyperparameters": [
        "$ARG1 was reliably one of the $ARG2"
    ],
    "hyperparameters*****set": [
        "$ARG1 that is the most di\ufb03cult to $ARG2",
        "$ARG1 is called the validation $ARG2",
        "$ARG1 , the validation $ARG2",
        "$ARG1 can be $ARG2",
        "$ARG1 , do not lose sight of your end goal : good performance on the test $ARG2",
        "$ARG1 having intrinsically non-di\ufb00erentiable interactions with the validation $ARG2"
    ],
    "algorithm*****hyperparameter": [
        "$ARG1 can mitigate these issues somewhat , but does so at the expense of introducing another $ARG2",
        "$ARG1 ( for ease of $ARG2",
        "$ARG1 from The goal of manual $ARG2",
        "$ARG1 runs training for every joint $ARG2",
        "$ARG1 then randomly samples joint $ARG2"
    ],
    "sensitivity*****learning": [
        "$ARG1 are somewhat axis-aligned , it can make sense to use a separate $ARG2"
    ],
    "optimization*****rate": [
        "$ARG1 FOR TRAINING DEEP MODELS $ARG2"
    ],
    "rate*****learning rate": [
        "$ARG1 for each parameter , and automatically adapt these $ARG2"
    ],
    "learning rate*****model": [
        "$ARG1 for $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of all $ARG2",
        "$ARG1 for each $ARG2",
        "$ARG1 requires monitoring both training and test error to diagnose whether your $ARG2",
        "$ARG1 , whether too high or too low , results in a $ARG2"
    ],
    "partial derivative*****loss": [
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "model*****learning rate": [
        "$ARG1 parameter , remains the same sign , then the $ARG2",
        "$ARG1 is highest when the $ARG2"
    ],
    "partial derivative*****learning rate": [
        "$ARG1 with respect to that parameter changes sign , then the $ARG2",
        "$ARG1 have a relatively small decrease in their $ARG2"
    ],
    "rule*****optimization": [
        "$ARG1 can only be applied to full batch $ARG2"
    ],
    "number*****learning rate": [
        "$ARG1 of incremental ( or mini-batch-based ) methods have been introduced that adapt the $ARG2",
        "$ARG1 of iterations and the $ARG2"
    ],
    "loss*****learning rate": [
        "$ARG1 have a correspondingly rapid decrease in their $ARG2"
    ],
    "learning rate*****partial derivative": [
        "$ARG1 , while parameters with small $ARG2"
    ],
    "convex optimization*****adagrad": [
        "$ARG1 , the $ARG2"
    ],
    "adagrad*****deep learning": [
        "$ARG1 performs well for some but not all $ARG2"
    ],
    "deep learning*****algorithm": [
        "$ARG1 The RMSProp $ARG2",
        "$ARG1 is powered by one very important $ARG2",
        "$ARG1 practitioner typically does not intend for the latent variables to take on any speci\ufb01c semantics ahead of time\u2014the training $ARG2"
    ],
    "algorithm*****adagrad": [
        "$ARG1 ( Hinton , 2012 ) modi\ufb01es $ARG2",
        "$ARG1 8.4 The $ARG2"
    ],
    "adagrad*****gradient": [
        "$ARG1 to perform better in the non-convex setting by changing the $ARG2"
    ],
    "adagrad*****function": [
        "$ARG1 is designed to converge rapidly when applied to a convex $ARG2"
    ],
    "learning*****pass": [
        "$ARG1 trajectory may $ARG2"
    ],
    "adagrad*****learning rate": [
        "$ARG1 shrinks the $ARG2"
    ],
    "learning rate*****gradient": [
        "$ARG1 according to the entire history of the squared $ARG2"
    ],
    "learning rate*****constant": [
        "$ARG1 \ue00f Require : Initial parameter \u03b8 Require : Small $ARG2"
    ],
    "constant*****gradient": [
        "$ARG1 \u03b4 , perhaps 10\u22127 , for numerical stability Initialize $ARG2",
        "$ARG1 factor on each step , just before performing the usual $ARG2"
    ],
    "variable*****minibatch": [
        "$ARG1 r = 0 while stopping criterion not met do Sample a $ARG2",
        "$ARG1 r = 0 while stopping criterion not met do Sample a $ARG2"
    ],
    "average*****adagrad": [
        "$ARG1 to discard history from the extreme past so that it can converge rapidly after \ufb01nding a convex bowl , as if it were an instance of the $ARG2"
    ],
    "algorithm*****nesterov momentum": [
        "$ARG1 8.5 and combined with $ARG2",
        "$ARG1 with $ARG2"
    ],
    "nesterov momentum*****algorithm": [
        "$ARG1 in $ARG2"
    ],
    "adagrad*****average": [
        "$ARG1 , the use of the moving $ARG2"
    ],
    "average*****hyperparameter": [
        "$ARG1 introduces a new $ARG2"
    ],
    "hyperparameter*****average": [
        "$ARG1 , \u03c1 , that controls the length scale of the moving $ARG2"
    ],
    "optimization algorithm*****algorithm": [
        "$ARG1 and is presented in $ARG2"
    ],
    "learning rate*****rate": [
        "$ARG1 \ue00f , decay $ARG2",
        "$ARG1 \ue00f , decay $ARG2"
    ],
    "constant*****minibatch": [
        "$ARG1 \u03b4 , usually 10\u22126 , used to stabilize division by small Initialize accumulation variables r = 0 while stopping criterion not met do Sample a $ARG2"
    ],
    "initialization*****algorithm": [
        "$ARG1 at the origin ( see $ARG2"
    ],
    "hyperparameters*****learning rate": [
        "$ARG1 , though the $ARG2"
    ],
    "optimization algorithm*****series": [
        "$ARG1 In this section , we discussed a $ARG2"
    ],
    "series*****challenge": [
        "$ARG1 of related algorithms that each seek to address the $ARG2"
    ],
    "challenge*****learning rate": [
        "$ARG1 of optimizing deep models by adapting the $ARG2"
    ],
    "number*****optimization algorithm": [
        "$ARG1 of $ARG2"
    ],
    "optimization algorithm*****learning": [
        "$ARG1 across a wide range of $ARG2",
        "$ARG1 , mean that the $ARG2"
    ],
    "learning rate*****algorithm": [
        "$ARG1 ( represented by RMSProp and AdaDelta ) performed fairly robustly , no single best $ARG2",
        "$ARG1 in \ufb01gure 4.6 , but even with the optimal step size the $ARG2"
    ],
    "algorithm*****8": [
        "$ARG1 to use , at this point , seems to depend CHAPTER $ARG2"
    ],
    "1*****algorithm": [
        "$ARG1 \u2212 \u03c1 ) g \ue00c g Compute velocity update : v \u2190 \u03b1v \u2212 \u221a\ue00fr \ue00c g. ( \u221a1r applied element-wise ) Apply update : \u03b8 \u2190 \u03b8 + v largely on the user \u2019 s familiarity with the $ARG2"
    ],
    "objective function*****regularization": [
        "$ARG1 that , for instance , include parameter $ARG2"
    ],
    "contrast*****second derivative": [
        "$ARG1 to \ufb01rstorder methods , second-order methods make use of $ARG2"
    ],
    "second derivative*****optimization": [
        "$ARG1 to improve $ARG2"
    ],
    "1*****bias": [
        "$ARG1 \u2212 \u03c1 2 ) g \ue00c g Correct $ARG2",
        "$ARG1 \u2212 \u03b8 ) m i=1 ( i ) ( \u03b8 ) \u2212 \u03b8 Since $ARG2",
        "$ARG1 \ue058 ( i ) To determine the $ARG2"
    ],
    "bias*****method": [
        "$ARG1 in second moment : r\u0302 \u2190 1\u2212\u03c1 Compute update : \u2206\u03b8 = \u2212\ue00f \u221a s\u0302 Apply update : \u03b8 \u2190 \u03b8 + \u2206\u03b8 ( operations applied element-wise ) Newton \u2019 s $ARG2"
    ],
    "optimization*****series": [
        "$ARG1 scheme based on using a second-order Taylor $ARG2"
    ],
    "function*****rule": [
        "$ARG1 , we obtain the Newton parameter update $ARG2"
    ],
    "function*****positive de\ufb01nite": [
        "$ARG1 ( with $ARG2"
    ],
    "positive de\ufb01nite*****gradient": [
        "$ARG1 H ) , by rescaling the $ARG2"
    ],
    "objective function*****algorithm": [
        "$ARG1 is convex but not quadratic ( there are higher-order terms ) , this update can be iterated , yielding the training $ARG2",
        "$ARG1 will never increase after one step of the $ARG2"
    ],
    "algorithm*****method": [
        "$ARG1 associated with Newton \u2019 s $ARG2",
        "$ARG1 includes occasional resets where the $ARG2",
        "$ARG1 has traditionally been cast as a batch $ARG2",
        "$ARG1 attempts to bring some of the advantages of Newton \u2019 s $ARG2",
        "$ARG1 computes the approximation M using the same $ARG2"
    ],
    "gradient*****positive de\ufb01nite": [
        "$ARG1 : g \u2190 m1 \u2207\u03b8 i L ( f ( x ( i ) ; \u03b8 ) , y ( i ) ) Compute Hessian : H \u2190 m \u22072\u03b8 i L ( f ( x ( i ) ; \u03b8 ) , y ( i ) ) Compute Hessian inverse : H \u22121 Compute update : \u2206\u03b8 = \u2212H \u22121 g Apply update : \u03b8 = \u03b8 + \u2206\u03b8 For surfaces that are not quadratic , as long as the Hessian remains $ARG2"
    ],
    "positive de\ufb01nite*****method": [
        "$ARG1 , Newton \u2019 s $ARG2",
        "$ARG1 quadratic , Newton \u2019 s $ARG2"
    ],
    "method*****positive de\ufb01nite": [
        "$ARG1 is appropriate only when the Hessian is $ARG2"
    ],
    "deep learning*****objective function": [
        "$ARG1 , the surface of the $ARG2",
        "$ARG1 models where the corresponding $ARG2"
    ],
    "objective function*****saddle points": [
        "$ARG1 is typically non-convex with many features , such as $ARG2"
    ],
    "regularization*****constant": [
        "$ARG1 strategies include adding a $ARG2"
    ],
    "constant*****diagonal": [
        "$ARG1 , \u03b1 , along the $ARG2"
    ],
    "regularization*****method": [
        "$ARG1 strategy is used in approximations to Newton \u2019 s $ARG2"
    ],
    "diagonal*****method": [
        "$ARG1 and the direction chosen by Newton \u2019 s $ARG2"
    ],
    "method*****standard": [
        "$ARG1 converges to the $ARG2"
    ],
    "method*****gradient descent": [
        "$ARG1 would make smaller steps than $ARG2",
        "$ARG1 is only appropriate when the nearby critical point is a minimum ( all the eigenvalues of the Hessian are positive ) , whereas $ARG2"
    ],
    "challenges*****objective function": [
        "$ARG1 created by certain features of the $ARG2"
    ],
    "objective function*****8": [
        "$ARG1 , CHAPTER $ARG2"
    ],
    "optimization*****saddle points": [
        "$ARG1 FOR TRAINING DEEP MODELS such as $ARG2"
    ],
    "number*****method": [
        "$ARG1 of parameters k can be in the millions ) , Newton \u2019 s $ARG2",
        "$ARG1 of parameters can be practically trained via Newton \u2019 s $ARG2",
        "$ARG1 of trials run by each $ARG2"
    ],
    "method of steepest descent*****line search": [
        "$ARG1 ( see section 4.3 for details ) , where $ARG2"
    ],
    "line search*****gradient": [
        "$ARG1 are applied iteratively in the direction associated with the $ARG2",
        "$ARG1 direction , when given by the $ARG2",
        "$ARG1 along the unaltered $ARG2"
    ],
    "gradient*****search": [
        "$ARG1 , is guaranteed to be orthogonal to the previous line Let the previous $ARG2",
        "$ARG1 at this point de\ufb01nes the current $ARG2"
    ],
    "line search*****directional derivative": [
        "$ARG1 terminates , the $ARG2"
    ],
    "multiple*****steepest descent": [
        "$ARG1 iterations of $ARG2"
    ],
    "gradient*****line search": [
        "$ARG1 at the end of each $ARG2",
        "$ARG1 into the tangent space of the feasible region before taking the step or beginning the $ARG2"
    ],
    "method*****search": [
        "$ARG1 of conjugate gradients , we seek to \ufb01nd a $ARG2"
    ],
    "search*****line search": [
        "$ARG1 direction that is conjugate to the previous $ARG2",
        "$ARG1 direction : \u03c1t = \u2212gt + \u03b2t\u03c1 t\u22121 Perform $ARG2"
    ],
    "search*****8": [
        "$ARG1 direction dt takes CHAPTER $ARG2"
    ],
    "optimization*****method of steepest descent": [
        "$ARG1 FOR TRAINING DEEP MODELS Figure 8.6 : The $ARG2"
    ],
    "method of steepest descent*****gradient": [
        "$ARG1 involves jumping to the point of lowest cost along the line de\ufb01ned by the $ARG2"
    ],
    "magnitude*****search": [
        "$ARG1 controls how much of the direction , dt\u22121 , we should add back to the current $ARG2"
    ],
    "gradient*****magnitude": [
        "$ARG1 along the previous direction does not increase in $ARG2"
    ],
    "method*****line search": [
        "$ARG1 requires at most k $ARG2",
        "$ARG1 of conjugate gradients is restarted with $ARG2",
        "$ARG1 of conjugate gradients , this procedure remains well behaved when the minimum of the $ARG2"
    ],
    "method*****set": [
        "$ARG1 Require : Initial parameters \u03b8 0 Require : Training $ARG2"
    ],
    "multiple*****constant": [
        "$ARG1 of some $ARG2"
    ],
    "constant*****search": [
        "$ARG1 k , such as k = 5 ) Compute $ARG2"
    ],
    "line search*****cost function": [
        "$ARG1 to \ufb01nd : \ue00f\u2217 = argmin\ue00f m i=1 L ( f ( x ; \u03b8 t + \ue00f\u03c1t ) , y ) ( On a truly quadratic $ARG2"
    ],
    "cost function*****method": [
        "$ARG1 , analytically solve for \ue00f\u2217 rather than explicitly searching for it ) Apply update : \u03b8t+1 = \u03b8t + \ue00f\u2217 \u03c1t Nonlinear Conjugate Gradients : So far we have discussed the $ARG2"
    ],
    "method*****objective function": [
        "$ARG1 of conjugate gradients as it is applied to quadratic $ARG2"
    ],
    "other*****deep learning": [
        "$ARG1 related $ARG2",
        "$ARG1 types of applications of $ARG2",
        "$ARG1 similarly generic assumptions can further improve $ARG2"
    ],
    "optimization*****stochastic gradient descent": [
        "$ARG1 with a few iterations of $ARG2",
        "$ARG1 procedure like $ARG2"
    ],
    "method*****minibatch": [
        "$ARG1 , $ARG2"
    ],
    "minibatch*****neural network": [
        "$ARG1 versions have been used successfully for the training of $ARG2"
    ],
    "quasi-newton methods*****algorithm": [
        "$ARG1 ( of which the BFGS $ARG2"
    ],
    "algorithm*****matrix": [
        "$ARG1 is the most prominent ) is to approximate the inverse with a $ARG2",
        "$ARG1 , for grouping the units into layers with a $ARG2"
    ],
    "derivation*****optimization": [
        "$ARG1 of the BFGS approximation is given in many textbooks on $ARG2"
    ],
    "mt*****line search": [
        "$ARG1 is updated , the direction of descent \u03c1t is determined by \u03c1t = Mtg t. A $ARG2"
    ],
    "algorithm*****series": [
        "$ARG1 iterates a $ARG2"
    ],
    "series*****line search": [
        "$ARG1 of $ARG2"
    ],
    "line search*****information": [
        "$ARG1 with the direction incorporating second-order $ARG2"
    ],
    "optimization*****line search": [
        "$ARG1 FOR TRAINING DEEP MODELS unlike conjugate gradients , the success of the approach is not heavily dependent on the $ARG2"
    ],
    "algorithm*****hessian matrix": [
        "$ARG1 must store the inverse $ARG2"
    ],
    "hessian matrix*****memory": [
        "$ARG1 , M , that requires O ( n2 ) $ARG2"
    ],
    "memory*****deep learning": [
        "$ARG1 , making BFGS impractical for most modern $ARG2",
        "$ARG1 and to produce the answer given the contents of the $ARG2"
    ],
    "deep learning*****memory": [
        "$ARG1 models that typically have millions of Limited $ARG2",
        "$ARG1 \u2022 $ARG2"
    ],
    "algorithm*****assumption": [
        "$ARG1 , but beginning with the $ARG2"
    ],
    "assumption*****identity matrix": [
        "$ARG1 that M ( t\u22121 ) is the $ARG2"
    ],
    "batch normalization*****neural network": [
        "$ARG1 ( Io\ufb00e and Szegedy , 2015 ) is one of the most exciting recent innovations in optimizing deep $ARG2"
    ],
    "neural network*****optimization algorithm": [
        "$ARG1 and it is actually not an $ARG2"
    ],
    "gradient*****assumption": [
        "$ARG1 tells how to update each parameter , under the $ARG2"
    ],
    "assumption*****other": [
        "$ARG1 that the $ARG2",
        "$ARG1 that the $ARG2",
        "$ARG1 that Bob \u2019 s personal running time is independent from all $ARG2",
        "$ARG1 that some $ARG2"
    ],
    "other*****constant": [
        "$ARG1 functions remain $ARG2"
    ],
    "optimization*****example": [
        "$ARG1 FOR TRAINING DEEP MODELS $ARG2",
        "$ARG1 as a simple $ARG2"
    ],
    "neural network*****activation function": [
        "$ARG1 that has only one unit per layer and does not use an $ARG2",
        "$ARG1 with nonlinear $ARG2",
        "$ARG1 layers take the form of \u03c6 ( XW + b ) where \u03c6 is some \ufb01xed nonlinear $ARG2",
        "$ARG1 re\ufb02ect a design choice to use linear transformations between layers and $ARG2"
    ],
    "set*****learning rate": [
        "$ARG1 the $ARG2"
    ],
    "learning rate*****other": [
        "$ARG1 , because the e\ufb00ects of an update to the parameters for one layer depends so strongly on all of the $ARG2"
    ],
    "batch normalization*****hidden layer": [
        "$ARG1 can be applied to any input or $ARG2"
    ],
    "row*****matrix": [
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "vector*****\u03c3": [
        "$ARG1 containing the mean of each unit and $ARG2"
    ],
    "vector*****standard deviation": [
        "$ARG1 containing the $ARG2"
    ],
    "broadcasting*****vector": [
        "$ARG1 the $ARG2"
    ],
    "\u03c3*****row": [
        "$ARG1 to be applied to every $ARG2"
    ],
    "row*****8": [
        "$ARG1 , the arithmetic is element-wise , so Hi , j is normalized by subtracting \u00b5j CHAPTER $ARG2"
    ],
    "standard deviation*****gradient": [
        "$ARG1 , and for applying them to normalize H. This means that the $ARG2",
        "$ARG1 or mean of h i ; the normalization operations remove the e\ufb00ect of such an action and zero out its component in the $ARG2"
    ],
    "operation*****standard deviation": [
        "$ARG1 that acts simply to increase the $ARG2"
    ],
    "statistics*****gradient descent": [
        "$ARG1 after each $ARG2"
    ],
    "batch normalization*****model": [
        "$ARG1 reparametrizes the $ARG2",
        "$ARG1 has thus made this $ARG2",
        "$ARG1 , reparametrizes the $ARG2"
    ],
    "example*****\u03c3": [
        "$ARG1 , without needing to use de\ufb01nitions of \u00b5 and $ARG2"
    ],
    "\u03c3*****minibatch": [
        "$ARG1 that depend on an entire $ARG2"
    ],
    "batch normalization*****variance": [
        "$ARG1 , we obtain the normalized h\u0302 l\u22121 that restores the zero mean and unit $ARG2",
        "$ARG1 acts to standardize only the mean and $ARG2"
    ],
    "weights*****8": [
        "$ARG1 to 0 can make the output become degenerate , and changing the sign CHAPTER $ARG2"
    ],
    "variance*****learning": [
        "$ARG1 of each unit in order to stabilize $ARG2"
    ],
    "statistics*****change": [
        "$ARG1 of a single unit to $ARG2"
    ],
    "standard deviation*****batch normalization": [
        "$ARG1 of each individual unit , and so far $ARG2"
    ],
    "batch normalization*****standard deviation": [
        "$ARG1 remains the most Normalizing the mean and $ARG2"
    ],
    "standard deviation*****neural network": [
        "$ARG1 of a unit can reduce the expressive power of the $ARG2"
    ],
    "variable*****standard deviation": [
        "$ARG1 to have any mean and $ARG2"
    ],
    "batch normalization*****8": [
        "$ARG1 to the input X , or to the transformed value XW + b. Io\ufb00e and Szegedy ( 2015 ) recommend CHAPTER $ARG2"
    ],
    "bias*****batch normalization": [
        "$ARG1 term should be omitted because it becomes redundant with the \u03b2 parameter applied by the $ARG2"
    ],
    "convolutional network*****\u03c3": [
        "$ARG1 , described in chapter 9 , it is important to apply the same normalizing \u00b5 and $ARG2"
    ],
    "\u03c3*****feature": [
        "$ARG1 at every spatial location within a $ARG2"
    ],
    "feature*****statistics": [
        "$ARG1 map , so that the $ARG2"
    ],
    "statistics*****feature": [
        "$ARG1 of the $ARG2"
    ],
    "coordinate descent*****subset": [
        "$ARG1 refers to minimizing with respect to a $ARG2"
    ],
    "coordinate descent*****optimization problem": [
        "$ARG1 makes the most sense when the di\ufb00erent variables in the $ARG2"
    ],
    "optimization problem*****optimization": [
        "$ARG1 can be clearly separated into groups that play relatively isolated roles , or when $ARG2"
    ],
    "optimization*****group": [
        "$ARG1 with respect to one $ARG2"
    ],
    "group*****optimization": [
        "$ARG1 of variables is signi\ufb01cantly more e\ufb03cient than $ARG2"
    ],
    "problem*****sparse coding": [
        "$ARG1 called $ARG2"
    ],
    "sparse coding*****matrix": [
        "$ARG1 , where the goal is to \ufb01nd a weight $ARG2"
    ],
    "matrix*****set": [
        "$ARG1 of activation values H to reconstruct the training $ARG2",
        "$ARG1 with m rows , we will describe it as a $ARG2",
        "$ARG1 must contain at least one $ARG2",
        "$ARG1 with more than m columns may have more than one such $ARG2",
        "$ARG1 into a $ARG2"
    ],
    "sparse coding*****weight decay": [
        "$ARG1 also involve $ARG2",
        "$ARG1 also involve $ARG2"
    ],
    "weight decay*****constraint": [
        "$ARG1 or a $ARG2",
        "$ARG1 or a $ARG2"
    ],
    "constraint*****solution": [
        "$ARG1 on the norms of the columns of W , in order to prevent the pathological $ARG2",
        "$ARG1 on the norms of the columns of W , in order to prevent the pathological $ARG2",
        "$ARG1 is not active , then the $ARG2",
        "$ARG1 would remain at least a local $ARG2"
    ],
    "algorithm*****dictionary": [
        "$ARG1 into two sets : the $ARG2"
    ],
    "objective function*****problem": [
        "$ARG1 with respect to either one of these sets of variables is a convex $ARG2"
    ],
    "coordinate descent*****8": [
        "$ARG1 thus gives CHAPTER $ARG2"
    ],
    "optimization*****convex optimization": [
        "$ARG1 strategy that allows us to use e\ufb03cient $ARG2",
        "$ARG1 is $ARG2"
    ],
    "coordinate descent*****variable": [
        "$ARG1 is not a very good strategy when the value of one $ARG2",
        "$ARG1 will make very slow progress because the \ufb01rst term does not allow a single $ARG2"
    ],
    "variable*****function": [
        "$ARG1 , as in the $ARG2"
    ],
    "function*****constant": [
        "$ARG1 f ( x ) = \ue000 2 the 2optimal ( x1 \u2212 x2 ) + \u03b1 x1 + x 2 where \u03b1 is a positive $ARG2",
        "$ARG1 that is closer to being $ARG2",
        "$ARG1 should be smooth or locally $ARG2"
    ],
    "solution*****set": [
        "$ARG1 is to $ARG2"
    ],
    "problem*****positive de\ufb01nite": [
        "$ARG1 in a single step because it is a $ARG2"
    ],
    "positive de\ufb01nite*****problem": [
        "$ARG1 quadratic $ARG2"
    ],
    "optimization*****1": [
        "$ARG1 \u03b8 ( $ARG2",
        "$ARG1 FOR TRAINING DEEP MODELS U ( $ARG2",
        "$ARG1 L [ p ] = \u03bb1 p ( x ) dx \u2212 $ARG2"
    ],
    "path*****optimization": [
        "$ARG1 taken by the $ARG2"
    ],
    "task*****greedy algorithm": [
        "$ARG1 are collectively known as $ARG2"
    ],
    "greedy algorithm*****problem": [
        "$ARG1 break a $ARG2"
    ],
    "greedy algorithm*****solution": [
        "$ARG1 can be computationally much cheaper than algorithms that solve for the best joint $ARG2"
    ],
    "solution*****quality": [
        "$ARG1 , and the $ARG2",
        "$ARG1 can greatly speed it up and improve the $ARG2"
    ],
    "quality*****solution": [
        "$ARG1 of a greedy $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "greedy algorithm*****optimization algorithm": [
        "$ARG1 may also be followed by a \ufb01ne-tuning stage in which a joint $ARG2"
    ],
    "solution*****pretraining": [
        "$ARG1 it $ARG2"
    ],
    "pretraining*****deep learning": [
        "$ARG1 , algorithms are ubiquitous in $ARG2"
    ],
    "supervised learning*****other": [
        "$ARG1 problems into $ARG2"
    ],
    "other*****supervised learning": [
        "$ARG1 simpler $ARG2",
        "$ARG1 Simple $ARG2"
    ],
    "greedy supervised pretraining*****supervised learning": [
        "$ARG1 , each stage consists of a $ARG2"
    ],
    "supervised learning*****task": [
        "$ARG1 training $ARG2"
    ],
    "example*****greedy supervised pretraining": [
        "$ARG1 of $ARG2"
    ],
    "greedy supervised pretraining*****hidden layer": [
        "$ARG1 is illustrated in \ufb01gure 8.7 , in which each added $ARG2"
    ],
    "pretraining*****convolutional network": [
        "$ARG1 one layer at a time , Simonyan and Zisserman ( 2015 ) pretrain a deep $ARG2"
    ],
    "convolutional network*****weights": [
        "$ARG1 ( eleven weight layers ) and then use the \ufb01rst four and last three layers from this network to initialize even deeper networks ( with up to nineteen layers of $ARG2",
        "$ARG1 ( Sainath et al. , 2013 ) that replicate $ARG2",
        "$ARG1 , however , typically have sparse interactions ( also referred to as sparse connectivity or sparse $ARG2"
    ],
    "illustration*****greedy supervised pretraining": [
        "$ARG1 of one form of $ARG2"
    ],
    "optimization*****process": [
        "$ARG1 , we can jointly \ufb01ne-tune all the layers , either only at the end or at each stage of this $ARG2"
    ],
    "pretraining*****optimization": [
        "$ARG1 may help both in terms of $ARG2"
    ],
    "8*****weights": [
        "$ARG1 layers of $ARG2"
    ],
    "set*****subset": [
        "$ARG1 of tasks ( a $ARG2",
        "$ARG1 of tasks ( another $ARG2",
        "$ARG1 encountered by each sub-network is indeed a $ARG2"
    ],
    "subset*****set": [
        "$ARG1 of the 1000 ImageNet object categories ) , with fewer training examples than for the \ufb01rst $ARG2",
        "$ARG1 is our validation $ARG2",
        "$ARG1 of data used to learn the parameters is still typically called the training $ARG2",
        "$ARG1 of the data is used as the test $ARG2",
        "$ARG1 of the parameters ( the unique $ARG2",
        "$ARG1 of the original training $ARG2"
    ],
    "other*****transfer learning": [
        "$ARG1 approaches to $ARG2"
    ],
    "transfer learning*****neural network": [
        "$ARG1 with $ARG2"
    ],
    "set*****hidden layer": [
        "$ARG1 of hints about how the $ARG2"
    ],
    "hidden layer*****optimization problem": [
        "$ARG1 should be used and can simplify the $ARG2"
    ],
    "classi\ufb01cation*****hidden layer": [
        "$ARG1 target , the objective is to predict the middle $ARG2"
    ],
    "optimization*****problem": [
        "$ARG1 techniques or changes in the architecture may also solve the $ARG2"
    ],
    "principle*****activation function": [
        "$ARG1 , we could use $ARG2"
    ],
    "stochastic gradient descent*****neural network": [
        "$ARG1 with momentum , which was used to train $ARG2"
    ],
    "activation function*****almost everywhere": [
        "$ARG1 that are di\ufb00erentiable $ARG2"
    ],
    "almost everywhere*****slope": [
        "$ARG1 and have signi\ufb01cant $ARG2"
    ],
    "recti\ufb01ed linear unit*****maxout": [
        "$ARG1 and $ARG2"
    ],
    "gradient*****singular value": [
        "$ARG1 \ufb02ows through many layers provided that the Jacobian of the linear transformation has reasonable $ARG2"
    ],
    "information*****solution": [
        "$ARG1 corresponds reasonably well to moving toward a distant $ARG2"
    ],
    "path*****gradient": [
        "$ARG1 from the lower layer \u2019 s parameters to the output , and thus mitigate the vanishing $ARG2"
    ],
    "change*****8": [
        "$ARG1 the architecture , so that intermediate layers ( especially the lower ones ) can get some hints about what they CHAPTER $ARG2"
    ],
    "optimization*****path": [
        "$ARG1 FOR TRAINING DEEP MODELS should do , via a shorter $ARG2"
    ],
    "continuation methods*****curriculum learning": [
        "$ARG1 and $ARG2"
    ],
    "curriculum learning*****challenges": [
        "$ARG1 As argued in section 8.2.7 , many of the $ARG2"
    ],
    "path*****continuation methods": [
        "$ARG1 through parameter space that local descent can $ARG2"
    ],
    "continuation methods*****optimization": [
        "$ARG1 are a family of strategies that can make $ARG2"
    ],
    "continuation methods*****series": [
        "$ARG1 is to construct a $ARG2"
    ],
    "series*****objective function": [
        "$ARG1 of $ARG2"
    ],
    "cost function*****process": [
        "$ARG1 motivating the entire $ARG2"
    ],
    "initialization*****cost function": [
        "$ARG1 is more likely to land in the region where local descent can minimize the $ARG2"
    ],
    "cost function*****solution": [
        "$ARG1 are designed so that a $ARG2",
        "$ARG1 has an analytical $ARG2"
    ],
    "continuation methods*****neural network": [
        "$ARG1 for $ARG2"
    ],
    "neural network*****objective function": [
        "$ARG1 training ) are usually based on smoothing the $ARG2"
    ],
    "continuation methods*****challenge": [
        "$ARG1 traditionally were mostly designed with the goal of overcoming the $ARG2"
    ],
    "continuation methods*****cost function": [
        "$ARG1 would construct easier $ARG2"
    ],
    "operation*****\u03c3": [
        "$ARG1 can be done by approximating J ( i ) ( \u03b8 ) = E\u03b8\ue030 \u223cN ( \u03b8 \ue030 ; \u03b8 , $ARG2"
    ],
    "information*****problem": [
        "$ARG1 about the location of a global minimum that we can \ufb01nd the global minimum by solving progressively less blurred versions of the $ARG2"
    ],
    "optimization problem*****continuation methods": [
        "$ARG1 remain NP-hard , even when $ARG2"
    ],
    "other*****continuation methods": [
        "$ARG1 two ways that $ARG2"
    ],
    "continuation methods*****method": [
        "$ARG1 fail both correspond to the $ARG2"
    ],
    "function*****matter": [
        "$ARG1 might not become convex , no $ARG2"
    ],
    "function*****track": [
        "$ARG1 may $ARG2"
    ],
    "track*****cost function": [
        "$ARG1 to a local rather than a global minimum of the original $ARG2"
    ],
    "continuation methods*****problem": [
        "$ARG1 were mostly originally designed to deal with the $ARG2"
    ],
    "objective function*****method": [
        "$ARG1 introduced by the continuation $ARG2"
    ],
    "method*****variance": [
        "$ARG1 can eliminate \ufb02at regions , decrease $ARG2"
    ],
    "hessian matrix*****correspondence": [
        "$ARG1 , or do anything else that will either make local updates easier to compute or improve the $ARG2"
    ],
    "correspondence*****solution": [
        "$ARG1 between local update directions and progress toward a global $ARG2"
    ],
    "curriculum learning*****method": [
        "$ARG1 or shaping can be interpreted as a continuation $ARG2"
    ],
    "curriculum learning*****learning": [
        "$ARG1 is based on the idea of planning a $ARG2"
    ],
    "method*****cost function": [
        "$ARG1 , where earlier J ( i ) are made easier by increasing the in\ufb02uence of simpler examples ( either by assigning their contributions to the $ARG2"
    ],
    "cost function*****neural language model": [
        "$ARG1 larger coe\ufb03cients , or by sampling them more frequently ) , and experimentally demonstrated that better results could be obtained by following a curriculum on a large-scale $ARG2"
    ],
    "curriculum learning*****computer vision": [
        "$ARG1 has been successful on a wide range of natural language ( Spitkovsky et al. , 2010 ; Collobert et al. , 2011a ; Mikolov et al. , 2011b ; Tu and Honavar , 2011 ) and $ARG2"
    ],
    "curriculum learning*****8": [
        "$ARG1 was also veri\ufb01ed as being consistent with the way in which humans teach ( Khan et al. , 2011 ) : teachers start by showing easier and CHAPTER $ARG2"
    ],
    "other*****curriculum learning": [
        "$ARG1 teaching strategies ( Basu and Christensen , Another important contribution to research on $ARG2"
    ],
    "curriculum learning*****recurrent neural network": [
        "$ARG1 arose in the context of training $ARG2"
    ],
    "recurrent neural network*****stochastic": [
        "$ARG1 to capture long-term dependencies : Zaremba and Sutskever ( 2014 ) found that much better results were obtained with a $ARG2"
    ],
    "stochastic*****average": [
        "$ARG1 curriculum , in which a random mix of easy and di\ufb03cult examples is always presented to the learner , but where the $ARG2"
    ],
    "turn*****neural network": [
        "$ARG1 to specializations of the $ARG2",
        "$ARG1 out to be bene\ufb01cial for $ARG2"
    ],
    "deep learning*****computer vision": [
        "$ARG1 to solve applications in $ARG2",
        "$ARG1 research on $ARG2",
        "$ARG1 for $ARG2"
    ],
    "computer vision*****speech recognition": [
        "$ARG1 , $ARG2"
    ],
    "speech recognition*****natural language processing": [
        "$ARG1 , $ARG2",
        "$ARG1 and $ARG2"
    ],
    "natural language processing*****other": [
        "$ARG1 , and $ARG2"
    ],
    "connectionism*****feature": [
        "$ARG1 : while an individual biological neuron or an individual $ARG2"
    ],
    "feature*****machine learning": [
        "$ARG1 in a $ARG2"
    ],
    "neural network*****accuracy": [
        "$ARG1 \u2019 s $ARG2"
    ],
    "other*****data structure": [
        "$ARG1 strategies , besides choosing whether to use \ufb01xed or \ufb02oating point , include optimizing $ARG2"
    ],
    "data structure*****vector": [
        "$ARG1 to avoid cache misses and using $ARG2"
    ],
    "model*****accuracy": [
        "$ARG1 , the $ARG2"
    ],
    "accuracy*****model": [
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 is just the proportion of examples for which the $ARG2",
        "$ARG1 and penalizes $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "neural network*****graphics processing unit": [
        "$ARG1 implementations are based on $ARG2"
    ],
    "video*****turn": [
        "$ARG1 gaming systems $ARG2"
    ],
    "matrix*****statement": [
        "$ARG1 ; there is no need to evaluate an if $ARG2"
    ],
    "statement*****matrix": [
        "$ARG1 per-vertex to determine which $ARG2"
    ],
    "memory*****capability": [
        "$ARG1 bandwidth , at the cost of having a lower clock speed and less branching $ARG2"
    ],
    "capability*****neural network": [
        "$ARG1 relative to traditional $ARG2"
    ],
    "memory*****system": [
        "$ARG1 bandwidth of the $ARG2"
    ],
    "system*****rate": [
        "$ARG1 often becomes the $ARG2"
    ],
    "neural network*****gpu": [
        "$ARG1 training algorithms typically do not involve much branching or sophisticated control , so they are appropriate for $ARG2",
        "$ARG1 easily bene\ufb01t from the parallelism of $ARG2",
        "$ARG1 on a $ARG2"
    ],
    "technique*****convolutional network": [
        "$ARG1 could be used to accelerate supervised $ARG2"
    ],
    "model*****memory": [
        "$ARG1 , massive parallelism , and high $ARG2",
        "$ARG1 be low than that the time and $ARG2",
        "$ARG1 that requires less $ARG2"
    ],
    "gpu*****memory": [
        "$ARG1 , most writable $ARG2",
        "$ARG1 implementations will be slow due to the lack of coalesced $ARG2"
    ],
    "gpu*****other": [
        "$ARG1 code is also inherently multi-threaded and the di\ufb00erent threads must be coordinated with each $ARG2"
    ],
    "memory*****multiple": [
        "$ARG1 , and j is a $ARG2"
    ],
    "convolution*****matrix multiplication": [
        "$ARG1 and $ARG2",
        "$ARG1 is thus dramatically more e\ufb03cient than dense $ARG2",
        "$ARG1 may be viewed as $ARG2"
    ],
    "example*****gpu": [
        "$ARG1 , the same Theano program can run on either CPU or $ARG2",
        "$ARG1 , training in $ARG2"
    ],
    "gpu*****change": [
        "$ARG1 , without needing to $ARG2"
    ],
    "other*****tensorflow": [
        "$ARG1 libraries like $ARG2"
    ],
    "inference*****example": [
        "$ARG1 is simple , because each input $ARG2",
        "$ARG1 by running the classi\ufb01ers in a sequence , abandoning any $ARG2",
        "$ARG1 for every $ARG2"
    ],
    "example*****process": [
        "$ARG1 we want to $ARG2",
        "$ARG1 , if we assign one machine to $ARG2",
        "$ARG1 x\u0303 \u2208 Rn obtained by an unknown corruption $ARG2",
        "$ARG1 , we choose to add the edge connecting a and c. ( Right ) To \ufb01nish the conversion $ARG2",
        "$ARG1 of how this $ARG2",
        "$ARG1 x , obtained through a given corruption $ARG2",
        "$ARG1 is transformed into one sample from this corruption $ARG2"
    ],
    "model parallelism*****multiple": [
        "$ARG1 , where $ARG2"
    ],
    "standard*****gradient descent": [
        "$ARG1 de\ufb01nition of $ARG2"
    ],
    "average*****gradient descent": [
        "$ARG1 amount of improvement that each $ARG2"
    ],
    "gradient descent*****other": [
        "$ARG1 step yields , because some of the cores overwrite each $ARG2"
    ],
    "other*****rate": [
        "$ARG1 \u2019 s progress , but the increased $ARG2"
    ],
    "rate*****learning": [
        "$ARG1 of production of steps causes the $ARG2"
    ],
    "gradient descent*****memory": [
        "$ARG1 , where the parameters are managed by a parameter server rather than stored in shared $ARG2"
    ],
    "gradient descent*****deep learning": [
        "$ARG1 remains the primary strategy for training large deep networks and is used by most major $ARG2"
    ],
    "memory*****inference": [
        "$ARG1 cost of running $ARG2"
    ],
    "inference*****machine learning": [
        "$ARG1 in a $ARG2"
    ],
    "inference*****model compression": [
        "$ARG1 is $ARG2"
    ],
    "model compression*****model": [
        "$ARG1 is to replace the original , expensive $ARG2",
        "$ARG1 is applicable when the size of the original $ARG2"
    ],
    "dynamic structure*****graph": [
        "$ARG1 in the $ARG2"
    ],
    "graph*****process": [
        "$ARG1 describing the computation needed to $ARG2"
    ],
    "neural network*****dynamic structure": [
        "$ARG1 can also exhibit $ARG2"
    ],
    "dynamic structure*****subset": [
        "$ARG1 internally by determining which $ARG2"
    ],
    "subset*****information": [
        "$ARG1 of features ( hidden units ) to compute given $ARG2"
    ],
    "dynamic structure*****neural network": [
        "$ARG1 inside $ARG2",
        "$ARG1 applied to $ARG2"
    ],
    "neural network*****conditional computation": [
        "$ARG1 is sometimes called $ARG2"
    ],
    "dynamic structure*****computer science": [
        "$ARG1 of computations is a basic $ARG2"
    ],
    "neural network*****subset": [
        "$ARG1 are based on determining which $ARG2"
    ],
    "subset*****group": [
        "$ARG1 of some $ARG2"
    ],
    "group*****neural network": [
        "$ARG1 of $ARG2"
    ],
    "example*****element": [
        "$ARG1 as soon as any one $ARG2",
        "$ARG1 , f ( A ) i , j gives $ARG2"
    ],
    "example*****street": [
        "$ARG1 , Google transcribes address numbers from $ARG2",
        "$ARG1 , we will use $ARG2",
        "$ARG1 , the $ARG2"
    ],
    "number*****machine learning": [
        "$ARG1 with one $ARG2"
    ],
    "decision tree*****example": [
        "$ARG1 themselves are an $ARG2"
    ],
    "example*****dynamic structure": [
        "$ARG1 of $ARG2"
    ],
    "deep learning*****dynamic structure": [
        "$ARG1 and $ARG2"
    ],
    "decision tree*****neural network": [
        "$ARG1 in which each node uses a $ARG2"
    ],
    "neural network*****inference": [
        "$ARG1 to make the splitting decision ( Guo and Gelfand , 1992 ) , though this has typically not been done with the primary goal of accelerating $ARG2",
        "$ARG1 research , hardware designers have worked on specialized hardware implementations that could speed up training and/or $ARG2"
    ],
    "mixture of experts*****set": [
        "$ARG1 ( Nowlan , 1990 ; Jacobs et al. , 1991 ) , in which the gater outputs a $ARG2"
    ],
    "weights*****softmax": [
        "$ARG1 ( obtained via a $ARG2"
    ],
    "reduction*****example": [
        "$ARG1 in computational cost , but if a single expert is chosen by the gater for each $ARG2"
    ],
    "example*****mixture of experts": [
        "$ARG1 , we obtain the hard $ARG2"
    ],
    "mixture of experts*****inference": [
        "$ARG1 ( Collobert et al. , 2001 , 2002 ) , which can considerably accelerate training and $ARG2"
    ],
    "reinforcement learning*****policy": [
        "$ARG1 techniques ( $ARG2"
    ],
    "gradient*****dropout": [
        "$ARG1 ) to learn a form of conditional $ARG2"
    ],
    "dropout*****reduction": [
        "$ARG1 on blocks of hidden units and get an actual $ARG2"
    ],
    "reduction*****quality": [
        "$ARG1 in computational cost without impacting negatively on the $ARG2"
    ],
    "average*****dynamic structure": [
        "$ARG1 over many possible inputs , and thus do not achieve all of the possible computational bene\ufb01ts of $ARG2"
    ],
    "convolution*****minibatch": [
        "$ARG1 on a $ARG2"
    ],
    "row*****design matrix": [
        "$ARG1 of a $ARG2"
    ],
    "design matrix*****set": [
        "$ARG1 by a di\ufb00erent $ARG2"
    ],
    "coherence*****gpu": [
        "$ARG1 and $ARG2"
    ],
    "inference*****neural network": [
        "$ARG1 of $ARG2"
    ],
    "precision*****inference": [
        "$ARG1 , at least at $ARG2",
        "$ARG1 is required during training than at $ARG2"
    ],
    "rate*****gpu": [
        "$ARG1 of progress of a single CPU or $ARG2"
    ],
    "era*****neural network": [
        "$ARG1 ) where the hardware implementations of $ARG2"
    ],
    "deep learning*****speech": [
        "$ARG1 ( e.g. , with $ARG2"
    ],
    "speech*****computer vision": [
        "$ARG1 , $ARG2"
    ],
    "8*****precision": [
        "$ARG1 and 16 bits of $ARG2"
    ],
    "precision*****neural network": [
        "$ARG1 can su\ufb03ce for using or training deep $ARG2"
    ],
    "inference*****number": [
        "$ARG1 time , and that some forms of dynamic \ufb01xed point representation of numbers can be used to reduce how many bits are required per $ARG2"
    ],
    "number*****backprop": [
        "$ARG1 reduces the hardware surface area , power requirements and computing time needed for performing multiplications , and multiplications are the most demanding of the operations needed to use or train a modern deep network with $ARG2"
    ],
    "computer vision*****deep learning": [
        "$ARG1 has traditionally been one of the most active research areas for $ARG2"
    ],
    "deep learning*****task": [
        "$ARG1 applications , because vision is a $ARG2",
        "$ARG1 has recently begun to have an important impact on this kind of $ARG2",
        "$ARG1 applied to a parsing $ARG2"
    ],
    "standard*****deep learning": [
        "$ARG1 benchmark tasks for $ARG2"
    ],
    "deep learning*****object recognition": [
        "$ARG1 algorithms are forms of $ARG2"
    ],
    "object recognition*****character": [
        "$ARG1 or optical $ARG2"
    ],
    "computer vision*****human": [
        "$ARG1 range from reproducing $ARG2"
    ],
    "computer vision*****video": [
        "$ARG1 application is to recognize sound waves from the vibrations they induce in objects visible in a $ARG2"
    ],
    "computer vision*****object recognition": [
        "$ARG1 is used for $ARG2"
    ],
    "object recognition*****image": [
        "$ARG1 or detection of some form , whether this means reporting which object is present in an $ARG2",
        "$ARG1 , where the input is an $ARG2"
    ],
    "image*****identity": [
        "$ARG1 with the $ARG2"
    ],
    "deep learning*****image": [
        "$ARG1 research , there is also a large body of work on $ARG2",
        "$ARG1 the $ARG2"
    ],
    "synthesis*****computer vision": [
        "$ARG1 ex nihilo is usually not considered a $ARG2"
    ],
    "computer vision*****image": [
        "$ARG1 endeavor , models capable of $ARG2"
    ],
    "synthesis*****image": [
        "$ARG1 are usually useful for $ARG2"
    ],
    "image*****computer vision": [
        "$ARG1 restoration , a $ARG2"
    ],
    "preprocessing*****deep learning": [
        "$ARG1 because the original input comes in a form that is di\ufb03cult for many $ARG2"
    ],
    "computer vision*****preprocessing": [
        "$ARG1 usually requires relatively little of this kind of $ARG2"
    ],
    "computer vision*****standard": [
        "$ARG1 architectures require images of a $ARG2"
    ],
    "pooling*****constant": [
        "$ARG1 regions to keep the output size $ARG2"
    ],
    "dataset augmentation*****preprocessing": [
        "$ARG1 may be seen as a way of $ARG2"
    ],
    "preprocessing*****set": [
        "$ARG1 the training $ARG2",
        "$ARG1 are applied to both the train and the test $ARG2"
    ],
    "dataset augmentation*****generalization": [
        "$ARG1 is an excellent way to reduce the $ARG2",
        "$ARG1 schemes can dramatically reduce the $ARG2"
    ],
    "generalization*****computer vision": [
        "$ARG1 error of most $ARG2"
    ],
    "other*****preprocessing": [
        "$ARG1 kinds of $ARG2"
    ],
    "generalization*****model": [
        "$ARG1 error and reduce the size of the $ARG2",
        "$ARG1 error is bounded from above by a quantity that grows as the $ARG2",
        "$ARG1 and over\ufb01tting in chapter 5 , we focused on three situations , where the $ARG2",
        "$ARG1 error ) is a large $ARG2",
        "$ARG1 , even if the $ARG2"
    ],
    "preprocessing*****human": [
        "$ARG1 of this kind is usually designed to remove some kind of variability in the input data that is easy for a $ARG2"
    ],
    "preprocessing*****model": [
        "$ARG1 is often unnecessary , and it is best to just let the $ARG2"
    ],
    "model*****invariant": [
        "$ARG1 learn which kinds of variability it should become $ARG2",
        "$ARG1 should be $ARG2",
        "$ARG1 should be $ARG2"
    ],
    "example*****alexnet": [
        "$ARG1 , the $ARG2"
    ],
    "system*****preprocessing": [
        "$ARG1 for classifying ImageNet only has one $ARG2"
    ],
    "contrast*****image": [
        "$ARG1 in the $ARG2",
        "$ARG1 of an $ARG2",
        "$ARG1 of the entire $ARG2",
        "$ARG1 by subtracting the mean from each $ARG2",
        "$ARG1 of a zero-contrast $ARG2",
        "$ARG1 is normalized across each small window , rather than over the $ARG2"
    ],
    "contrast*****magnitude": [
        "$ARG1 simply refers to the $ARG2"
    ],
    "magnitude*****image": [
        "$ARG1 of the di\ufb00erence between the bright and the dark pixels in an $ARG2"
    ],
    "deep learning*****contrast": [
        "$ARG1 , $ARG2"
    ],
    "contrast*****standard deviation": [
        "$ARG1 usually refers to the $ARG2"
    ],
    "standard deviation*****image": [
        "$ARG1 of the pixels in an $ARG2",
        "$ARG1 allows the same s to be used regardless of $ARG2"
    ],
    "image*****tensor": [
        "$ARG1 represented by a $ARG2"
    ],
    "tensor*****row": [
        "$ARG1 X \u2208 Rr\u00d7c\u00d73 , with Xi , j,1 being the red intensity at $ARG2",
        "$ARG1 W. The indices into W are respectively : i , the output channel , j , the output $ARG2"
    ],
    "global contrast normalization*****contrast": [
        "$ARG1 ( GCN ) aims to prevent images from having varying amounts of $ARG2"
    ],
    "image*****standard deviation": [
        "$ARG1 , then rescaling it so that the $ARG2"
    ],
    "standard deviation*****constant": [
        "$ARG1 across its pixels is equal to some $ARG2"
    ],
    "constant*****change": [
        "$ARG1 s. This approach is complicated by the fact that no scaling factor can $ARG2"
    ],
    "change*****contrast": [
        "$ARG1 the $ARG2"
    ],
    "contrast*****information": [
        "$ARG1 often have little $ARG2"
    ],
    "regularization*****bias": [
        "$ARG1 parameter \u03bb to $ARG2"
    ],
    "bias*****standard deviation": [
        "$ARG1 the estimate of the $ARG2"
    ],
    "image*****constant": [
        "$ARG1 X\ue030 , Xi , j , k max \ue00f , \u03bb + Xi , j , k \u2212 X\u0304 Xi , j , k \u2212 X\u0304 Datasets consisting of large images cropped to interesting objects are unlikely to contain any images with nearly $ARG2",
        "$ARG1 much more , discarding all regions of $ARG2"
    ],
    "constant*****regularization": [
        "$ARG1 intensity , making aggressive $ARG2"
    ],
    "standard deviation*****1": [
        "$ARG1 across examples close to $ARG2"
    ],
    "standard deviation*****norm": [
        "$ARG1 in equation 12.3 is just a rescaling of the L2 $ARG2",
        "$ARG1 rather than L2 $ARG2",
        "$ARG1 rather than the L 2 $ARG2"
    ],
    "norm*****image": [
        "$ARG1 of the $ARG2"
    ],
    "norm*****standard deviation": [
        "$ARG1 because the $ARG2",
        "$ARG1 is proportional to the $ARG2"
    ],
    "standard deviation*****number": [
        "$ARG1 includes division by the $ARG2"
    ],
    "number*****standard deviation": [
        "$ARG1 of pixels , so GCN based on $ARG2"
    ],
    "property*****neural network": [
        "$ARG1 because $ARG2",
        "$ARG1 of a $ARG2"
    ],
    "graphical model*****multiple": [
        "$ARG1 have problems with representing $ARG2",
        "$ARG1 with $ARG2"
    ],
    "example*****distance": [
        "$ARG1 to a direction rather than a direction and a $ARG2"
    ],
    "operation*****sphering": [
        "$ARG1 known as $ARG2"
    ],
    "sphering*****operation": [
        "$ARG1 and it is not the same $ARG2"
    ],
    "variance*****multivariate normal distribution": [
        "$ARG1 , so that the $ARG2"
    ],
    "multivariate normal distribution*****pca": [
        "$ARG1 used by $ARG2"
    ],
    "sphering*****whitening": [
        "$ARG1 is more commonly known as $ARG2"
    ],
    "global contrast normalization*****image": [
        "$ARG1 will often fail to highlight $ARG2"
    ],
    "image*****global contrast normalization": [
        "$ARG1 in the shadow of a building ) then $ARG2"
    ],
    "local contrast normalization*****contrast": [
        "$ARG1 ensures that the $ARG2"
    ],
    "contrast*****local contrast normalization": [
        "$ARG1 Various de\ufb01nitions of $ARG2"
    ],
    "other*****standard deviation": [
        "$ARG1 cases , this is a weighted mean and weighted $ARG2",
        "$ARG1 , and thus more likely to have zero $ARG2"
    ],
    "standard deviation*****weights": [
        "$ARG1 using Gaussian $ARG2"
    ],
    "color images*****process": [
        "$ARG1 , some strategies $ARG2"
    ],
    "local contrast normalization*****image": [
        "$ARG1 modi\ufb01es the $ARG2"
    ],
    "row*****kernel": [
        "$ARG1 , may lose some detail due to the bandwidth of the normalization $ARG2"
    ],
    "local contrast normalization*****separable convolution": [
        "$ARG1 can usually be implemented e\ufb03ciently by using $ARG2"
    ],
    "separable convolution*****feature": [
        "$ARG1 ( see section 9.8 ) to compute $ARG2"
    ],
    "feature*****standard deviation": [
        "$ARG1 maps of local means and local $ARG2"
    ],
    "standard deviation*****feature": [
        "$ARG1 , then using element-wise subtraction and element-wise division on di\ufb00erent $ARG2"
    ],
    "local contrast normalization*****operation": [
        "$ARG1 is a di\ufb00erentiable $ARG2"
    ],
    "operation*****hidden layer": [
        "$ARG1 and can also be used as a nonlinearity applied to the $ARG2"
    ],
    "hidden layer*****preprocessing": [
        "$ARG1 of a network , as well as a $ARG2"
    ],
    "global contrast normalization*****local contrast normalization": [
        "$ARG1 , we typically need to regularize $ARG2"
    ],
    "set*****change": [
        "$ARG1 by adding extra copies of the training examples that have been modi\ufb01ed with transformations that do not $ARG2"
    ],
    "object recognition*****classi\ufb01cation": [
        "$ARG1 is a $ARG2",
        "$ARG1 is the same basic technology that allows computers to recognize faces ( Taigman et al. , 2014 ) , which can be used to automatically tag people in photo collections and allow computers to interact more naturally with \u2022 $ARG2"
    ],
    "task*****dataset augmentation": [
        "$ARG1 that is especially amenable to this form of $ARG2"
    ],
    "dataset augmentation*****invariant": [
        "$ARG1 because the class is $ARG2"
    ],
    "computer vision*****dataset augmentation": [
        "$ARG1 applications , more advanced transformations are commonly used for $ARG2"
    ],
    "speech recognition*****deep learning": [
        "$ARG1 systems preprocess the input using specialized hand-designed features , but some ( Jaitly and Hinton , 2011 ) $ARG2",
        "$ARG1 community towards $ARG2"
    ],
    "automatic speech recognition*****task": [
        "$ARG1 ( ASR ) $ARG2"
    ],
    "task*****function": [
        "$ARG1 consists of creating a $ARG2",
        "$ARG1 of either minimizing or maximizing some $ARG2"
    ],
    "distribution*****speech recognition": [
        "$ARG1 relating the inputs X to the targets Since the 1980s and until about 2009\u20132012 , state-of-the art $ARG2"
    ],
    "speech recognition*****gaussian mixture model": [
        "$ARG1 systems primarily combined hidden Markov models ( HMMs ) and $ARG2"
    ],
    "speech recognition*****neural network": [
        "$ARG1 was actually one of the \ufb01rst areas where $ARG2",
        "$ARG1 and the e\ufb00ort that had been invested in building these systems on the basis of GMM-HMMs , the industry did not see a compelling argument for switching to $ARG2",
        "$ARG1 incorporated deep $ARG2"
    ],
    "example*****rate": [
        "$ARG1 , Robinson and Fallside ( 1991 ) achieved 26 % phoneme error $ARG2"
    ],
    "accuracy*****neural network": [
        "$ARG1 was dramatically improved by using $ARG2"
    ],
    "speech*****deep learning": [
        "$ARG1 researchers applied a form of $ARG2",
        "$ARG1 groups in industry had started exploring $ARG2"
    ],
    "unsupervised learning*****speech recognition": [
        "$ARG1 to $ARG2"
    ],
    "deep learning*****restricted boltzmann machine": [
        "$ARG1 was based on training undirected probabilistic models called $ARG2"
    ],
    "restricted boltzmann machine*****model": [
        "$ARG1 ( RBMs ) to $ARG2"
    ],
    "speech recognition*****unsupervised pretraining": [
        "$ARG1 tasks , $ARG2"
    ],
    "unsupervised pretraining*****deep feedforward network": [
        "$ARG1 was used to build $ARG2"
    ],
    "pretraining*****boltzmann machine": [
        "$ARG1 and $ARG2"
    ],
    "boltzmann machine*****recti\ufb01ed linear unit": [
        "$ARG1 to being based on techniques such as $ARG2"
    ],
    "rate*****speech recognition": [
        "$ARG1 in $ARG2"
    ],
    "matter*****speech recognition": [
        "$ARG1 of roughly two years , most of the industrial products for $ARG2"
    ],
    "weights*****time-delay neural network": [
        "$ARG1 across time and frequency , improving over the earlier $ARG2"
    ],
    "time-delay neural network*****weights": [
        "$ARG1 that replicated $ARG2"
    ],
    "vector*****image": [
        "$ARG1 but as an $ARG2",
        "$ARG1 , x i , as the $ARG2"
    ],
    "deep learning*****system": [
        "$ARG1 ASR is to let the $ARG2"
    ],
    "natural language processing*****human": [
        "$ARG1 ( NLP ) is the use of $ARG2"
    ],
    "natural language processing*****machine translation": [
        "$ARG1 includes applications such as $ARG2"
    ],
    "machine translation*****human": [
        "$ARG1 , in which the learner must read a sentence in one $ARG2"
    ],
    "neural network*****natural language processing": [
        "$ARG1 techniques can be successfully applied to $ARG2"
    ],
    "conditional probability*****1": [
        "$ARG1 of the n-th token given the preceding n \u2212 $ARG2"
    ],
    "n-gram*****maximum likelihood": [
        "$ARG1 models is straightforward because the $ARG2"
    ],
    "unigram*****bigram": [
        "$ARG1 for n=1 , $ARG2",
        "$ARG1 or a $ARG2"
    ],
    "bigram*****trigram": [
        "$ARG1 for n=2 , and $ARG2"
    ],
    "inference*****character": [
        "$ARG1 in P n , we must omit the \ufb01nal $ARG2"
    ],
    "example*****trigram": [
        "$ARG1 , we demonstrate how a $ARG2"
    ],
    "probability*****conditional probability": [
        "$ARG1 of the sentence \u201c THE DOG RAN AWAY. \u201d The \ufb01rst words of the sentence can not be handled by the default formula based on $ARG2"
    ],
    "maximum likelihood*****n-gram": [
        "$ARG1 for $ARG2"
    ],
    "n-gram*****set": [
        "$ARG1 models is that Pn as estimated from training $ARG2"
    ],
    "technique*****probability": [
        "$ARG1 consists of adding non-zero $ARG2"
    ],
    "method*****inference": [
        "$ARG1 can be justi\ufb01ed as Bayesian $ARG2"
    ],
    "mixture model*****n-gram": [
        "$ARG1 containing higher-order and lower-order $ARG2"
    ],
    "n-gram*****curse of dimensionality": [
        "$ARG1 models are particularly vulnerable to the $ARG2"
    ],
    "view*****n-gram": [
        "$ARG1 a classical $ARG2"
    ],
    "other*****k-nearest neighbors": [
        "$ARG1 words , it can be viewed as a local non-parametric predictor , similar to $ARG2"
    ],
    "model*****distance": [
        "$ARG1 is even more severe than usual , because any two di\ufb00erent words have the same $ARG2"
    ],
    "information*****generalization": [
        "$ARG1 from any \u201c neighbors \u201d \u2014only training examples that repeat literally the same context are useful for local $ARG2"
    ],
    "n-gram*****class-based language models": [
        "$ARG1 models , $ARG2"
    ],
    "neural language model*****model": [
        "$ARG1 or NLMs are a class of language $ARG2",
        "$ARG1 to $ARG2"
    ],
    "model*****curse of dimensionality": [
        "$ARG1 designed to overcome the $ARG2"
    ],
    "problem*****distributed representation": [
        "$ARG1 for modeling natural language sequences by using a $ARG2"
    ],
    "n-gram*****neural language model": [
        "$ARG1 models , $ARG2"
    ],
    "information*****number": [
        "$ARG1 from each training sentence to an exponentially large $ARG2"
    ],
    "curse of dimensionality*****model": [
        "$ARG1 requires the $ARG2"
    ],
    "interpretation*****view": [
        "$ARG1 , we $ARG2"
    ],
    "view*****dimension": [
        "$ARG1 the raw symbols as points in a space of $ARG2"
    ],
    "feature*****dimension": [
        "$ARG1 space of lower $ARG2"
    ],
    "vector*****distance": [
        "$ARG1 , so every pair of words is at Euclidean $ARG2",
        "$ARG1 x measures the $ARG2"
    ],
    "embedding*****model": [
        "$ARG1 space , words that frequently appear in similar contexts ( or any pair of words sharing some \u201c features \u201d learned by the $ARG2"
    ],
    "word embedding*****other": [
        "$ARG1 space to show how semantically similar words map to representations that are close to each $ARG2"
    ],
    "hidden layer*****convolutional network": [
        "$ARG1 of a $ARG2"
    ],
    "image*****vector": [
        "$ARG1 embedding. \u201d Usually NLP practitioners are much more interested in this idea of embeddings because natural language does not originally lie in a real-valued $ARG2"
    ],
    "hidden layer*****change": [
        "$ARG1 has provided a more qualitatively dramatic $ARG2"
    ],
    "change*****distributed representation": [
        "$ARG1 in the way the data is The basic idea of using $ARG2"
    ],
    "distributed representation*****natural language processing": [
        "$ARG1 to improve models for $ARG2"
    ],
    "natural language processing*****neural network": [
        "$ARG1 is not restricted to $ARG2"
    ],
    "graphical model*****distributed representation": [
        "$ARG1 that have $ARG2",
        "$ARG1 and computational complexity , $ARG2"
    ],
    "word embedding*****machine translation": [
        "$ARG1 obtained from a neural $ARG2"
    ],
    "model*****embedding": [
        "$ARG1 ( Bahdanau et al. , 2015 ) , zooming in on speci\ufb01c areas where semantically related words have $ARG2"
    ],
    "embedding*****other": [
        "$ARG1 vectors that are close to each $ARG2"
    ],
    "distribution*****a\ufb03ne": [
        "$ARG1 is to apply an $ARG2"
    ],
    "a\ufb03ne*****softmax function": [
        "$ARG1 transformation from a hidden representation to the output space , then apply the $ARG2"
    ],
    "matrix*****a\ufb03ne": [
        "$ARG1 describing the linear component of this $ARG2"
    ],
    "a\ufb03ne*****dimension": [
        "$ARG1 transformation is very large , because its output $ARG2"
    ],
    "memory*****matrix": [
        "$ARG1 cost to represent the $ARG2"
    ],
    "softmax*****matrix multiplication": [
        "$ARG1 is normalized across all |V| outputs , it is necessary to perform the full $ARG2"
    ],
    "matrix multiplication*****dot product": [
        "$ARG1 at training time as well as test time\u2014we can not calculate only the $ARG2"
    ],
    "dot product*****vector": [
        "$ARG1 with the weight $ARG2"
    ],
    "output layer*****gradient": [
        "$ARG1 thus arise both at training time ( to compute the likelihood and its $ARG2"
    ],
    "gradient*****standard": [
        "$ARG1 can be computed e\ufb03ciently ( Vincent et al. , 2015 ) , but the $ARG2"
    ],
    "weights*****output layer": [
        "$ARG1 W and learned biases b , then the a\ufb03ne-softmax $ARG2"
    ],
    "contains*****operation": [
        "$ARG1 n h elements then the above $ARG2"
    ],
    "operation*****neural language model": [
        "$ARG1 dominates the computation of most $ARG2"
    ],
    "list*****neural language model": [
        "$ARG1 The \ufb01rst $ARG2"
    ],
    "neural language model*****softmax": [
        "$ARG1 ( Bengio et al. , 2001 , 2003 ) dealt with the high cost of using a $ARG2",
        "$ARG1 \u2019 s $ARG2"
    ],
    "softmax*****number": [
        "$ARG1 over a large $ARG2"
    ],
    "shortlist*****n-gram": [
        "$ARG1 L of most frequent words ( handled by the neural net ) and a tail T = V\\L of more rare words ( handled by an $ARG2"
    ],
    "probability*****list": [
        "$ARG1 that a word appearing after context C belongs to the tail $ARG2"
    ],
    "1*****neural language model": [
        "$ARG1 i\u2208TP ( y = i | C , i \u2208 T ) P ( i \u2208 T | C ) where P ( y = i | C , i \u2208 L ) is provided by the $ARG2"
    ],
    "neural language model*****n-gram": [
        "$ARG1 and P ( y = i | C , i \u2208 T ) is provided by the $ARG2",
        "$ARG1 with n-grams A major advantage of $ARG2",
        "$ARG1 and an $ARG2"
    ],
    "softmax*****sigmoid": [
        "$ARG1 layer , rather than a separate $ARG2"
    ],
    "list*****potential": [
        "$ARG1 approach is that the $ARG2"
    ],
    "generalization*****neural language model": [
        "$ARG1 advantage of the $ARG2"
    ],
    "exploration*****output layer": [
        "$ARG1 of alternative methods to deal with high-dimensional outputs , A classical approach ( Goodman , 2001 ) to reducing the computational burden of high-dimensional $ARG2"
    ],
    "probability*****path": [
        "$ARG1 of a choosing a word is given by the product of the probabilities of choosing the branch leading to that word at every node on a $ARG2",
        "$ARG1 of a word y can be done by multiplying three probabilities , associated with the binary decisions to move left or right at each node on the $ARG2"
    ],
    "set*****supervised learning": [
        "$ARG1 , we can use $ARG2",
        "$ARG1 targets were Most $ARG2"
    ],
    "supervised learning*****logistic regression model": [
        "$ARG1 to train the $ARG2"
    ],
    "gradient*****hidden layer": [
        "$ARG1 with respect to the output parameters but also the gradients with respect to the $ARG2"
    ],
    "tree structure*****expected": [
        "$ARG1 to minimize the $ARG2"
    ],
    "illustration*****8": [
        "$ARG1 of a simple hierarchy of word categories , with $ARG2"
    ],
    "1*****contains": [
        "$ARG1 ) $ARG2"
    ],
    "number*****path": [
        "$ARG1 of words |V| : the choice of one out of |V| words can be obtained by doing O ( log |V| ) operations ( one for each of the nodes on the $ARG2"
    ],
    "probability*****rule": [
        "$ARG1 of sampling an output y decomposes into a product of conditional probabilities , using the chain $ARG2",
        "$ARG1 of 12 , the weight scaling $ARG2"
    ],
    "average*****number": [
        "$ARG1 of the $ARG2"
    ],
    "softmax*****probability": [
        "$ARG1 is that it brings computational bene\ufb01ts both at training time and at test time , if at test time we want to compute the $ARG2",
        "$ARG1 can never predict a $ARG2"
    ],
    "probability*****softmax": [
        "$ARG1 of all |V| words will remain expensive even with the hierarchical $ARG2",
        "$ARG1 of the output sequence p ( y | x ) < t for some threshold t. Initially , the de\ufb01nition of p ( y | x ) was ad-hoc , based on simply multiplying all of the $ARG2"
    ],
    "tree structure*****solution": [
        "$ARG1 does not provide an e\ufb03cient and exact $ARG2"
    ],
    "neural language model*****gradient": [
        "$ARG1 is to avoid explicitly computing the contribution of the $ARG2"
    ],
    "negative phase*****expectation": [
        "$ARG1 term is an $ARG2"
    ],
    "bias*****distribution": [
        "$ARG1 introduced by sampling from the wrong $ARG2"
    ],
    "technique*****importance sampling": [
        "$ARG1 called $ARG2"
    ],
    "importance sampling*****weights": [
        "$ARG1 is not e\ufb03cient because it requires computing $ARG2"
    ],
    "solution*****biased importance sampling": [
        "$ARG1 adopted for this application is called $ARG2"
    ],
    "biased importance sampling*****weights": [
        "$ARG1 , where the importance $ARG2"
    ],
    "weights*****negative phase": [
        "$ARG1 are used to give the appropriate importance to the m negative samples from q used to form the estimated $ARG2"
    ],
    "1*****unigram": [
        "$ARG1 \ue058 \u2202a ni P ( i | C ) A $ARG2"
    ],
    "importance sampling*****softmax": [
        "$ARG1 is not only useful for speeding up models with large $ARG2"
    ],
    "output layer*****vector": [
        "$ARG1 , where the output is a sparse $ARG2"
    ],
    "example*****bag of words": [
        "$ARG1 is a $ARG2"
    ],
    "bag of words*****vector": [
        "$ARG1 is a sparse $ARG2"
    ],
    "loss function*****element": [
        "$ARG1 we use for training might most naturally be described in terms of comparing every $ARG2"
    ],
    "algorithm*****loss": [
        "$ARG1 minimizes the $ARG2"
    ],
    "loss*****number": [
        "$ARG1 reconstruction for the \u201c positive words \u201d ( those that are non-zero in the target ) and an equal $ARG2",
        "$ARG1 changes over time ( indicated as $ARG2"
    ],
    "number*****vector": [
        "$ARG1 of negative samples rather than proportional to the size of the output $ARG2",
        "$ARG1 of non-zero entries in a $ARG2",
        "$ARG1 of pools , in order to provide a \ufb01xed-size $ARG2"
    ],
    "noise-contrastive estimation*****loss": [
        "$ARG1 and Ranking $ARG2"
    ],
    "other*****neural language model": [
        "$ARG1 approaches based on sampling have been proposed to reduce the computational cost of training $ARG2"
    ],
    "loss*****neural language model": [
        "$ARG1 proposed by Collobert and Weston ( 2008a ) , which views the output of the $ARG2"
    ],
    "loss*****1": [
        "$ARG1 proposed then is max ( 0 , $ARG2"
    ],
    "speech recognition*****translation": [
        "$ARG1 and text generation ( including conditional text generation tasks such as $ARG2"
    ],
    "n-gram*****neural network": [
        "$ARG1 models over $ARG2"
    ],
    "neural network*****n-gram": [
        "$ARG1 is that $ARG2"
    ],
    "n-gram*****model": [
        "$ARG1 models achieve high $ARG2",
        "$ARG1 language $ARG2"
    ],
    "embedding*****pass": [
        "$ARG1 in each $ARG2"
    ],
    "pass*****example": [
        "$ARG1 , so we can increase the vocabulary size without increasing the computation time per $ARG2"
    ],
    "other*****tiled convolution": [
        "$ARG1 models , such as $ARG2"
    ],
    "tiled convolution*****parameter sharing": [
        "$ARG1 networks , can add parameters while reducing the degree of $ARG2"
    ],
    "neural network*****matrix multiplication": [
        "$ARG1 layers based on $ARG2",
        "$ARG1 layers use $ARG2",
        "$ARG1 de\ufb01ned by $ARG2"
    ],
    "matrix multiplication*****number": [
        "$ARG1 use an amount of computation proportional to the $ARG2"
    ],
    "neural network*****set": [
        "$ARG1 with an extra $ARG2"
    ],
    "model*****contains": [
        "$ARG1 capacity is huge\u2014the new portion of the architecture $ARG2"
    ],
    "contains*****process": [
        "$ARG1 up to |sV |n parameters\u2014but the amount of added computation needed to $ARG2"
    ],
    "machine translation*****task": [
        "$ARG1 is the $ARG2"
    ],
    "example*****translation": [
        "$ARG1 , many languages put adjectives after nouns , so when translated to English directly they yield phrases such as \u201c apple red. \u201d The proposal mechanism suggests many variants of the suggested $ARG2"
    ],
    "system*****model": [
        "$ARG1 , a language $ARG2"
    ],
    "neural network*****machine translation": [
        "$ARG1 for $ARG2"
    ],
    "machine translation*****model": [
        "$ARG1 was to upgrade the language $ARG2",
        "$ARG1 involves producing an output sentence given an input sentence , it makes sense to extend the natural language $ARG2"
    ],
    "model*****translation": [
        "$ARG1 of a $ARG2",
        "$ARG1 has already been designed to be partially $ARG2"
    ],
    "system*****neural language model": [
        "$ARG1 by using a $ARG2"
    ],
    "machine translation*****n-gram": [
        "$ARG1 systems had used an $ARG2",
        "$ARG1 include not just traditional back-o\ufb00 $ARG2"
    ],
    "n-gram*****machine translation": [
        "$ARG1 based models used for $ARG2"
    ],
    "distribution*****variable": [
        "$ARG1 over some $ARG2",
        "$ARG1 over that $ARG2",
        "$ARG1 over a binary $ARG2",
        "$ARG1 , but only a single $ARG2",
        "$ARG1 over a single discrete $ARG2",
        "$ARG1 P ( c ) over the latent $ARG2",
        "$ARG1 P ( x ) even though it is possible to describe P ( x ) without reference to the latent $ARG2"
    ],
    "variable*****list": [
        "$ARG1 or a $ARG2"
    ],
    "encoder*****modality": [
        "$ARG1 of data from one $ARG2"
    ],
    "modality*****encoder": [
        "$ARG1 ( such as the $ARG2"
    ],
    "mapping*****decoder": [
        "$ARG1 from French sentences to hidden representations capturing the meaning of sentences ) as the input to a $ARG2",
        "$ARG1 x to h ) and the $ARG2",
        "$ARG1 h to Copying the input to the output may sound useless , but we are typically not interested in the output of the $ARG2"
    ],
    "decoder*****modality": [
        "$ARG1 for another $ARG2"
    ],
    "modality*****decoder": [
        "$ARG1 ( such as the $ARG2"
    ],
    "mapping*****modality": [
        "$ARG1 from hidden representations capturing the meaning of sentences to English ) , we can train systems that translate from one $ARG2"
    ],
    "translation*****model": [
        "$ARG1 more \ufb02exible , we would like to use a $ARG2"
    ],
    "model*****data structure": [
        "$ARG1 \ufb01rst reads the input sequence and emits a $ARG2"
    ],
    "list*****vector": [
        "$ARG1 of vectors , or it may be a $ARG2"
    ],
    "vector*****tensor": [
        "$ARG1 or $ARG2"
    ],
    "vector*****average": [
        "$ARG1 c is formed by taking a weighted $ARG2",
        "$ARG1 \ufb01eld estimates the score \u2207 x log pdata ( x ) up to a multiplicative factor that is the $ARG2"
    ],
    "average*****feature": [
        "$ARG1 of $ARG2"
    ],
    "feature*****weights": [
        "$ARG1 vectors h ( t ) with $ARG2",
        "$ARG1 ( a hidden unit with the same $ARG2"
    ],
    "feature*****neural network": [
        "$ARG1 vectors h are hidden units of a $ARG2"
    ],
    "1*****average": [
        "$ARG1 ] and are intended to concentrate around just one h ( t ) so that the weighted $ARG2"
    ],
    "weights*****softmax function": [
        "$ARG1 \u03b1 ( t ) are usually produced by applying a $ARG2"
    ],
    "attention*****gradient descent": [
        "$ARG1 mechanism is more expensive computationally than directly indexing the desired h ( t ) , but direct indexing can not be trained with $ARG2"
    ],
    "attention*****optimization algorithm": [
        "$ARG1 mechanism based on weighted averages is a smooth , di\ufb00erentiable approximation that can be trained with existing $ARG2"
    ],
    "attention*****system": [
        "$ARG1 mechanism used to focus on speci\ufb01c parts of the input sequence at each time step is illustrated in We can think of an attention-based $ARG2"
    ],
    "system*****1": [
        "$ARG1 as having three components : $ARG2"
    ],
    "process*****distributed representation": [
        "$ARG1 that \u201c reads \u201d raw data ( such as source words in a source sentence ) , and converts them into $ARG2"
    ],
    "list*****feature": [
        "$ARG1 of $ARG2"
    ],
    "process*****content": [
        "$ARG1 that \u201c exploits \u201d the $ARG2"
    ],
    "task*****attention": [
        "$ARG1 , at each time step having the ability put $ARG2"
    ],
    "attention*****content": [
        "$ARG1 on the $ARG2"
    ],
    "matrix*****word embedding": [
        "$ARG1 relating the $ARG2"
    ],
    "back-propagation*****identity": [
        "$ARG1 , with symbols corresponding to the $ARG2"
    ],
    "identity*****neural network": [
        "$ARG1 of family members and the $ARG2"
    ],
    "natural language processing*****model": [
        "$ARG1 is marked by transitions in the popularity of di\ufb00erent ways of representing the input to the $ARG2"
    ],
    "neural language model*****word embedding": [
        "$ARG1 , which produce interpretable $ARG2"
    ],
    "neural language model*****natural language processing": [
        "$ARG1 have been extended into several $ARG2"
    ],
    "natural language processing*****part-of-speech tagging": [
        "$ARG1 applications , such as parsing ( Henderson , 2003 , 2004 ; Collobert , 2011 ) , $ARG2"
    ],
    "part-of-speech tagging*****multi-task learning": [
        "$ARG1 , semantic role labeling , chunking , etc , sometimes using a single $ARG2"
    ],
    "multi-task learning*****word embedding": [
        "$ARG1 architecture ( Collobert and Weston , 2008a ; Collobert et al. , 2011a ) in which the $ARG2"
    ],
    "word embedding*****dimensionality reduction": [
        "$ARG1 are shared across Two-dimensional visualizations of embeddings became a popular tool for analyzing language models following the development of the t-SNE $ARG2"
    ],
    "algorithm*****word embedding": [
        "$ARG1 ( van der Maaten and Hinton , 2008 ) and its high-pro\ufb01le application to visualization $ARG2"
    ],
    "deep learning*****standard": [
        "$ARG1 that are di\ufb00erent from the $ARG2"
    ],
    "object recognition*****speech recognition": [
        "$ARG1 , $ARG2",
        "$ARG1 , $ARG2"
    ],
    "machine learning*****information technology": [
        "$ARG1 in the $ARG2"
    ],
    "information technology*****potential": [
        "$ARG1 sector is the ability to make recommendations of items to $ARG2"
    ],
    "probability*****expected": [
        "$ARG1 of some action ( the user buying the product , or some proxy for this action ) or the $ARG2"
    ],
    "matching*****video game": [
        "$ARG1 players for $ARG2"
    ],
    "video game*****matching": [
        "$ARG1 , or $ARG2"
    ],
    "problem*****supervised learning": [
        "$ARG1 is handled like a $ARG2",
        "$ARG1 of modeling p ( x ) by splitting it into n $ARG2"
    ],
    "problem*****expected value": [
        "$ARG1 ( predicting some conditional $ARG2"
    ],
    "expected value*****classi\ufb01cation": [
        "$ARG1 ) or a probabilistic $ARG2"
    ],
    "problem*****conditional probability": [
        "$ARG1 ( predicting the $ARG2"
    ],
    "conditional probability*****recommender systems": [
        "$ARG1 of some discrete The early work on $ARG2"
    ],
    "recommender systems*****information": [
        "$ARG1 relied on minimal $ARG2"
    ],
    "similarity*****variable": [
        "$ARG1 between the patterns of values of the target $ARG2"
    ],
    "learning*****distributed representation": [
        "$ARG1 a $ARG2"
    ],
    "variable*****method": [
        "$ARG1 ( such as a rating ) is a simple parametric $ARG2"
    ],
    "dot product*****embedding": [
        "$ARG1 between the user $ARG2",
        "$ARG1 between this song $ARG2"
    ],
    "other*****word embedding": [
        "$ARG1 , just like $ARG2"
    ],
    "singular value decomposition*****matrix": [
        "$ARG1 of the $ARG2",
        "$ARG1 ( SVD ) provides another way to factorize a $ARG2"
    ],
    "recommender systems*****machine learning": [
        "$ARG1 using advanced $ARG2"
    ],
    "machine learning*****recommender systems": [
        "$ARG1 and yielded improvements in $ARG2"
    ],
    "distributed representation*****neural network": [
        "$ARG1 , one of the \ufb01rst uses of $ARG2"
    ],
    "matrix*****neural network": [
        "$ARG1 have also been explored in the $ARG2"
    ],
    "similarity*****other": [
        "$ARG1 with $ARG2"
    ],
    "information*****content-based recommender systems": [
        "$ARG1 are called $ARG2"
    ],
    "set*****embedding": [
        "$ARG1 of user features or item features to an $ARG2"
    ],
    "embedding*****deep learning": [
        "$ARG1 can be learned through a $ARG2"
    ],
    "content*****audio": [
        "$ARG1 such as from musical $ARG2"
    ],
    "supervised learning*****reinforcement learning": [
        "$ARG1 and into the realm of $ARG2",
        "$ARG1 ( Simard et al. , 1992 ) but also in the context of $ARG2"
    ],
    "system*****view": [
        "$ARG1 to collect data , we get a biased and incomplete $ARG2",
        "$ARG1 , from the point of $ARG2"
    ],
    "view*****other": [
        "$ARG1 of the preferences of users : we only see the responses of users to the items they were recommended and not to the $ARG2",
        "$ARG1 is that we should only compare convolutional models to $ARG2",
        "$ARG1 helps us to derive some of the $ARG2"
    ],
    "information*****example": [
        "$ARG1 on users for whom no recommendation has been made ( for $ARG2",
        "$ARG1 that an $ARG2",
        "$ARG1 THEORY For $ARG2"
    ],
    "information*****other": [
        "$ARG1 about what outcome would have resulted from recommending any of the $ARG2"
    ],
    "system*****probability": [
        "$ARG1 that continues picking the wrong decisions even as more and more data is collected , because the correct decision initially had a very low $ARG2",
        "$ARG1 must assign it a low $ARG2"
    ],
    "special case*****reinforcement learning": [
        "$ARG1 of $ARG2"
    ],
    "contextual bandits*****variable": [
        "$ARG1 refers to the case where the action is taken in the context of some input $ARG2"
    ],
    "mapping*****policy": [
        "$ARG1 from context to action is also called a $ARG2"
    ],
    "loop*****distribution": [
        "$ARG1 between the learner and the data $ARG2"
    ],
    "distribution*****reinforcement learning": [
        "$ARG1 ( which now depends on the actions of the learner ) is a central research issue in the $ARG2"
    ],
    "reinforcement learning*****exploration": [
        "$ARG1 requires choosing a tradeo\ufb00 between $ARG2"
    ],
    "exploration*****exploitation": [
        "$ARG1 and $ARG2",
        "$ARG1 or $ARG2",
        "$ARG1 and $ARG2"
    ],
    "policy*****1": [
        "$ARG1 and continue taking action a in order to be relatively sure of obtaining a reward of $ARG2"
    ],
    "exploration*****expected": [
        "$ARG1 can be implemented in many ways , ranging from occasionally taking random actions intended to cover the entire space of possible actions , to model-based approaches that compute a choice of action based on its $ARG2"
    ],
    "policy*****supervised learning": [
        "$ARG1 improves , we move toward $ARG2"
    ],
    "supervised learning*****exploration": [
        "$ARG1 has no tradeo\ufb00 between $ARG2"
    ],
    "reinforcement learning*****policies": [
        "$ARG1 , besides the exploration-exploitation trade-o\ufb00 , is the di\ufb03culty of evaluating and comparing di\ufb00erent $ARG2"
    ],
    "reinforcement learning*****interaction": [
        "$ARG1 involves $ARG2"
    ],
    "loop*****set": [
        "$ARG1 means that it is not straightforward to evaluate the learner \u2019 s performance using a \ufb01xed $ARG2"
    ],
    "reasoning*****question answering": [
        "$ARG1 and $ARG2"
    ],
    "deep learning*****machine translation": [
        "$ARG1 approaches have been very successful in language modeling , $ARG2"
    ],
    "machine translation*****natural language processing": [
        "$ARG1 and $ARG2"
    ],
    "search*****machine learning": [
        "$ARG1 engines already use $ARG2"
    ],
    "machine learning*****relations": [
        "$ARG1 for this purpose but much more remains to be done to improve these more advanced Knowledge , $ARG2"
    ],
    "relations*****question answering": [
        "$ARG1 and $ARG2"
    ],
    "question answering*****distributed representation": [
        "$ARG1 One interesting research direction is determining how $ARG2"
    ],
    "distributed representation*****relations": [
        "$ARG1 can be trained to capture the $ARG2"
    ],
    "relations*****other": [
        "$ARG1 allow us to formalize facts about objects and how objects interact with each $ARG2",
        "$ARG1 can be considered as any $ARG2"
    ],
    "mathematics*****binary relation": [
        "$ARG1 , a $ARG2"
    ],
    "binary relation*****set": [
        "$ARG1 is a $ARG2"
    ],
    "set*****ordered pair": [
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2"
    ],
    "set*****relation": [
        "$ARG1 are said to have the $ARG2"
    ],
    "relation*****set": [
        "$ARG1 while those who are not in the $ARG2",
        "$ARG1 \u201c is less than \u201d on the $ARG2"
    ],
    "example*****relation": [
        "$ARG1 , we can de\ufb01ne the $ARG2",
        "$ARG1 by replacing one entity in the $ARG2"
    ],
    "ordered pair*****1": [
        "$ARG1 S = { ( 1,2 ) , ( $ARG2"
    ],
    "example*****relations": [
        "$ARG1 , we could de\ufb01ne the has_fur attribute , and apply it to entities like Many applications require representing $ARG2"
    ],
    "relations*****reasoning": [
        "$ARG1 and $ARG2",
        "$ARG1 combined with a $ARG2"
    ],
    "relational database*****information": [
        "$ARG1 , which stores this same kind of $ARG2"
    ],
    "database*****arti\ufb01cial intelligence": [
        "$ARG1 is intended to convey commonsense knowledge about everyday life or expert knowledge about an application area to an $ARG2"
    ],
    "system*****database": [
        "$ARG1 , we call the $ARG2"
    ],
    "database*****base": [
        "$ARG1 a knowledge $ARG2"
    ],
    "freebase*****wordnet": [
        "$ARG1 , OpenCyc , $ARG2"
    ],
    "wordnet*****wikibase": [
        "$ARG1 , or $ARG2"
    ],
    "wikibase*****1": [
        "$ARG1 , $ARG2"
    ],
    "relations*****base": [
        "$ARG1 can be learned by considering each triplet in a knowledge $ARG2",
        "$ARG1 absent from the knowledge $ARG2"
    ],
    "base*****example": [
        "$ARG1 as a training $ARG2"
    ],
    "model*****relations": [
        "$ARG1 entities and $ARG2"
    ],
    "neural language model*****vector": [
        "$ARG1 learn a $ARG2"
    ],
    "vector*****distributed representation": [
        "$ARG1 that provides a $ARG2"
    ],
    "relations*****learning": [
        "$ARG1 by $ARG2"
    ],
    "learning*****embedding": [
        "$ARG1 an $ARG2"
    ],
    "vector*****relation": [
        "$ARG1 for each $ARG2",
        "$ARG1 of variational parameters z and obtain h\u0302 via the $ARG2"
    ],
    "relations*****multiple": [
        "$ARG1 is so close that researchers have trained representations of such entities by using both knowledge bases and natural language sentences ( Bordes et al. , 2011 , 2012 ; Wang et al. , 2014a ) or combining data from $ARG2"
    ],
    "learning*****relations": [
        "$ARG1 about $ARG2"
    ],
    "relations*****relation": [
        "$ARG1 between entities ( Paccanaro and Hinton , 2000 ) posited highly constrained parametric forms ( \u201c linear relational embeddings \u201d ) , often using a di\ufb00erent form of representation for the $ARG2",
        "$ARG1 , with the idea that a $ARG2"
    ],
    "other*****relations": [
        "$ARG1 entity ( Bordes et al. , 2012 ) , allowing us to make statements about $ARG2"
    ],
    "relations*****model": [
        "$ARG1 , but more \ufb02exibility is put in the machinery that combines them in order to $ARG2"
    ],
    "link prediction*****graph": [
        "$ARG1 : predicting missing arcs in the knowledge $ARG2"
    ],
    "model*****link prediction": [
        "$ARG1 on a $ARG2"
    ],
    "task*****dataset": [
        "$ARG1 is di\ufb03cult because we have only a $ARG2",
        "$ARG1 can fail to learn anything useful about the $ARG2",
        "$ARG1 was preceded by a signi\ufb01cant amount of $ARG2"
    ],
    "dataset*****model": [
        "$ARG1 , we are unsure whether the $ARG2",
        "$ARG1 size on the train and test error , as well as on the optimal $ARG2",
        "$ARG1 , the $ARG2"
    ],
    "precision*****model": [
        "$ARG1 at 10 % metric counts how many times the $ARG2",
        "$ARG1 is the fraction of detections reported by the $ARG2"
    ],
    "distributed representation*****word-sense disambiguation": [
        "$ARG1 for them is $ARG2"
    ],
    "word-sense disambiguation*****task": [
        "$ARG1 ( Navigli and Velardi , 2005 ; Bordes et al. , 2012 ) , which is the $ARG2"
    ],
    "process*****question answering": [
        "$ARG1 and understanding of natural language could allow us to build a general $ARG2"
    ],
    "system*****process": [
        "$ARG1 must be able to $ARG2",
        "$ARG1 should $ARG2",
        "$ARG1 to $ARG2"
    ],
    "process*****information": [
        "$ARG1 input $ARG2"
    ],
    "memory network*****question answering": [
        "$ARG1 were \ufb01rst proposed to solve a toy $ARG2"
    ],
    "turn*****challenges": [
        "$ARG1 to part III , in which we step into the territory of research\u2014methods that are designed to work with less training data or to perform a greater variety of tasks , where the $ARG2"
    ],
    "machine learning*****coverage": [
        "$ARG1 textbooks with a more comprehensive $ARG2"
    ],
    "coverage*****bishop": [
        "$ARG1 of the fundamentals , such as Murphy ( 2012 ) or $ARG2"
    ],
    "machine learning*****hyperparameters": [
        "$ARG1 algorithms have settings called $ARG2"
    ],
    "hyperparameters*****learning": [
        "$ARG1 that must be determined external to the $ARG2",
        "$ARG1 are not adapted by the $ARG2",
        "$ARG1 for another $ARG2",
        "$ARG1 are switches that specify whether or not to use some optional component of the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 , such as the range of values that should be explored for each of the $ARG2"
    ],
    "statistics*****inference": [
        "$ARG1 : frequentist estimators and Bayesian $ARG2"
    ],
    "machine learning*****supervised learning": [
        "$ARG1 algorithms can be divided into the categories of $ARG2",
        "$ARG1 BASICS can solve the $ARG2",
        "$ARG1 BASICS This same strategy can be applied to essentially any $ARG2",
        "$ARG1 textbooks for more material on traditional $ARG2"
    ],
    "unsupervised learning*****learning": [
        "$ARG1 ; we describe these categories and give some examples of simple $ARG2",
        "$ARG1 techniques such as $ARG2"
    ],
    "deep learning*****optimization algorithm": [
        "$ARG1 algorithms are based on an $ARG2"
    ],
    "optimization algorithm*****cost function": [
        "$ARG1 , a $ARG2",
        "$ARG1 may not be guaranteed to arrive at even a local minimum in a reasonable amount of time , but it often \ufb01nds a very low value of the $ARG2"
    ],
    "learning*****measure": [
        "$ARG1 ? Mitchell ( 1997 ) provides the de\ufb01nition \u201c A computer program is said to learn from experience E with respect to some class of tasks T and performance $ARG2"
    ],
    "view*****machine learning": [
        "$ARG1 , $ARG2",
        "$ARG1 of the target y being provided by an instructor or teacher who shows the $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "task*****process": [
        "$ARG1 , \u201d the $ARG2",
        "$ARG1 , the $ARG2"
    ],
    "learning*****task": [
        "$ARG1 itself is not the $ARG2",
        "$ARG1 is our means of attaining the ability to perform the $ARG2"
    ],
    "example*****collection": [
        "$ARG1 is a $ARG2",
        "$ARG1 of a class , but the individual members of the $ARG2",
        "$ARG1 , if you have a $ARG2",
        "$ARG1 , consider a $ARG2",
        "$ARG1 , if your data $ARG2"
    ],
    "collection*****machine learning": [
        "$ARG1 of features that have been quantitatively measured from some object or event that we want the $ARG2"
    ],
    "vector*****feature": [
        "$ARG1 is another $ARG2"
    ],
    "machine learning*****classi\ufb01cation": [
        "$ARG1 tasks include the following : \u2022 $ARG2",
        "$ARG1 tasks , it is reasonably straightforward to create new This approach is easiest for $ARG2"
    ],
    "classi\ufb01cation*****task": [
        "$ARG1 : In this type of $ARG2",
        "$ARG1 and even some regression tasks , the $ARG2"
    ],
    "example*****probability distribution": [
        "$ARG1 , where f outputs a $ARG2",
        "$ARG1 , some regularizer terms may not be the logarithm of a $ARG2"
    ],
    "task*****object recognition": [
        "$ARG1 is $ARG2"
    ],
    "set*****image": [
        "$ARG1 of pixel brightness values ) , and the output is a numeric code identifying the object in the $ARG2"
    ],
    "example*****command": [
        "$ARG1 , the Willow Garage PR2 robot is able to act as a waiter that can recognize di\ufb00erent kinds of drinks and deliver them to people on $ARG2"
    ],
    "object recognition*****deep learning": [
        "$ARG1 is best accomplished with $ARG2"
    ],
    "classi\ufb01cation*****vector": [
        "$ARG1 becomes more challenging if the computer program is not guaranteed that every measurement in its input $ARG2"
    ],
    "probability distribution*****classi\ufb01cation": [
        "$ARG1 over all of the relevant variables , then solve the $ARG2"
    ],
    "classi\ufb01cation*****set": [
        "$ARG1 functions needed for each possible $ARG2"
    ],
    "set*****missing inputs": [
        "$ARG1 of $ARG2"
    ],
    "missing inputs*****function": [
        "$ARG1 , but we only need to learn a single $ARG2"
    ],
    "function*****joint probability distribution": [
        "$ARG1 describing the $ARG2"
    ],
    "other*****missing inputs": [
        "$ARG1 tasks described in this section can also be generalized to work with $ARG2"
    ],
    "missing inputs*****example": [
        "$ARG1 is just one $ARG2"
    ],
    "machine learning*****task": [
        "$ARG1 BASICS \u2022 Regression : In this type of $ARG2",
        "$ARG1 BASICS \u2022 Denoising : In this type of $ARG2",
        "$ARG1 algorithms will generally perform best when their capacity is appropriate for the true complexity of the $ARG2",
        "$ARG1 algorithms to perform well on a speci\ufb01c $ARG2",
        "$ARG1 goal of solving a $ARG2"
    ],
    "task*****classi\ufb01cation": [
        "$ARG1 is similar to $ARG2",
        "$ARG1 such as $ARG2",
        "$ARG1 were $ARG2"
    ],
    "classi\ufb01cation*****format": [
        "$ARG1 , except that the $ARG2"
    ],
    "task*****expected": [
        "$ARG1 is the prediction of the $ARG2",
        "$ARG1 and it achieves 5 % test error , we have no straightforward way of knowing if this is the $ARG2"
    ],
    "expected*****set": [
        "$ARG1 claim amount that an insured person will make ( used to $ARG2",
        "$ARG1 training $ARG2",
        "$ARG1 test $ARG2",
        "$ARG1 to be necessary to improve test $ARG2"
    ],
    "transcription*****task": [
        "$ARG1 : In this type of $ARG2"
    ],
    "example*****character": [
        "$ARG1 , in optical $ARG2",
        "$ARG1 , optical $ARG2"
    ],
    "character*****image": [
        "$ARG1 recognition , the computer program is shown a photograph containing an $ARG2"
    ],
    "image*****format": [
        "$ARG1 of text and is asked to return this text in the form of a sequence of characters ( e.g. , in ASCII or Unicode $ARG2"
    ],
    "deep learning*****process": [
        "$ARG1 to $ARG2"
    ],
    "speech recognition*****audio": [
        "$ARG1 , where the computer program is provided an $ARG2"
    ],
    "structured output*****task": [
        "$ARG1 tasks involve any $ARG2"
    ],
    "task*****vector": [
        "$ARG1 where the output is a $ARG2"
    ],
    "vector*****other": [
        "$ARG1 ( or $ARG2",
        "$ARG1 y are orthogonal to each $ARG2"
    ],
    "data structure*****multiple": [
        "$ARG1 containing $ARG2"
    ],
    "transcription*****translation": [
        "$ARG1 and $ARG2"
    ],
    "translation*****other": [
        "$ARG1 tasks described above , but also many $ARG2"
    ],
    "information*****probability distribution": [
        "$ARG1 , the thief \u2019 s purchases will often come from a di\ufb00erent $ARG2",
        "$ARG1 in the input and produces a single output ( or a $ARG2",
        "$ARG1 theory allows us to quantify the amount of uncertainty in a $ARG2",
        "$ARG1 theory to characterize $ARG2"
    ],
    "synthesis*****task": [
        "$ARG1 and sampling : In this type of $ARG2"
    ],
    "synthesis*****machine learning": [
        "$ARG1 and sampling via $ARG2"
    ],
    "machine learning*****content": [
        "$ARG1 can be useful for media applications where it can be expensive or boring for an artist to generate large volumes of $ARG2"
    ],
    "example*****video game": [
        "$ARG1 , $ARG2"
    ],
    "task*****audio": [
        "$ARG1 , we provide a written sentence and ask the program to emit an $ARG2"
    ],
    "example*****conditional probability distribution": [
        "$ARG1 x from its corrupted version x\u0303 , or more generally predict the $ARG2"
    ],
    "density estimation*****probability mass function estimation": [
        "$ARG1 or $ARG2"
    ],
    "probability mass function estimation*****density estimation": [
        "$ARG1 : In the $ARG2"
    ],
    "function*****probability density function": [
        "$ARG1 pmodel : R n \u2192 R , where pmodel ( x ) can be interpreted as a $ARG2",
        "$ARG1 f ( x ) , such as when we want to \ufb01nd the $ARG2"
    ],
    "probability density function*****probability mass function": [
        "$ARG1 ( if x is continuous ) or a $ARG2",
        "$ARG1 ( PDF ) rather than a $ARG2"
    ],
    "task*****algorithm": [
        "$ARG1 well ( we will specify exactly what that means when we discuss performance measures P ) , the $ARG2"
    ],
    "density estimation*****distribution": [
        "$ARG1 allows us to explicitly capture that $ARG2"
    ],
    "principle*****distribution": [
        "$ARG1 , we can then perform computations on that $ARG2"
    ],
    "density estimation*****probability distribution": [
        "$ARG1 to obtain a $ARG2"
    ],
    "probability distribution*****distribution": [
        "$ARG1 p ( x ) , we can use that $ARG2",
        "$ARG1 p ( x ) , or some interesting properties of that $ARG2",
        "$ARG1 over b via a conditional $ARG2",
        "$ARG1 or which approach uses the fewest edges to describe the $ARG2",
        "$ARG1 to create a richer $ARG2"
    ],
    "density estimation*****other": [
        "$ARG1 does not always allow us to solve all of these related tasks , because in many cases the required operations on p ( x ) are computationally Of course , many $ARG2"
    ],
    "measure*****machine learning": [
        "$ARG1 , P In order to evaluate the abilities of a $ARG2",
        "$ARG1 the e\ufb00ectiveness of a complete application that includes $ARG2"
    ],
    "algorithm*****measure": [
        "$ARG1 , we must design a quantitative $ARG2",
        "$ARG1 to guess which single attribute we care about , and allows us to $ARG2"
    ],
    "measure*****task": [
        "$ARG1 P is speci\ufb01c to the $ARG2",
        "$ARG1 that gives partial credit for getting some elements of the sequence correct ? When performing a regression $ARG2",
        "$ARG1 for the $ARG2"
    ],
    "task*****system": [
        "$ARG1 T being carried out by the $ARG2",
        "$ARG1 , should we penalize the $ARG2"
    ],
    "missing inputs*****transcription": [
        "$ARG1 , and $ARG2"
    ],
    "transcription*****measure": [
        "$ARG1 , we often $ARG2"
    ],
    "measure*****accuracy": [
        "$ARG1 the $ARG2",
        "$ARG1 the $ARG2",
        "$ARG1 the $ARG2"
    ],
    "machine learning*****information": [
        "$ARG1 BASICS also obtain equivalent $ARG2",
        "$ARG1 , we can also apply $ARG2"
    ],
    "information*****rate": [
        "$ARG1 by measuring the error $ARG2"
    ],
    "rate*****model": [
        "$ARG1 , the proportion of examples for which the $ARG2"
    ],
    "rate*****expected": [
        "$ARG1 as the $ARG2"
    ],
    "density estimation*****measure": [
        "$ARG1 , it does not make sense to $ARG2"
    ],
    "accuracy*****rate": [
        "$ARG1 , error $ARG2",
        "$ARG1 have a nearly 100 % error $ARG2",
        "$ARG1 , or equivalently , the error $ARG2"
    ],
    "rate*****other": [
        "$ARG1 , or any $ARG2"
    ],
    "other*****loss": [
        "$ARG1 kind of 0-1 $ARG2"
    ],
    "average*****model": [
        "$ARG1 log-probability the $ARG2"
    ],
    "set*****measure": [
        "$ARG1 of data that is separate from the data used for training the machine The choice of performance $ARG2",
        "$ARG1 , we can compute some error $ARG2",
        "$ARG1 is acceptable , then $ARG2"
    ],
    "measure*****system": [
        "$ARG1 that corresponds well to the desired behavior of the $ARG2"
    ],
    "example*****transcription": [
        "$ARG1 , when performing a $ARG2"
    ],
    "task*****measure": [
        "$ARG1 , should we $ARG2",
        "$ARG1 and $ARG2"
    ],
    "accuracy*****system": [
        "$ARG1 of the $ARG2",
        "$ARG1 is a poor way to characterize the performance of such a $ARG2"
    ],
    "system*****measure": [
        "$ARG1 at transcribing entire sequences , or should we use a more \ufb01ne-grained performance $ARG2",
        "$ARG1 is established , it is time to $ARG2"
    ],
    "other*****measure": [
        "$ARG1 cases , we know what quantity we would ideally like to $ARG2",
        "$ARG1 hand , provides a $ARG2"
    ],
    "learning*****dataset": [
        "$ARG1 algorithms in this book can be understood as being allowed to experience an entire $ARG2"
    ],
    "dataset*****collection": [
        "$ARG1 is a $ARG2",
        "$ARG1 is a $ARG2",
        "$ARG1 or $ARG2"
    ],
    "unsupervised learning*****dataset": [
        "$ARG1 algorithms experience a $ARG2",
        "$ARG1 can be supported by de\ufb01ning a $ARG2"
    ],
    "deep learning*****probability distribution": [
        "$ARG1 , we usually want to learn the entire $ARG2",
        "$ARG1 Figure 16.10 : Examples of complete graphs , which can describe any $ARG2",
        "$ARG1 , we often want to have a $ARG2"
    ],
    "probability distribution*****dataset": [
        "$ARG1 that generated a $ARG2"
    ],
    "dataset*****density estimation": [
        "$ARG1 , whether explicitly as in $ARG2"
    ],
    "density estimation*****synthesis": [
        "$ARG1 or implicitly for tasks like $ARG2"
    ],
    "unsupervised learning*****other": [
        "$ARG1 algorithms perform $ARG2"
    ],
    "other*****dataset": [
        "$ARG1 roles , like clustering , which consists of dividing the $ARG2"
    ],
    "supervised learning*****dataset": [
        "$ARG1 algorithms experience a $ARG2"
    ],
    "algorithm*****dataset": [
        "$ARG1 can study the Iris $ARG2",
        "$ARG1 is able to achieve the minimum possible training error ( which might be greater than zero , if two identical inputs are associated with di\ufb00erent outputs ) on any regression $ARG2",
        "$ARG1 5.1 , in which a partition of the $ARG2",
        "$ARG1 A when the given $ARG2",
        "$ARG1 De\ufb01ne KFoldXV ( D , A , L , k ) : Require : D , the given $ARG2",
        "$ARG1 combines a $ARG2",
        "$ARG1 just takes a $ARG2"
    ],
    "unsupervised learning*****vector": [
        "$ARG1 involves observing several examples of a random $ARG2"
    ],
    "vector*****probability distribution": [
        "$ARG1 x , and attempting to implicitly or explicitly learn the $ARG2",
        "$ARG1 \u00b5 de\ufb01nes a $ARG2"
    ],
    "distribution*****supervised learning": [
        "$ARG1 , while $ARG2"
    ],
    "vector*****learning": [
        "$ARG1 y , and $ARG2"
    ],
    "supervised learning*****view": [
        "$ARG1 originates from the $ARG2"
    ],
    "unsupervised learning*****algorithm": [
        "$ARG1 , there is no instructor or teacher , and the $ARG2"
    ],
    "example*****chain rule of probability": [
        "$ARG1 , the $ARG2"
    ],
    "chain rule of probability*****vector": [
        "$ARG1 states that for a $ARG2"
    ],
    "learning*****unsupervised learning": [
        "$ARG1 p ( y | x ) by using traditional $ARG2"
    ],
    "supervised learning*****machine learning": [
        "$ARG1 are not completely formal or distinct concepts , they do help to roughly categorize some of the things we do with $ARG2"
    ],
    "classi\ufb01cation*****structured output": [
        "$ARG1 and $ARG2"
    ],
    "structured output*****supervised learning": [
        "$ARG1 problems as $ARG2"
    ],
    "density estimation*****support": [
        "$ARG1 in $ARG2"
    ],
    "support*****other": [
        "$ARG1 of $ARG2"
    ],
    "learning*****collection": [
        "$ARG1 , an entire $ARG2"
    ],
    "collection*****example": [
        "$ARG1 of examples is labeled as containing or not containing an $ARG2"
    ],
    "example*****reinforcement learning": [
        "$ARG1 , $ARG2"
    ],
    "reinforcement learning*****loop": [
        "$ARG1 algorithms interact with an environment , so there is a feedback $ARG2"
    ],
    "loop*****learning": [
        "$ARG1 between the $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of a $ARG2"
    ],
    "information*****reinforcement learning": [
        "$ARG1 about $ARG2"
    ],
    "collection*****turn": [
        "$ARG1 of examples , which are in $ARG2"
    ],
    "dataset*****design matrix": [
        "$ARG1 is with a $ARG2",
        "$ARG1 with a $ARG2",
        "$ARG1 as a $ARG2",
        "$ARG1 containing a $ARG2"
    ],
    "design matrix*****matrix": [
        "$ARG1 is a $ARG2"
    ],
    "matrix*****example": [
        "$ARG1 containing a di\ufb00erent $ARG2"
    ],
    "matrix*****feature": [
        "$ARG1 corresponds to a di\ufb00erent $ARG2"
    ],
    "contains*****example": [
        "$ARG1 150 examples with four features for each $ARG2",
        "$ARG1 as elements the abstract examples z ( i ) ( for the i-th $ARG2"
    ],
    "learning*****design matrix": [
        "$ARG1 algorithms in this book in terms of how they operate on $ARG2"
    ],
    "collection*****vector": [
        "$ARG1 of photographs with di\ufb00erent widths and heights , then di\ufb00erent photographs will contain di\ufb00erent numbers of pixels , so not all of the photographs may be described with the same length of $ARG2"
    ],
    "dataset*****matrix": [
        "$ARG1 as a $ARG2"
    ],
    "supervised learning*****example": [
        "$ARG1 , the $ARG2"
    ],
    "contains*****collection": [
        "$ARG1 a label or target as well as a $ARG2"
    ],
    "algorithm*****object recognition": [
        "$ARG1 to perform $ARG2"
    ],
    "design matrix*****feature": [
        "$ARG1 of $ARG2"
    ],
    "feature*****vector": [
        "$ARG1 observations X , we also provide a $ARG2",
        "$ARG1 , the weight $ARG2",
        "$ARG1 spaces , we may not use literally the $ARG2"
    ],
    "vector*****example": [
        "$ARG1 of labels y , with yi providing the label for $ARG2",
        "$ARG1 of errors e for each $ARG2",
        "$ARG1 associated with each $ARG2"
    ],
    "system*****example": [
        "$ARG1 to transcribe entire sentences , then the label for each $ARG2",
        "$ARG1 that associates each training $ARG2",
        "$ARG1 provides an $ARG2"
    ],
    "linear regression*****machine learning": [
        "$ARG1 Our de\ufb01nition of a $ARG2"
    ],
    "algorithm*****task": [
        "$ARG1 that is capable of improving a computer program \u2019 s performance at some $ARG2",
        "$ARG1 B on the given $ARG2",
        "$ARG1 encodes his or her prior knowledge of the $ARG2",
        "$ARG1 that is already known to perform best on the previously studied $ARG2"
    ],
    "algorithm*****linear regression": [
        "$ARG1 : $ARG2",
        "$ARG1 we have studied so far , $ARG2"
    ],
    "system*****vector": [
        "$ARG1 that can take a $ARG2"
    ],
    "linear regression*****function": [
        "$ARG1 , the output is a linear $ARG2",
        "$ARG1 ) where our goal is to estimate the value of a parameter ( and assuming it is possible to identify the true parameter ) , not the value of a $ARG2"
    ],
    "weights*****feature": [
        "$ARG1 that determine how each $ARG2"
    ],
    "machine learning*****feature": [
        "$ARG1 BASICS then increasing the value of that $ARG2"
    ],
    "feature*****magnitude": [
        "$ARG1 \u2019 s weight is large in $ARG2"
    ],
    "mean squared error*****model": [
        "$ARG1 of the $ARG2",
        "$ARG1 as the reconstruction cost is equivalent ( Vincent , 2011 ) to training a speci\ufb01c kind of undirected probabilistic $ARG2"
    ],
    "set*****mean squared error": [
        "$ARG1 , then the $ARG2"
    ],
    "mean squared error*****1": [
        "$ARG1 is given by $ARG2"
    ],
    "weights*****algorithm": [
        "$ARG1 w in a way that reduces MSEtest when the $ARG2"
    ],
    "mean squared error*****set": [
        "$ARG1 on the training $ARG2",
        "$ARG1 on the training $ARG2"
    ],
    "mse*****gradient": [
        "$ARG1 train , we can simply solve for where its $ARG2"
    ],
    "machine learning*****mse": [
        "$ARG1 BASICS $ARG2",
        "$ARG1 BASICS The $ARG2"
    ],
    "mse*****linear regression": [
        "$ARG1 ( train ) Figure 5.1 : A $ARG2"
    ],
    "set*****feature": [
        "$ARG1 consisting of ten data points , each containing one $ARG2",
        "$ARG1 of kernels , rather than the size of the entire output $ARG2"
    ],
    "vector*****contains": [
        "$ARG1 w $ARG2",
        "$ARG1 that $ARG2"
    ],
    "linear regression*****set": [
        "$ARG1 learns to $ARG2"
    ],
    "normal equations*****mean squared error": [
        "$ARG1 , which we can see minimizes the $ARG2"
    ],
    "system*****solution": [
        "$ARG1 of equations whose $ARG2",
        "$ARG1 Ax = b to have a $ARG2"
    ],
    "solution*****normal equations": [
        "$ARG1 is given by equation 5.12 is known as the $ARG2"
    ],
    "linear regression*****model": [
        "$ARG1 is often used to refer to a slightly more sophisticated $ARG2",
        "$ARG1 , a $ARG2"
    ],
    "model*****mapping": [
        "$ARG1 y\u0302 = w \ue03ex + b so the $ARG2"
    ],
    "function*****mapping": [
        "$ARG1 but the $ARG2",
        "$ARG1 f\u02c6 $ARG2"
    ],
    "mapping*****a\ufb03ne": [
        "$ARG1 from features to predictions is now an $ARG2"
    ],
    "a\ufb03ne*****model": [
        "$ARG1 functions means that the plot of the $ARG2"
    ],
    "model*****pass": [
        "$ARG1 \u2019 s predictions still looks like a line , but it need not $ARG2"
    ],
    "1*****bias parameter": [
        "$ARG1 entry plays the role of the $ARG2"
    ],
    "bias*****algorithm": [
        "$ARG1 , in which a statistical estimation $ARG2"
    ],
    "linear regression*****learning": [
        "$ARG1 is of course an extremely simple and limited $ARG2",
        "$ARG1 Here we consider the Bayesian estimation approach to $ARG2"
    ],
    "algorithm*****challenge": [
        "$ARG1 design and demonstrate how these principles can be used to build more complicated Capacity , Over\ufb01tting and Under\ufb01tting The central $ARG2"
    ],
    "challenge*****machine learning": [
        "$ARG1 in $ARG2"
    ],
    "generalization*****expected value": [
        "$ARG1 error is de\ufb01ned as the $ARG2"
    ],
    "distribution*****system": [
        "$ARG1 of inputs we expect the $ARG2"
    ],
    "generalization*****machine learning": [
        "$ARG1 error of a $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 in traditional $ARG2",
        "$ARG1 error of a $ARG2"
    ],
    "machine learning*****set": [
        "$ARG1 BASICS training $ARG2",
        "$ARG1 promises to \ufb01nd rules that are probably correct about most members of the $ARG2",
        "$ARG1 BASICS region containing all the points x that have the same $ARG2",
        "$ARG1 it tends to be used more loosely to designate a connected $ARG2"
    ],
    "set*****statistical learning theory": [
        "$ARG1 ? The \ufb01eld of $ARG2"
    ],
    "probability distribution*****data generating process": [
        "$ARG1 over datasets called the $ARG2"
    ],
    "probability distribution*****other": [
        "$ARG1 as each $ARG2",
        "$ARG1 interact with each $ARG2",
        "$ARG1 is de\ufb01ned in terms of the $ARG2",
        "$ARG1 over some variables given the value of $ARG2",
        "$ARG1 by combining $ARG2"
    ],
    "assumption*****data generating process": [
        "$ARG1 allows us to describe the $ARG2"
    ],
    "data generating process*****probability distribution": [
        "$ARG1 with a $ARG2"
    ],
    "probability distribution*****example": [
        "$ARG1 over a single $ARG2"
    ],
    "distribution*****data generating distribution": [
        "$ARG1 the $ARG2"
    ],
    "connection*****expected": [
        "$ARG1 we can observe between the training and test error is that the $ARG2"
    ],
    "model*****expected": [
        "$ARG1 is equal to the $ARG2"
    ],
    "set*****expected": [
        "$ARG1 error is exactly the same as the $ARG2"
    ],
    "set*****dataset": [
        "$ARG1 error , because both expectations are formed using the same $ARG2"
    ],
    "dataset*****process": [
        "$ARG1 sampling $ARG2"
    ],
    "conditions*****dataset": [
        "$ARG1 is the name we assign to the $ARG2"
    ],
    "process*****expected": [
        "$ARG1 , the $ARG2"
    ],
    "expected*****expected value": [
        "$ARG1 test error is greater than or equal to the $ARG2"
    ],
    "challenges*****machine learning": [
        "$ARG1 in $ARG2"
    ],
    "algorithm*****hypothesis space": [
        "$ARG1 is by choosing its $ARG2"
    ],
    "hypothesis space*****set": [
        "$ARG1 , the $ARG2",
        "$ARG1 consisting of the $ARG2"
    ],
    "algorithm*****solution": [
        "$ARG1 is allowed to select as being the $ARG2",
        "$ARG1 a preference for one $ARG2"
    ],
    "set*****hypothesis space": [
        "$ARG1 of all linear functions of its input as its $ARG2",
        "$ARG1 of functions allowed in its $ARG2"
    ],
    "linear regression*****hypothesis space": [
        "$ARG1 to include polynomials , rather than just linear functions , in its $ARG2",
        "$ARG1 , has a $ARG2"
    ],
    "feature*****linear regression": [
        "$ARG1 provided to the $ARG2"
    ],
    "function*****normal equations": [
        "$ARG1 of the parameters , so we can still use the $ARG2"
    ],
    "normal equations*****model": [
        "$ARG1 to train the $ARG2"
    ],
    "task*****principle": [
        "$ARG1 they Figure 5.2 shows this $ARG2"
    ],
    "problem*****function": [
        "$ARG1 where the true underlying $ARG2",
        "$ARG1 as just a $ARG2",
        "$ARG1 with this approach is that it fails to account for the distortion of space introduced by the $ARG2"
    ],
    "function*****problem": [
        "$ARG1 is unable to capture the curvature in the true underlying $ARG2"
    ],
    "other*****pass": [
        "$ARG1 functions that $ARG2"
    ],
    "moore-penrose pseudoinverse*****normal equations": [
        "$ARG1 to solve the underdetermined $ARG2"
    ],
    "model*****learning": [
        "$ARG1 speci\ufb01es which family of functions the $ARG2",
        "$ARG1 structures , $ARG2",
        "$ARG1 family , and evolves throughout the $ARG2",
        "$ARG1 that is intended not merely to learn to denoise its input but to learn a good internal representation as a side e\ufb00ect of $ARG2",
        "$ARG1 , the ability of the $ARG2"
    ],
    "representational capacity*****model": [
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "function*****optimization problem": [
        "$ARG1 within this family is a very di\ufb03cult $ARG2"
    ],
    "algorithm*****e\ufb00ective capacity": [
        "$ARG1 \u2019 s $ARG2"
    ],
    "e\ufb00ective capacity*****representational capacity": [
        "$ARG1 may be less than the $ARG2",
        "$ARG1 is constrained by three factors : the $ARG2"
    ],
    "statistical learning theory*****model": [
        "$ARG1 provides various means of quantifying $ARG2"
    ],
    "vapnik-chervonenkis dimension*****vc dimension": [
        "$ARG1 , or $ARG2"
    ],
    "vc dimension*****set": [
        "$ARG1 is de\ufb01ned as being the largest possible value of m for which there exists a training $ARG2"
    ],
    "model*****statistical learning theory": [
        "$ARG1 allows $ARG2"
    ],
    "statistical learning theory*****generalization": [
        "$ARG1 show that the discrepancy between training error and $ARG2"
    ],
    "model*****e\ufb00ective capacity": [
        "$ARG1 is especially di\ufb03cult because the $ARG2",
        "$ARG1 with low $ARG2"
    ],
    "e\ufb00ective capacity*****optimization algorithm": [
        "$ARG1 is limited by the capabilities of the $ARG2"
    ],
    "graph*****generalization": [
        "$ARG1 , training error and $ARG2"
    ],
    "concept*****non-parametric model": [
        "$ARG1 of $ARG2"
    ],
    "parametric model*****linear regression": [
        "$ARG1 , such as $ARG2"
    ],
    "parametric model*****function": [
        "$ARG1 learn a $ARG2"
    ],
    "non-parametric model*****algorithm": [
        "$ARG1 are just theoretical abstractions ( such as an $ARG2"
    ],
    "non-parametric model*****function": [
        "$ARG1 by making their complexity a $ARG2"
    ],
    "algorithm*****nearest neighbor regression": [
        "$ARG1 is $ARG2",
        "$ARG1 , $ARG2"
    ],
    "linear regression*****vector": [
        "$ARG1 , which has a \ufb01xed-length $ARG2"
    ],
    "weights*****nearest neighbor regression": [
        "$ARG1 , the $ARG2"
    ],
    "algorithm*****distance": [
        "$ARG1 can also be generalized to $ARG2",
        "$ARG1 in which the metric used is not the generic Euclidean $ARG2"
    ],
    "other*****norm": [
        "$ARG1 than the L2 $ARG2"
    ],
    "norm*****distance": [
        "$ARG1 , such as learned $ARG2"
    ],
    "example*****loop": [
        "$ARG1 , we could imagine an outer $ARG2"
    ],
    "learning*****linear regression": [
        "$ARG1 that changes the degree of the polynomial learned by $ARG2",
        "$ARG1 the $ARG2"
    ],
    "supervised learning*****mapping": [
        "$ARG1 , the $ARG2"
    ],
    "mapping*****stochastic": [
        "$ARG1 from x to y may be inherently $ARG2"
    ],
    "stochastic*****function": [
        "$ARG1 , or y may be a deterministic $ARG2"
    ],
    "distribution*****bayes error": [
        "$ARG1 p ( x , y ) is called the $ARG2"
    ],
    "non-parametric model*****generalization": [
        "$ARG1 , more data yields better $ARG2"
    ],
    "parametric model*****bayes error": [
        "$ARG1 with less than optimal capacity will asymptote to an error value that exceeds the $ARG2"
    ],
    "inductive reasoning*****set": [
        "$ARG1 , or inferring general rules from a limited $ARG2"
    ],
    "rule*****set": [
        "$ARG1 describing every member of a $ARG2"
    ],
    "information*****set": [
        "$ARG1 about every member of that $ARG2"
    ],
    "machine learning*****problem": [
        "$ARG1 avoids this $ARG2",
        "$ARG1 BASICS One di\ufb03culty pertaining to clustering is that the clustering $ARG2"
    ],
    "problem*****reasoning": [
        "$ARG1 by o\ufb00ering only probabilistic rules , rather than the entirely certain rules used in purely logical $ARG2"
    ],
    "no free lunch theorem*****machine learning": [
        "$ARG1 for $ARG2",
        "$ARG1 implies that we must design our $ARG2",
        "$ARG1 has made it clear that there is no best $ARG2"
    ],
    "machine learning*****data generating distribution": [
        "$ARG1 ( Wolpert , 1996 ) states that , averaged over all possible $ARG2",
        "$ARG1 algorithms perform well on data drawn from the kinds of $ARG2"
    ],
    "data generating distribution*****classi\ufb01cation": [
        "$ARG1 , every $ARG2"
    ],
    "algorithm*****rate": [
        "$ARG1 has the same error $ARG2"
    ],
    "mse*****set": [
        "$ARG1 on the training and test $ARG2"
    ],
    "bayes error*****algorithm": [
        "$ARG1 , due to the ability of the training $ARG2"
    ],
    "model*****bayes error": [
        "$ARG1 ) must rise to at least the $ARG2"
    ],
    "average*****data generating distribution": [
        "$ARG1 over all possible $ARG2"
    ],
    "algorithm*****learning": [
        "$ARG1 or the absolute best $ARG2",
        "$ARG1 itself ( though we can design a nested $ARG2",
        "$ARG1 allows us to make large $ARG2",
        "$ARG1 a\ufb00ects the $ARG2"
    ],
    "model*****representational capacity": [
        "$ARG1 \u2019 s $ARG2",
        "$ARG1 with more layers and more hidden units per layer has higher $ARG2"
    ],
    "representational capacity*****hypothesis space": [
        "$ARG1 by adding or removing functions from the $ARG2"
    ],
    "hypothesis space*****learning": [
        "$ARG1 of solutions the $ARG2"
    ],
    "example*****problem": [
        "$ARG1 of increasing or decreasing the degree of a polynomial for a regression $ARG2",
        "$ARG1 , if we have a two-class $ARG2",
        "$ARG1 , a convex $ARG2",
        "$ARG1 , in the Monty Hall $ARG2"
    ],
    "hypothesis space*****identity": [
        "$ARG1 , but by the speci\ufb01c $ARG2"
    ],
    "solution*****hypothesis space": [
        "$ARG1 in its $ARG2"
    ],
    "machine learning*****solution": [
        "$ARG1 BASICS data signi\ufb01cantly better than the preferred $ARG2"
    ],
    "linear regression*****weight decay": [
        "$ARG1 to include $ARG2",
        "$ARG1 with $ARG2",
        "$ARG1 with a $ARG2",
        "$ARG1 with $ARG2"
    ],
    "weight decay*****mean squared error": [
        "$ARG1 , we minimize a sum comprising both the $ARG2"
    ],
    "mean squared error*****weights": [
        "$ARG1 on the training and a criterion J ( w ) that expresses a preference for the $ARG2"
    ],
    "weights*****norm": [
        "$ARG1 to have smaller squared L2 $ARG2",
        "$ARG1 the relative contribution of the $ARG2",
        "$ARG1 that should be a\ufb00ected by a $ARG2",
        "$ARG1 the relative contribution of the $ARG2"
    ],
    "model*****weight decay": [
        "$ARG1 \u2019 s tendency to over\ufb01t or under\ufb01t via $ARG2",
        "$ARG1 is capable of representing functions with much more complicated shape , $ARG2",
        "$ARG1 is under\ufb01tting when $ARG2"
    ],
    "weight decay*****regression model": [
        "$ARG1 , we can train a high-degree polynomial $ARG2"
    ],
    "regression model*****example": [
        "$ARG1 to our $ARG2"
    ],
    "function*****slope": [
        "$ARG1 with no $ARG2"
    ],
    "weight decay*****function": [
        "$ARG1 has encouraged it to use a simpler $ARG2"
    ],
    "weight decay*****moore-penrose pseudoinverse": [
        "$ARG1 approaching zero ( i.e. , using the $ARG2"
    ],
    "moore-penrose pseudoinverse*****problem": [
        "$ARG1 to solve the underdetermined $ARG2"
    ],
    "problem*****regularization": [
        "$ARG1 with minimal $ARG2"
    ],
    "model*****hypothesis space": [
        "$ARG1 \u2019 s capacity than including or excluding members from the $ARG2"
    ],
    "function*****hypothesis space": [
        "$ARG1 from a $ARG2"
    ],
    "hypothesis space*****function": [
        "$ARG1 as expressing an in\ufb01nitely strong preference against that $ARG2"
    ],
    "algorithm*****generalization": [
        "$ARG1 that is intended to reduce its $ARG2",
        "$ARG1 that is intended to reduce its $ARG2"
    ],
    "regularization*****machine learning": [
        "$ARG1 is one of the central concerns of the \ufb01eld of $ARG2",
        "$ARG1 In order to generalize well , $ARG2",
        "$ARG1 is necessary for $ARG2",
        "$ARG1 to solve underdetermined problems extends beyond $ARG2",
        "$ARG1 is a central theme of $ARG2"
    ],
    "algorithm*****regularization": [
        "$ARG1 , and , in particular , no best form of $ARG2",
        "$ARG1 can not discover that certain functions do a good job of minimizing the training cost , or if $ARG2"
    ],
    "regularization*****task": [
        "$ARG1 that is well-suited to the particular $ARG2"
    ],
    "deep learning*****hyperparameters": [
        "$ARG1 in general and this book in particular is that a very wide range of tasks ( such as all of the intellectual tasks that people can do ) may all be solved e\ufb00ectively using very general-purpose forms $ARG2",
        "$ARG1 algorithms come with many $ARG2"
    ],
    "hyperparameters*****machine learning": [
        "$ARG1 and Validation Sets Most $ARG2",
        "$ARG1 do and how $ARG2"
    ],
    "algorithm*****hyperparameters": [
        "$ARG1 learns the best $ARG2",
        "$ARG1 searches for good values of the $ARG2",
        "$ARG1 and choose its $ARG2",
        "$ARG1 \u2019 s $ARG2"
    ],
    "example*****hyperparameter": [
        "$ARG1 we saw in \ufb01gure 5.2 , there is a single $ARG2",
        "$ARG1 of a $ARG2"
    ],
    "weight decay*****example": [
        "$ARG1 is another $ARG2"
    ],
    "hyperparameter*****learning": [
        "$ARG1 that the $ARG2"
    ],
    "machine learning*****hyperparameter": [
        "$ARG1 BASICS setting must be a $ARG2"
    ],
    "hyperparameter*****set": [
        "$ARG1 on the training $ARG2",
        "$ARG1 has a U-shaped validation $ARG2",
        "$ARG1 , the user selects a small \ufb01nite $ARG2"
    ],
    "hyperparameters*****model": [
        "$ARG1 that control $ARG2",
        "$ARG1 would always choose the maximum possible $ARG2",
        "$ARG1 that control $ARG2",
        "$ARG1 to reduce e\ufb00ective $ARG2",
        "$ARG1 on $ARG2",
        "$ARG1 which the $ARG2"
    ],
    "set*****weight decay": [
        "$ARG1 better with a higher degree polynomial and a $ARG2"
    ],
    "set*****distribution": [
        "$ARG1 , composed of examples coming from the same $ARG2"
    ],
    "generalization*****hyperparameters": [
        "$ARG1 error during or after training , allowing for the $ARG2"
    ],
    "set*****process": [
        "$ARG1 , even though this may be confused with the larger pool of data used for the entire training $ARG2"
    ],
    "subset*****hyperparameters": [
        "$ARG1 of data used to guide the selection of $ARG2"
    ],
    "hyperparameter optimization*****generalization": [
        "$ARG1 is complete , the $ARG2"
    ],
    "average*****algorithm": [
        "$ARG1 test error , making it di\ufb03cult to claim that $ARG2"
    ],
    "cross-validation*****algorithm": [
        "$ARG1 procedure , shown in $ARG2",
        "$ARG1 , it is still common practice to use them to declare that $ARG2"
    ],
    "unbiased*****variance": [
        "$ARG1 estimators of the $ARG2",
        "$ARG1 sample $ARG2"
    ],
    "average*****bias": [
        "$ARG1 error estimators ( Bengio and Grandvalet , 2004 ) , but approximations are typically Estimators , $ARG2"
    ],
    "variance*****statistics": [
        "$ARG1 The \ufb01eld of $ARG2"
    ],
    "variance*****generalization": [
        "$ARG1 are useful to formally characterize notions of $ARG2",
        "$ARG1 ( dashed ) tends to increase , yielding another U-shaped curve for $ARG2",
        "$ARG1 are meaningful components of $ARG2"
    ],
    "vector*****parametric model": [
        "$ARG1 of parameters in some $ARG2"
    ],
    "parametric model*****weights": [
        "$ARG1 , such as the $ARG2"
    ],
    "set*****independent and identically distributed": [
        "$ARG1 of m $ARG2"
    ],
    "machine learning*****algorithm": [
        "$ARG1 BASICS $ARG2",
        "$ARG1 experiments , it is common to say that $ARG2",
        "$ARG1 is how to make an $ARG2",
        "$ARG1 practitioner also needs to know how to choose an $ARG2"
    ],
    "algorithm*****cross-validation": [
        "$ARG1 5.1 The k-fold $ARG2"
    ],
    "dataset*****generalization": [
        "$ARG1 D is too small for a simple train/test or train/valid split to yield accurate estimation of $ARG2"
    ],
    "generalization*****loss": [
        "$ARG1 error , because the mean of a $ARG2",
        "$ARG1 error bounds ( Baxter , 1995 ) can be achieved because of the shared parameters , for which statistical strength can be $ARG2"
    ],
    "dataset*****contains": [
        "$ARG1 D $ARG2",
        "$ARG1 that $ARG2",
        "$ARG1 and also $ARG2"
    ],
    "example*****supervised learning": [
        "$ARG1 ) , which could stand for an ( input , target ) pair z ( i ) = ( x ( i ) , y ( i ) ) in the case of $ARG2"
    ],
    "dataset*****learning": [
        "$ARG1 , with elements z ( i ) Require : A , the $ARG2"
    ],
    "function*****dataset": [
        "$ARG1 that takes a $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "dataset*****function": [
        "$ARG1 as input and outputs a learned $ARG2",
        "$ARG1 and outputs a $ARG2"
    ],
    "function*****loss function": [
        "$ARG1 Require : L , the $ARG2",
        "$ARG1 L ( x , g ( f ( x ) ) ) where L is a $ARG2"
    ],
    "scalar*****number": [
        "$ARG1 \u2208 R Require : k , the $ARG2",
        "$ARG1 is just a single $ARG2",
        "$ARG1 , or \u201c Let n \u2208 N be the $ARG2"
    ],
    "point estimator*****statistic": [
        "$ARG1 or $ARG2"
    ],
    "statistic*****function": [
        "$ARG1 is any $ARG2"
    ],
    "point estimator*****estimator": [
        "$ARG1 is very general and allows the designer of an $ARG2"
    ],
    "function*****estimator": [
        "$ARG1 thus quali\ufb01es as an $ARG2",
        "$ARG1 might make a good $ARG2"
    ],
    "estimator*****function": [
        "$ARG1 is a $ARG2",
        "$ARG1 that we might want to consider is how much we expect it to vary as a $ARG2",
        "$ARG1 decreases as a $ARG2"
    ],
    "estimator*****point estimator": [
        "$ARG1 f\u02c6 is simply a $ARG2"
    ],
    "point estimator*****function": [
        "$ARG1 in $ARG2"
    ],
    "bias*****estimator": [
        "$ARG1 of an $ARG2",
        "$ARG1 ( \u03b8\u0302 ) = 0 , we say that our $ARG2",
        "$ARG1 induced by the $ARG2"
    ],
    "estimator*****bias": [
        "$ARG1 is de\ufb01ned as : $ARG2",
        "$ARG1 is biased , we can substitute equation 5.22 into equation 5.20 : $ARG2",
        "$ARG1 to determine its $ARG2",
        "$ARG1 to exhibit low $ARG2",
        "$ARG1 and then analyzing its $ARG2",
        "$ARG1 works by trading increased $ARG2"
    ],
    "machine learning*****expectation": [
        "$ARG1 BASICS where the $ARG2"
    ],
    "random variable*****data generating distribution": [
        "$ARG1 ) and \u03b8 is the true underlying value of \u03b8 used to de\ufb01ne the $ARG2"
    ],
    "unbiased*****bias": [
        "$ARG1 if $ARG2"
    ],
    "asymptotically unbiased*****bias": [
        "$ARG1 if lim m\u2192\u221e $ARG2"
    ],
    "example*****bernoulli distribution": [
        "$ARG1 : $ARG2",
        "$ARG1 : $ARG2"
    ],
    "bernoulli distribution*****set": [
        "$ARG1 Consider a $ARG2",
        "$ARG1 We once again consider a $ARG2"
    ],
    "1*****estimator": [
        "$ARG1 \ue058 ( i ) To determine whether this $ARG2"
    ],
    "example*****gaussian distribution": [
        "$ARG1 : $ARG2",
        "$ARG1 , a more complicated penalty term can be derived by using a mixture of Gaussians , rather than a single $ARG2"
    ],
    "gaussian distribution*****1": [
        "$ARG1 p ( x ( i ) ) = N ( x ( i ) ; \u00b5 , \u03c32 ) , where i \u2208 { $ARG2"
    ],
    "machine learning*****recall": [
        "$ARG1 BASICS $ARG2",
        "$ARG1 BASICS $ARG2"
    ],
    "recall*****probability density function": [
        "$ARG1 that the Gaussian $ARG2"
    ],
    "probability density function*****\u03c3": [
        "$ARG1 is given by ( i ) \u2212 \u00b5 ) 2 p ( x ( i ) ; \u00b5 , $ARG2"
    ],
    "\u03c3*****estimator": [
        "$ARG1 2 ) = \u221a A common $ARG2"
    ],
    "bias*****sample mean": [
        "$ARG1 of the $ARG2"
    ],
    "sample mean*****bias": [
        "$ARG1 , we are again interested in calculating $ARG2"
    ],
    "1*****sample mean": [
        "$ARG1 \ue058 \ue068 ( i ) \ue069 Thus we \ufb01nd that the $ARG2",
        "$ARG1 \ue058 \ue010 ( i ) x \u2212 \u00b5\u0302 m , where \u00b5\u0302m is the $ARG2"
    ],
    "sample mean*****unbiased": [
        "$ARG1 is an $ARG2"
    ],
    "estimator*****example": [
        "$ARG1 of Gaussian mean $ARG2"
    ],
    "gaussian distribution*****example": [
        "$ARG1 As an $ARG2"
    ],
    "machine learning*****bias": [
        "$ARG1 BASICS We begin by evaluating the term E [ \u03c3\u0302m x ( i ) \u2212 \u00b5\u0302 m Returning to equation 5.37 , we conclude that the $ARG2"
    ],
    "variance*****standard error": [
        "$ARG1 and $ARG2",
        "$ARG1 is called the $ARG2",
        "$ARG1 or the $ARG2"
    ],
    "standard error*****property": [
        "$ARG1 Another $ARG2"
    ],
    "property*****estimator": [
        "$ARG1 of the $ARG2"
    ],
    "expectation*****estimator": [
        "$ARG1 of the $ARG2"
    ],
    "variance*****random variable": [
        "$ARG1 Var ( \u03b8 ) where the $ARG2",
        "$ARG1 of multinoulli-distributed $ARG2"
    ],
    "random variable*****set": [
        "$ARG1 is the training $ARG2",
        "$ARG1 or $ARG2"
    ],
    "machine learning*****variance": [
        "$ARG1 BASICS The $ARG2"
    ],
    "standard error*****estimator": [
        "$ARG1 of an $ARG2"
    ],
    "estimator*****measure": [
        "$ARG1 provides a $ARG2"
    ],
    "measure*****dataset": [
        "$ARG1 of how we would expect the estimate we compute from data to vary as we independently resample the $ARG2"
    ],
    "dataset*****data generating process": [
        "$ARG1 from the underlying $ARG2"
    ],
    "bias*****statistic": [
        "$ARG1 we would also like it to have relatively When we compute any $ARG2"
    ],
    "statistic*****number": [
        "$ARG1 using a \ufb01nite $ARG2"
    ],
    "distribution*****statistics": [
        "$ARG1 and their $ARG2"
    ],
    "expected*****estimator": [
        "$ARG1 degree of variation in any $ARG2",
        "$ARG1 deviation\u2014in a squared error sense\u2014 between the $ARG2"
    ],
    "standard error of the mean*****variance": [
        "$ARG1 is given by SE ( \u00b5\u0302m ) = \ue074Var x ( i ) = \u221a , where \u03c32 is the true $ARG2"
    ],
    "standard error*****\u03c3": [
        "$ARG1 is often estimated by using an estimate of $ARG2"
    ],
    "variance*****unbiased": [
        "$ARG1 nor the square root of the $ARG2",
        "$ARG1 provide an $ARG2"
    ],
    "unbiased*****standard deviation": [
        "$ARG1 estimate of the $ARG2"
    ],
    "standard error of the mean*****machine learning": [
        "$ARG1 is very useful in $ARG2"
    ],
    "generalization*****sample mean": [
        "$ARG1 error by computing the $ARG2"
    ],
    "sample mean*****set": [
        "$ARG1 of the error on the test $ARG2"
    ],
    "set*****accuracy": [
        "$ARG1 determines the $ARG2",
        "$ARG1 a high $ARG2"
    ],
    "central limit theorem*****normal distribution": [
        "$ARG1 , which tells us that the mean will be approximately distributed with a $ARG2"
    ],
    "normal distribution*****standard error": [
        "$ARG1 , we can use the $ARG2"
    ],
    "standard error*****probability": [
        "$ARG1 to compute the $ARG2"
    ],
    "probability*****expectation": [
        "$ARG1 that the true $ARG2"
    ],
    "example*****normal distribution": [
        "$ARG1 , the 95 % con\ufb01dence interval centered on the mean \u00b5\u0302 m is ( \u00b5\u0302m \u2212 1.96SE ( \u02c6 \u02c6 m + 1.96SE ( \u02c6 \u00b5m ) ) , \u00b5 m ) , \u00b5 under the $ARG2",
        "$ARG1 , consider estimating the mean parameter \u00b5 of a $ARG2"
    ],
    "bernoulli distribution*****recall": [
        "$ARG1 ( $ARG2"
    ],
    "recall*****1": [
        "$ARG1 P ( x ( i ) ; \u03b8 ) = \u03b8 x ( $ARG2"
    ],
    "1*****variance": [
        "$ARG1 \u2212 \u03b8 ) ( 1\u2212x ) ) .\ue050This time we are interested in computing the $ARG2",
        "$ARG1 \u2212 \u03b8 ) The $ARG2",
        "$ARG1 , because the reconstruction error penalty encourages the CAE to encode the directions with the most local $ARG2"
    ],
    "property*****consistency": [
        "$ARG1 of popular estimators that we will return to when we discuss $ARG2",
        "$ARG1 of $ARG2"
    ],
    "variance*****bias": [
        "$ARG1 to Minimize Mean Squared $ARG2",
        "$ARG1 and decrease $ARG2",
        "$ARG1 signi\ufb01cantly while not overly increasing the $ARG2",
        "$ARG1 rather than $ARG2"
    ],
    "measure*****estimator": [
        "$ARG1 two di\ufb00erent sources of error in an $ARG2"
    ],
    "bias*****expected": [
        "$ARG1 measures the $ARG2"
    ],
    "variance*****other": [
        "$ARG1 on the $ARG2"
    ],
    "measure*****expected": [
        "$ARG1 of the deviation from the $ARG2",
        "$ARG1 how close we are to the true parameter is by the $ARG2"
    ],
    "model*****bias": [
        "$ARG1 with large $ARG2"
    ],
    "mean squared error*****mse": [
        "$ARG1 ( $ARG2"
    ],
    "mse*****bias": [
        "$ARG1 = E [ ( \u03b8\u0302m \u2212 \u03b8 ) 2 ] = $ARG2",
        "$ARG1 incorporates both the $ARG2",
        "$ARG1 and these are estimators that manage to keep both their $ARG2",
        "$ARG1 ( where $ARG2"
    ],
    "mse*****expected": [
        "$ARG1 measures the overall $ARG2"
    ],
    "variance*****machine learning": [
        "$ARG1 is tightly linked to the $ARG2"
    ],
    "generalization*****mse": [
        "$ARG1 error is measured by the $ARG2"
    ],
    "generalization*****variance": [
        "$ARG1 error ) , increasing capacity tends to increase $ARG2"
    ],
    "convergence*****probability": [
        "$ARG1 in $ARG2"
    ],
    "consistency*****almost sure convergence": [
        "$ARG1 referring to the $ARG2"
    ],
    "almost sure convergence*****random variable": [
        "$ARG1 of a sequence of $ARG2"
    ],
    "consistency*****bias": [
        "$ARG1 ensures that the $ARG2"
    ],
    "estimator*****number": [
        "$ARG1 diminishes as the $ARG2",
        "$ARG1 asymptotically , as the $ARG2"
    ],
    "normal distribution*****\u03c3": [
        "$ARG1 N ( x ; \u00b5 , $ARG2",
        "$ARG1 N ( x ; \u00b5 , \u03c32 ) exhibits a classic \u201c bell curve \u201d shape , with the x coordinate of its central peak given by \u00b5 , and the width of its peak controlled by $ARG2",
        "$ARG1 , with \u00b5 = 0 and $ARG2"
    ],
    "\u03c3*****dataset": [
        "$ARG1 2 ) , with a $ARG2"
    ],
    "dataset*****1": [
        "$ARG1 consisting of m samples : { x ( $ARG2"
    ],
    "1*****dataset": [
        "$ARG1 ) of the $ARG2"
    ],
    "dataset*****unbiased": [
        "$ARG1 as an $ARG2"
    ],
    "unbiased*****matter": [
        "$ARG1 no $ARG2"
    ],
    "variance*****principle": [
        "$ARG1 , we would like to have some $ARG2"
    ],
    "optimization problem*****change": [
        "$ARG1 , we observe that taking the logarithm of the likelihood does not $ARG2"
    ],
    "change*****cost function": [
        "$ARG1 when we rescale the $ARG2"
    ],
    "cost function*****expectation": [
        "$ARG1 , we can divide by m to obtain a version of the criterion that is expressed as an $ARG2"
    ],
    "maximum likelihood estimation*****view": [
        "$ARG1 is to $ARG2"
    ],
    "view*****empirical distribution": [
        "$ARG1 it as minimizing the dissimilarity between the $ARG2",
        "$ARG1 the $ARG2"
    ],
    "distribution*****divergence": [
        "$ARG1 , with the degree of dissimilarity between the two measured by the KL $ARG2"
    ],
    "function*****data generating process": [
        "$ARG1 only of the $ARG2",
        "$ARG1 or the true $ARG2"
    ],
    "data generating process*****model": [
        "$ARG1 , not the $ARG2",
        "$ARG1 so we can never know for sure if the $ARG2",
        "$ARG1 is almost certainly outside the $ARG2",
        "$ARG1 ) into a round hole ( our $ARG2"
    ],
    "model*****divergence": [
        "$ARG1 to minimize the KL $ARG2"
    ],
    "cross-entropy*****softmax": [
        "$ARG1 \u201d to identify speci\ufb01cally the negative log-likelihood of a Bernoulli or $ARG2"
    ],
    "loss*****empirical distribution": [
        "$ARG1 consisting of a negative log-likelihood is a crossentropy between the $ARG2"
    ],
    "cross-entropy*****empirical distribution": [
        "$ARG1 between the $ARG2"
    ],
    "empirical distribution*****model": [
        "$ARG1 and a Gaussian $ARG2"
    ],
    "data generating distribution*****divergence": [
        "$ARG1 pdata , but we have no direct access to this While the optimal \u03b8 is the same regardless of whether we are maximizing the likelihood or minimizing the KL $ARG2"
    ],
    "divergence*****objective function": [
        "$ARG1 , the values of the $ARG2"
    ],
    "maximum likelihood*****divergence": [
        "$ARG1 as minimum KL $ARG2"
    ],
    "mean squared error*****maximum likelihood": [
        "$ARG1 The $ARG2",
        "$ARG1 than the $ARG2"
    ],
    "estimator*****conditional probability": [
        "$ARG1 can readily be generalized to the case where our goal is to estimate a $ARG2"
    ],
    "linear regression*****maximum likelihood": [
        "$ARG1 as $ARG2",
        "$ARG1 , introduced earlier in section 5.1.4 , may be justi\ufb01ed as a $ARG2"
    ],
    "linear regression*****algorithm": [
        "$ARG1 as an $ARG2"
    ],
    "mapping*****mean squared error": [
        "$ARG1 from x to y\u0302 is chosen to minimize $ARG2"
    ],
    "linear regression*****view": [
        "$ARG1 from the point of $ARG2"
    ],
    "view*****maximum likelihood estimation": [
        "$ARG1 of $ARG2"
    ],
    "algorithm*****distribution": [
        "$ARG1 is now to \ufb01t the $ARG2"
    ],
    "maximum likelihood estimation*****learning": [
        "$ARG1 procedure to yield the same $ARG2"
    ],
    "machine learning*****\u03c3": [
        "$ARG1 BASICS examples are assumed to be i.i.d. , the conditional log-likelihood ( equation 5.63 ) is log p ( y ( i ) | x ( i ) ; \u03b8 ) \ue00dy\u0302 ( i ) \u2212 y ( i ) \ue00d2 = \u2212 m log $ARG2"
    ],
    "\u03c3*****linear regression": [
        "$ARG1 \u2212 log ( 2\u03c0 ) \u2212 where y\u0302 ( i ) is the output of the $ARG2"
    ],
    "linear regression*****number": [
        "$ARG1 on the i-th input x ( i ) and m is the $ARG2"
    ],
    "1*****mean squared error": [
        "$ARG1 \ue058 ( i ) ||y\u0302 \u2212 y ( i ) ||2 , we immediately see that maximizing the log-likelihood with respect to w yields the same estimate of the parameters w as does minimizing the $ARG2"
    ],
    "mse*****maximum likelihood estimation": [
        "$ARG1 as a $ARG2"
    ],
    "number*****rate": [
        "$ARG1 of examples m \u2192 \u221e , in terms of its $ARG2"
    ],
    "conditions*****maximum likelihood": [
        "$ARG1 , the $ARG2"
    ],
    "estimator*****property": [
        "$ARG1 has the $ARG2",
        "$ARG1 , many of which share the $ARG2"
    ],
    "consistency*****number": [
        "$ARG1 ( see section 5.4.5 above ) , meaning that as the $ARG2"
    ],
    "number*****maximum likelihood": [
        "$ARG1 of training examples approaches in\ufb01nity , the $ARG2"
    ],
    "maximum likelihood*****data generating process": [
        "$ARG1 can recover the correct pdata , but will not be able to determine which value of \u03b8 was used by the $ARG2"
    ],
    "other*****maximum likelihood": [
        "$ARG1 inductive principles besides the $ARG2"
    ],
    "machine learning*****statistic": [
        "$ARG1 BASICS consistent estimators can di\ufb00er in their $ARG2"
    ],
    "statistic*****estimator": [
        "$ARG1 e\ufb03ciency , meaning that one consistent $ARG2"
    ],
    "estimator*****generalization": [
        "$ARG1 may obtain lower $ARG2"
    ],
    "number*****linear regression": [
        "$ARG1 of samples m , or equivalently , may require fewer examples to obtain a \ufb01xed level of Statistical e\ufb03ciency is typically studied in the parametric case ( like in $ARG2"
    ],
    "mean squared error*****expectation": [
        "$ARG1 , computing the squared di\ufb00erence between the estimated and true parameter values , where the $ARG2"
    ],
    "mean squared error*****estimator": [
        "$ARG1 decreases as m increases , and for m large , the Cram\u00e9r-Rao lower bound ( Rao , 1945 ; Cram\u00e9r , 1946 ) shows that no consistent $ARG2"
    ],
    "estimator*****mean squared error": [
        "$ARG1 has a lower $ARG2"
    ],
    "maximum likelihood*****consistency": [
        "$ARG1 For these reasons ( $ARG2"
    ],
    "consistency*****maximum likelihood": [
        "$ARG1 and e\ufb03ciency ) , $ARG2"
    ],
    "estimator*****machine learning": [
        "$ARG1 to use for $ARG2"
    ],
    "number*****regularization": [
        "$ARG1 of examples is small enough to yield over\ufb01tting behavior , $ARG2",
        "$ARG1 of training iterations , it also has the bene\ufb01t of providing $ARG2",
        "$ARG1 of training iterations \u03c4 plays a role inversely proportional to the L2 $ARG2"
    ],
    "regularization*****weight decay": [
        "$ARG1 strategies such as $ARG2",
        "$ARG1 strategies like $ARG2",
        "$ARG1 parameter , and the inverse of \u03c4\ue00f plays the role of the $ARG2",
        "$ARG1 while $ARG2",
        "$ARG1 ( or $ARG2",
        "$ARG1 terms such as $ARG2"
    ],
    "weight decay*****maximum likelihood": [
        "$ARG1 may be used to obtain a biased version of $ARG2"
    ],
    "maximum likelihood*****variance": [
        "$ARG1 that has less $ARG2"
    ],
    "random variable*****function": [
        "$ARG1 on account of it being a $ARG2",
        "$ARG1 , rather than the name of the $ARG2",
        "$ARG1 x , a $ARG2"
    ],
    "other*****random variable": [
        "$ARG1 hand , the true parameter \u03b8 is unknown or uncertain and thus is represented as a $ARG2"
    ],
    "machine learning*****distribution": [
        "$ARG1 practitioner selects a prior $ARG2",
        "$ARG1 BASICS prior $ARG2",
        "$ARG1 BASICS where \u00b50 and \u039b0 are the prior $ARG2",
        "$ARG1 BASICS $ARG2"
    ],
    "example*****uniform distribution": [
        "$ARG1 , one might assume a priori that \u03b8 lies in some \ufb01nite range or volume , with a $ARG2"
    ],
    "magnitude*****function": [
        "$ARG1 coe\ufb03cients , or a $ARG2"
    ],
    "rule*****1": [
        "$ARG1 : p ( \u03b8 | x , ... , x p ( x ( $ARG2",
        "$ARG1 py ( y ) = px ( 2y ) then py will be 0 everywhere except the interval [ 0 , 12 ] , and it will be $ARG2"
    ],
    "maximum likelihood*****distribution": [
        "$ARG1 approach that makes predictions using a point estimate of \u03b8 , the Bayesian approach is to make predictions using a full $ARG2"
    ],
    "estimator*****change": [
        "$ARG1 is an assessment of how the estimate might $ARG2"
    ],
    "probability*****estimator": [
        "$ARG1 , making the Bayesian approach simple to justify , while the frequentist machinery for constructing an $ARG2"
    ],
    "estimator*****dataset": [
        "$ARG1 is based on the rather ad hoc decision to summarize all knowledge contained in the $ARG2"
    ],
    "dataset*****maximum likelihood": [
        "$ARG1 with a single point The second important di\ufb00erence between the Bayesian approach to estimation and the $ARG2"
    ],
    "linear regression*****linear map": [
        "$ARG1 , we learn a $ARG2"
    ],
    "linear map*****vector": [
        "$ARG1 from an input $ARG2"
    ],
    "mse*****variance": [
        "$ARG1 formulation in assuming that the Gaussian $ARG2"
    ],
    "distribution*****vector": [
        "$ARG1 mean $ARG2",
        "$ARG1 over a random $ARG2"
    ],
    "vector*****covariance matrix": [
        "$ARG1 and $ARG2"
    ],
    "covariance matrix*****distribution": [
        "$ARG1 With the prior thus speci\ufb01ed , we can now proceed in determining the posterior $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "distribution*****inference": [
        "$ARG1 allows us to gain some intuition for the e\ufb00ect of Bayesian $ARG2",
        "$ARG1 , so the $ARG2"
    ],
    "set*****linear regression": [
        "$ARG1 \u039b0 = 1\u03b1 I , then \u00b5m gives the same estimate of w as does frequentist $ARG2"
    ],
    "process*****covariance matrix": [
        "$ARG1 with an in\ufb01nitely wide prior on w. The more important di\ufb00erence is that the Bayesian estimate provides a $ARG2"
    ],
    "covariance*****diagonal": [
        "$ARG1 structure , we typically assume a $ARG2",
        "$ARG1 of z is $ARG2"
    ],
    "posterior probability*****probability": [
        "$ARG1 ( or maximal $ARG2"
    ],
    "weight decay*****learning": [
        "$ARG1 penalty , plus a term that does not depend on w and does not a\ufb00ect the $ARG2",
        "$ARG1 term has modi\ufb01ed the $ARG2"
    ],
    "inference*****weights": [
        "$ARG1 with a Gaussian prior on the $ARG2",
        "$ARG1 with a \ue050 Gaussian prior on the $ARG2",
        "$ARG1 over the $ARG2"
    ],
    "inference*****information": [
        "$ARG1 has the advantage of leveraging $ARG2"
    ],
    "information*****variance": [
        "$ARG1 helps to reduce the $ARG2"
    ],
    "learning*****weight decay": [
        "$ARG1 regularized with $ARG2"
    ],
    "weight decay*****map approximation": [
        "$ARG1 , can be interpreted as making the $ARG2"
    ],
    "map approximation*****inference": [
        "$ARG1 to Bayesian $ARG2",
        "$ARG1 to Bayesian $ARG2"
    ],
    "view*****regularization": [
        "$ARG1 applies when the $ARG2"
    ],
    "regularization*****inference": [
        "$ARG1 penalties correspond to MAP Bayesian $ARG2",
        "$ARG1 strategies can be interpreted as MAP Bayesian $ARG2",
        "$ARG1 is equivalent to MAP Bayesian $ARG2"
    ],
    "regularization*****prior probability distribution": [
        "$ARG1 terms depend on the data , which of course a $ARG2"
    ],
    "inference*****regularization": [
        "$ARG1 provides a straightforward way to design complicated yet interpretable $ARG2",
        "$ARG1 , and that in particular , L 2 $ARG2"
    ],
    "recall*****supervised learning": [
        "$ARG1 from section 5.1.3 that $ARG2"
    ],
    "supervised learning*****learning": [
        "$ARG1 algorithms are , roughly speaking , $ARG2"
    ],
    "human*****set": [
        "$ARG1 \u201c supervisor , \u201d but the term still applies even when the training $ARG2",
        "$ARG1 practitioner , since one can usually tell early on if some $ARG2"
    ],
    "supervised learning*****probability distribution": [
        "$ARG1 algorithms in this book are based on estimating a $ARG2"
    ],
    "maximum likelihood estimation*****vector": [
        "$ARG1 to \ufb01nd the best parameter $ARG2"
    ],
    "linear regression*****classi\ufb01cation": [
        "$ARG1 to the $ARG2"
    ],
    "classi\ufb01cation*****probability distribution": [
        "$ARG1 scenario by de\ufb01ning a di\ufb00erent family of $ARG2"
    ],
    "normal distribution*****linear regression": [
        "$ARG1 over real-valued numbers that we used for $ARG2"
    ],
    "problem*****logistic sigmoid": [
        "$ARG1 is to use the $ARG2"
    ],
    "logistic regression*****model": [
        "$ARG1 ( a somewhat strange name since we use the $ARG2"
    ],
    "linear regression*****weights": [
        "$ARG1 , we were able to \ufb01nd the optimal $ARG2"
    ],
    "weights*****normal equations": [
        "$ARG1 by solving the $ARG2"
    ],
    "solution*****weights": [
        "$ARG1 for its optimal $ARG2"
    ],
    "problem*****conditional probability distribution": [
        "$ARG1 , by writing down a parametric family of $ARG2"
    ],
    "supervised learning*****support vector machine": [
        "$ARG1 is the $ARG2"
    ],
    "model*****logistic regression": [
        "$ARG1 is similar to $ARG2",
        "$ARG1 like $ARG2"
    ],
    "logistic regression*****function": [
        "$ARG1 in that it is driven by a linear $ARG2"
    ],
    "logistic regression*****support vector machine": [
        "$ARG1 , the $ARG2"
    ],
    "support vector machine*****identity": [
        "$ARG1 does not provide probabilities , but only outputs a class $ARG2"
    ],
    "support vector machine*****kernel trick": [
        "$ARG1 is the $ARG2"
    ],
    "kernel trick*****machine learning": [
        "$ARG1 consists of observing that many $ARG2"
    ],
    "machine learning*****dot product": [
        "$ARG1 algorithms can be written exclusively in terms of $ARG2"
    ],
    "function*****support vector machine": [
        "$ARG1 used by the $ARG2"
    ],
    "support vector machine*****example": [
        "$ARG1 can w x+b = b+ \u03b1 ix\ue03ex ( i ) where x ( i ) is a training $ARG2"
    ],
    "algorithm*****feature": [
        "$ARG1 this way allows us to replace x by the output of a given $ARG2"
    ],
    "function*****dot product": [
        "$ARG1 \u03c6 ( x ) and the $ARG2",
        "$ARG1 k often admits an implementation that is signi\ufb01cantly more computational e\ufb03cient than naively constructing two \u03c6 ( x ) vectors and explicitly taking their $ARG2",
        "$ARG1 k ( x , x ( i ) ) = min ( x , x ( i ) ) that is exactly equivalent to the corresponding in\ufb01nite-dimensional $ARG2"
    ],
    "dot product*****function": [
        "$ARG1 with a $ARG2"
    ],
    "function*****kernel": [
        "$ARG1 k ( x , x ( i ) ) = \u03c6 ( x ) \u00b7 \u03c6 ( x ( i ) ) called a $ARG2",
        "$ARG1 w ) as the $ARG2",
        "$ARG1 of a $ARG2"
    ],
    "other*****inner product": [
        "$ARG1 kinds of $ARG2"
    ],
    "inner product*****example": [
        "$ARG1 , for $ARG2"
    ],
    "example*****inner product": [
        "$ARG1 , $ARG2"
    ],
    "dot product*****kernel": [
        "$ARG1 with $ARG2"
    ],
    "function*****preprocessing": [
        "$ARG1 is exactly equivalent to $ARG2"
    ],
    "preprocessing*****learning": [
        "$ARG1 the data by applying \u03c6 ( x ) to all inputs , then $ARG2"
    ],
    "learning*****linear model": [
        "$ARG1 a $ARG2"
    ],
    "function*****convex optimization": [
        "$ARG1 of x using $ARG2"
    ],
    "optimization algorithm*****view": [
        "$ARG1 can $ARG2"
    ],
    "kernel*****feature": [
        "$ARG1 , we construct a $ARG2",
        "$ARG1 can only extract one kind of $ARG2",
        "$ARG1 that performs well at all positions in the convolutional $ARG2"
    ],
    "gaussian kernel*****\u03c3": [
        "$ARG1 k ( u , v ) = N ( u \u2212 v ; 0 , $ARG2"
    ],
    "\u03c3*****standard": [
        "$ARG1 ) is the $ARG2"
    ],
    "kernel*****radial basis function": [
        "$ARG1 is also known as the $ARG2"
    ],
    "radial basis function*****kernel": [
        "$ARG1 ( RBF ) $ARG2"
    ],
    "gaussian kernel*****dot product": [
        "$ARG1 corresponds to a $ARG2"
    ],
    "dot product*****derivation": [
        "$ARG1 in an in\ufb01nite-dimensional space , but the $ARG2"
    ],
    "derivation*****example": [
        "$ARG1 of this space is less straightforward than in our $ARG2"
    ],
    "example*****kernel": [
        "$ARG1 of the min $ARG2",
        "$ARG1 then requires evaluating the $ARG2",
        "$ARG1 by setting each $ARG2"
    ],
    "gaussian kernel*****template matching": [
        "$ARG1 as performing a kind of $ARG2"
    ],
    "example*****template": [
        "$ARG1 x associated with training label y becomes a $ARG2"
    ],
    "distance*****gaussian kernel": [
        "$ARG1 , the $ARG2"
    ],
    "gaussian kernel*****template": [
        "$ARG1 has a large response , indicating that x\ue030 is very similar to the x $ARG2"
    ],
    "support vector machine*****algorithm": [
        "$ARG1 are not the only $ARG2"
    ],
    "algorithm*****kernel trick": [
        "$ARG1 that can be enhanced using the $ARG2"
    ],
    "kernel trick*****kernel": [
        "$ARG1 is known as $ARG2"
    ],
    "kernel*****function": [
        "$ARG1 machines is that the cost of evaluating the decision $ARG2"
    ],
    "support vector machine*****learning": [
        "$ARG1 are able to mitigate this by $ARG2"
    ],
    "learning*****vector": [
        "$ARG1 an \u03b1 $ARG2",
        "$ARG1 algorithms to billions of examples if we must remember a dynamically updated $ARG2"
    ],
    "machine learning*****support": [
        "$ARG1 BASICS as $ARG2"
    ],
    "kernel*****dataset": [
        "$ARG1 machines also su\ufb00er from a high computational cost of training when the $ARG2"
    ],
    "deep learning*****kernel": [
        "$ARG1 was designed to overcome these limitations of $ARG2"
    ],
    "neural network*****kernel": [
        "$ARG1 could outperform the RBF $ARG2"
    ],
    "k-nearest neighbors*****classi\ufb01cation": [
        "$ARG1 is a family of techniques that can be used for $ARG2"
    ],
    "algorithm*****k-nearest neighbors": [
        "$ARG1 , $ARG2"
    ],
    "k-nearest neighbors*****number": [
        "$ARG1 is not restricted to a \ufb01xed $ARG2"
    ],
    "supervised learning*****average": [
        "$ARG1 where we can de\ufb01ne an $ARG2"
    ],
    "classi\ufb01cation*****average": [
        "$ARG1 , we can $ARG2",
        "$ARG1 rules is brittle , but if we $ARG2"
    ],
    "average*****probability distribution": [
        "$ARG1 over these one-hot codes as giving a $ARG2"
    ],
    "measure*****loss": [
        "$ARG1 performance with 0-1 $ARG2"
    ],
    "bayes error*****number": [
        "$ARG1 as the $ARG2"
    ],
    "algorithm*****bayes error": [
        "$ARG1 to use all of these neighbors to vote , rather than randomly choosing one of them , the procedure converges to the $ARG2"
    ],
    "k-nearest neighbors*****accuracy": [
        "$ARG1 allows it to obtain high $ARG2"
    ],
    "accuracy*****set": [
        "$ARG1 given a large training $ARG2",
        "$ARG1 , it was important to $ARG2"
    ],
    "k-nearest neighbors*****feature": [
        "$ARG1 is that it can not learn that one $ARG2"
    ],
    "task*****isotropic": [
        "$ARG1 with x \u2208 R 100 drawn from an $ARG2"
    ],
    "machine learning*****decision tree": [
        "$ARG1 BASICS Figure 5.7 : Diagrams describing how a $ARG2"
    ],
    "example*****decision tree": [
        "$ARG1 to de\ufb01ne , so it is not possible for the $ARG2"
    ],
    "decision tree*****function": [
        "$ARG1 to learn a $ARG2"
    ],
    "algorithm*****decision tree": [
        "$ARG1 that also breaks the input space into regions and has separate parameters for each region is the $ARG2",
        "$ARG1 can be considered non-parametric if it is allowed to learn a tree of arbitrary size , though $ARG2"
    ],
    "decision tree*****turn": [
        "$ARG1 are usually regularized with size constraints that $ARG2"
    ],
    "turn*****parametric model": [
        "$ARG1 them into $ARG2"
    ],
    "decision tree*****constant": [
        "$ARG1 as they are typically used , with axis-aligned splits and $ARG2"
    ],
    "constant*****logistic regression": [
        "$ARG1 outputs within each node , struggle to solve some problems that are easy even for $ARG2"
    ],
    "problem*****boundary": [
        "$ARG1 and the positive class occurs wherever x2 > x1 , the decision $ARG2"
    ],
    "decision tree*****boundary": [
        "$ARG1 will thus need to approximate the decision $ARG2"
    ],
    "boundary*****function": [
        "$ARG1 with many nodes , implementing a step $ARG2"
    ],
    "learning*****thinking": [
        "$ARG1 algorithms by $ARG2"
    ],
    "thinking*****decision tree": [
        "$ARG1 about the similarities and di\ufb00erences between sophisticated algorithms and k-NN or $ARG2"
    ],
    "unsupervised learning*****information": [
        "$ARG1 refers to most attempts to extract $ARG2"
    ],
    "distribution*****human": [
        "$ARG1 that do not require $ARG2"
    ],
    "density estimation*****learning": [
        "$ARG1 , $ARG2"
    ],
    "machine learning*****unsupervised learning": [
        "$ARG1 BASICS A classic $ARG2"
    ],
    "information*****constraint": [
        "$ARG1 about x as possible while obeying some penalty or $ARG2"
    ],
    "sparse representation*****dataset": [
        "$ARG1 ( Barlow , 1989 ; Olshausen and Field , 1996 ; Hinton and Ghahramani , 1997 ) embed the $ARG2"
    ],
    "sparse representation*****information": [
        "$ARG1 typically requires increasing the dimensionality of the representation , so that the representation becoming mostly zeroes does not discard too much $ARG2"
    ],
    "pca*****unsupervised learning": [
        "$ARG1 as an $ARG2"
    ],
    "machine learning*****pca": [
        "$ARG1 BASICS Figure 5.8 : $ARG2",
        "$ARG1 BASICS $ARG2"
    ],
    "pca*****variance": [
        "$ARG1 learns a linear projection that aligns the direction of greatest $ARG2",
        "$ARG1 , this disentangling takes the form of \ufb01nding a rotation of the input space ( described by W ) that aligns the principal axes of $ARG2"
    ],
    "correlation*****other": [
        "$ARG1 with each $ARG2"
    ],
    "independence*****representation learning": [
        "$ARG1 , a $ARG2"
    ],
    "pca*****dimensionality reduction": [
        "$ARG1 as a simple and e\ufb00ective $ARG2"
    ],
    "method*****information": [
        "$ARG1 that preserves as much of the $ARG2"
    ],
    "unbiased*****covariance matrix": [
        "$ARG1 sample $ARG2"
    ],
    "pca*****diagonal": [
        "$ARG1 \ufb01nds a representation ( through linear transformation ) z = x\ue03eW where Var [ z ] is $ARG2",
        "$ARG1 results in a $ARG2"
    ],
    "variance*****matrix": [
        "$ARG1 of X as : ( U \u03a3W \ue03e ) \ue03e U \u03a3W \ue03e W \u03a3\ue03e U \ue03eU \u03a3W \ue03e W \u03a32 W \ue03e , Var [ x ] = where we use the fact that U \ue03e U = I because the U $ARG2"
    ],
    "matrix*****singular value decomposition": [
        "$ARG1 of the $ARG2",
        "$ARG1 has a $ARG2"
    ],
    "diagonal*****\u03c3": [
        "$ARG1 as required : W \ue03eW $ARG2"
    ],
    "machine learning*****diagonal": [
        "$ARG1 BASICS The above analysis shows that when we project the data x to z , via the linear transformation W , the resulting representation has a $ARG2"
    ],
    "\u03c3*****pca": [
        "$ARG1 2 ) which immediately implies that the individual elements of z are This ability of $ARG2"
    ],
    "pca*****property": [
        "$ARG1 to transform data into a representation where the elements are mutually uncorrelated is a very important $ARG2"
    ],
    "property*****pca": [
        "$ARG1 of $ARG2"
    ],
    "example*****factors of variation": [
        "$ARG1 of a representation that attempts to disentangle the unknown $ARG2"
    ],
    "variance*****correlation": [
        "$ARG1 with the basis of the new representation space associated While $ARG2"
    ],
    "correlation*****dependency": [
        "$ARG1 is an important category of $ARG2"
    ],
    "dependency*****learning": [
        "$ARG1 between elements of the data , we are also interested in $ARG2"
    ],
    "k-means*****algorithm": [
        "$ARG1 clustering $ARG2"
    ],
    "other*****k-means": [
        "$ARG1 entries of the representation h are The one-hot code provided by $ARG2"
    ],
    "k-means*****example": [
        "$ARG1 clustering is an $ARG2"
    ],
    "example*****sparse representation": [
        "$ARG1 of a $ARG2",
        "$ARG1 of $ARG2"
    ],
    "other*****sparse representation": [
        "$ARG1 algorithms that learn more \ufb02exible $ARG2"
    ],
    "sparse representation*****example": [
        "$ARG1 , where more than one entry can be non-zero for each input x. One-hot codes are an extreme $ARG2"
    ],
    "sparse representation*****distributed representation": [
        "$ARG1 that lose many of the bene\ufb01ts of a $ARG2"
    ],
    "measure*****average": [
        "$ARG1 properties of the clustering such as the $ARG2"
    ],
    "average*****distance": [
        "$ARG1 Euclidean $ARG2",
        "$ARG1 based on the $ARG2"
    ],
    "feature*****task": [
        "$ARG1 but obtain a di\ufb00erent , equally valid clustering that is not relevant to our $ARG2"
    ],
    "information*****similarity": [
        "$ARG1 about $ARG2"
    ],
    "machine learning*****gradient descent": [
        "$ARG1 BASICS $ARG2",
        "$ARG1 BASICS $ARG2",
        "$ARG1 models described in part II work very well when trained with $ARG2"
    ],
    "machine learning*****generalization": [
        "$ARG1 is that large training sets are necessary for good $ARG2",
        "$ARG1 algorithms that rely exclusively on the local smoothness prior for $ARG2",
        "$ARG1 models achieve good $ARG2"
    ],
    "generalization*****cost function": [
        "$ARG1 , but large training sets are also more computationally The $ARG2"
    ],
    "cost function*****machine learning": [
        "$ARG1 used by a $ARG2"
    ],
    "gradient*****stochastic gradient descent": [
        "$ARG1 step becomes prohibitively The insight of $ARG2"
    ],
    "expectation*****set": [
        "$ARG1 may be approximately estimated using a small $ARG2"
    ],
    "gradient descent*****optimization problem": [
        "$ARG1 to non-convex $ARG2"
    ],
    "stochastic gradient descent*****deep learning": [
        "$ARG1 has many important uses outside the context of $ARG2"
    ],
    "convergence*****set": [
        "$ARG1 usually increases with training $ARG2",
        "$ARG1 on its respective training $ARG2"
    ],
    "view*****model": [
        "$ARG1 , one can argue that the asymptotic cost of training a $ARG2",
        "$ARG1 , with this chosen h , we are maximizing log p $ARG2",
        "$ARG1 provides a di\ufb00erent motivation for training an autoencoder : it is a way of approximately training a generative $ARG2",
        "$ARG1 some images with the detections proposed by the $ARG2"
    ],
    "deep learning*****kernel trick": [
        "$ARG1 , the main way to learn nonlinear models was to use the $ARG2"
    ],
    "kernel trick*****linear model": [
        "$ARG1 in combination with a $ARG2"
    ],
    "learning*****matrix": [
        "$ARG1 algorithms require constructing an m \u00d7 m $ARG2"
    ],
    "deep learning*****stochastic gradient descent": [
        "$ARG1 garnered additional interest in industry , because it provided a scalable way of training nonlinear models on large $ARG2"
    ],
    "stochastic gradient descent*****8": [
        "$ARG1 and many enhancements to it are described further in chapter $ARG2"
    ],
    "deep learning*****dataset": [
        "$ARG1 algorithms can be described as particular instances of a fairly simple recipe : combine a speci\ufb01cation of a $ARG2"
    ],
    "dataset*****cost function": [
        "$ARG1 , a $ARG2"
    ],
    "machine learning*****cost function": [
        "$ARG1 BASICS X and y , the $ARG2"
    ],
    "1*****optimization algorithm": [
        "$ARG1 ) , and , in most cases , the $ARG2"
    ],
    "gradient*****normal equations": [
        "$ARG1 of the cost is zero using the $ARG2"
    ],
    "example*****weight decay": [
        "$ARG1 , we can add $ARG2",
        "$ARG1 , $ARG2",
        "$ARG1 , the smallest allowable $ARG2",
        "$ARG1 , the minimum $ARG2"
    ],
    "weight decay*****linear regression": [
        "$ARG1 to the $ARG2"
    ],
    "optimization algorithm*****unsupervised learning": [
        "$ARG1 supports both supervised and $ARG2"
    ],
    "example*****support": [
        "$ARG1 shows how to $ARG2"
    ],
    "contains*****model": [
        "$ARG1 only X and providing an appropriate unsupervised cost and $ARG2"
    ],
    "example*****pca": [
        "$ARG1 , we can obtain the \ufb01rst $ARG2",
        "$ARG1 is the $ARG2"
    ],
    "vector*****loss function": [
        "$ARG1 by specifying that our $ARG2"
    ],
    "model*****norm": [
        "$ARG1 is de\ufb01ned to have w with $ARG2",
        "$ARG1 is equivalent to imposing a penalty on the $ARG2"
    ],
    "norm*****function": [
        "$ARG1 one and reconstruction $ARG2",
        "$ARG1 is any $ARG2"
    ],
    "decision tree*****k-means": [
        "$ARG1 or $ARG2"
    ],
    "k-means*****cost function": [
        "$ARG1 require special-case optimizers because their $ARG2"
    ],
    "machine learning*****list": [
        "$ARG1 algorithms can be described using this recipe helps to see the di\ufb00erent algorithms as part of a taxonomy of methods for doing related tasks that work for similar reasons , rather than as a long $ARG2"
    ],
    "curse of dimensionality*****machine learning": [
        "$ARG1 Many $ARG2"
    ],
    "phenomenon*****curse of dimensionality": [
        "$ARG1 is known as the $ARG2"
    ],
    "curse of dimensionality*****computer science": [
        "$ARG1 arises in many places in $ARG2"
    ],
    "challenge*****curse of dimensionality": [
        "$ARG1 posed by the $ARG2"
    ],
    "curse of dimensionality*****challenge": [
        "$ARG1 is a statistical $ARG2"
    ],
    "challenge*****number": [
        "$ARG1 arises because the $ARG2"
    ],
    "function*****change": [
        "$ARG1 we learn should not $ARG2",
        "$ARG1 f \u2217 that f\u2217 ( x ) \u2248 f \u2217 ( x + \ue00f ) for most con\ufb01gurations x and small $ARG2",
        "$ARG1 can $ARG2",
        "$ARG1 that does not $ARG2",
        "$ARG1 because it tells us how to $ARG2",
        "$ARG1 can $ARG2",
        "$ARG1 to $ARG2"
    ],
    "deep learning*****generalization": [
        "$ARG1 introduces additional ( explicit and implicit ) priors in order to reduce the $ARG2"
    ],
    "other*****example": [
        "$ARG1 words , if we know a good answer for an input x ( for $ARG2",
        "$ARG1 cases , the network must produce some \ufb01xed-size output , for $ARG2"
    ],
    "example*****k-nearest neighbors": [
        "$ARG1 of the local constancy approach is the $ARG2"
    ],
    "k-nearest neighbors*****learning": [
        "$ARG1 family of $ARG2"
    ],
    "algorithm*****kernel": [
        "$ARG1 copies the output from nearby training examples , most $ARG2",
        "$ARG1 will learn the appropriate values of the $ARG2"
    ],
    "kernel*****set": [
        "$ARG1 machines interpolate between training $ARG2",
        "$ARG1 must be explicitly stored separately , we usually assume that these functions are zero everywhere but the \ufb01nite $ARG2"
    ],
    "kernel*****similarity": [
        "$ARG1 can be thought of as a $ARG2"
    ],
    "function*****template matching": [
        "$ARG1 that performs $ARG2"
    ],
    "template matching*****example": [
        "$ARG1 , by measuring how closely a test $ARG2"
    ],
    "deep learning*****template matching": [
        "$ARG1 is derived from studying the limitations of local $ARG2"
    ],
    "decision tree*****learning": [
        "$ARG1 also su\ufb00er from the limitations of exclusively smoothness-based $ARG2"
    ],
    "learning*****decision tree": [
        "$ARG1 because they break the input space into as many regions as there are leaves and use a separate parameter ( or sometimes many parameters for extensions of $ARG2"
    ],
    "generalization*****example": [
        "$ARG1 and the smoothness or local constancy prior , we would be guaranteed to correctly guess the color of a new point if it lies within the same checkerboard square as a training $ARG2",
        "$ARG1 exhibit similar behaviors : each training $ARG2"
    ],
    "machine learning*****illustration": [
        "$ARG1 BASICS Figure 5.10 : $ARG2"
    ],
    "example*****boundary": [
        "$ARG1 ( represented here by a circle ) within each region de\ufb01nes the region $ARG2"
    ],
    "change*****dimension": [
        "$ARG1 smoothly but in a di\ufb00erent way along each $ARG2"
    ],
    "number*****data generating distribution": [
        "$ARG1 of regions , e.g. , O ( 2k ) , can be de\ufb01ned with O ( k ) examples , so long as we introduce some dependencies between the regions via additional assumptions about the underlying $ARG2"
    ],
    "task*****assumption": [
        "$ARG1 by providing the $ARG2"
    ],
    "deep learning*****multiple": [
        "$ARG1 is that we assume that the data was generated by the composition of factors or features , potentially at $ARG2"
    ],
    "distributed representation*****challenges": [
        "$ARG1 counter the exponential $ARG2"
    ],
    "machine learning*****concept": [
        "$ARG1 BASICS An important $ARG2"
    ],
    "concept*****machine learning": [
        "$ARG1 underlying many ideas in $ARG2"
    ],
    "example*****manifold": [
        "$ARG1 of the world \u2019 s surface as a $ARG2",
        "$ARG1 of training data lying near a one-dimensional $ARG2",
        "$ARG1 , a \ufb01gure eight is a $ARG2",
        "$ARG1 , the $ARG2",
        "$ARG1 the $ARG2"
    ],
    "manifold*****machine learning": [
        "$ARG1 , \u201d in $ARG2",
        "$ARG1 , it can be most natural for $ARG2",
        "$ARG1 coordinates is challenging , but holds the promise to improve many $ARG2",
        "$ARG1 Many $ARG2"
    ],
    "manifold*****dimension": [
        "$ARG1 that has a single $ARG2"
    ],
    "manifold learning*****collection": [
        "$ARG1 algorithms surmount this obstacle by assuming that most of R n consists of invalid inputs , and that interesting inputs occur only along a $ARG2"
    ],
    "collection*****manifolds": [
        "$ARG1 of $ARG2",
        "$ARG1 of 0-dimensional $ARG2"
    ],
    "manifolds*****subset": [
        "$ARG1 containing a small $ARG2"
    ],
    "function*****manifold": [
        "$ARG1 occurring only along directions that lie on the $ARG2",
        "$ARG1 that behaves correctly on the $ARG2",
        "$ARG1 insensitive to perturbations of the input around the data points , we cause the autoencoder to recover the $ARG2",
        "$ARG1 , as in the $ARG2"
    ],
    "manifold learning*****unsupervised learning": [
        "$ARG1 was introduced in the case of continuous-valued data and the $ARG2",
        "$ARG1 has mostly focused on $ARG2"
    ],
    "unsupervised learning*****probability": [
        "$ARG1 setting , although this $ARG2"
    ],
    "probability*****supervised learning": [
        "$ARG1 concentration idea can be generalized to both discrete data and the $ARG2"
    ],
    "supervised learning*****assumption": [
        "$ARG1 setting : the key $ARG2"
    ],
    "assumption*****probability": [
        "$ARG1 remains that $ARG2"
    ],
    "probability*****assumption": [
        "$ARG1 mass is The $ARG2"
    ],
    "assumption*****manifold": [
        "$ARG1 that the data lies along a low-dimensional $ARG2"
    ],
    "manifold hypothesis*****probability distribution": [
        "$ARG1 is that the $ARG2"
    ],
    "machine learning*****uniform distribution": [
        "$ARG1 BASICS Figure 5.12 : Sampling images uniformly at random ( by randomly picking each pixel according to a $ARG2"
    ],
    "probability*****image": [
        "$ARG1 to generate an $ARG2"
    ],
    "probability distribution*****number": [
        "$ARG1 are not su\ufb03cient to show that the data lies on a reasonably small $ARG2",
        "$ARG1 that was used to sample \u00b5 at training Because this sum includes an exponential $ARG2",
        "$ARG1 over a very large $ARG2"
    ],
    "number*****manifolds": [
        "$ARG1 of $ARG2"
    ],
    "other*****manifold": [
        "$ARG1 highly similar examples that may be reached by applying transformations to traverse the $ARG2"
    ],
    "manifold*****image": [
        "$ARG1 in $ARG2",
        "$ARG1 is de\ufb01ned by the input $ARG2"
    ],
    "human*****manifold": [
        "$ARG1 faces may not be connected to the $ARG2"
    ],
    "manifold*****dataset": [
        "$ARG1 structure of a $ARG2"
    ],
    "mathematics*****machine learning": [
        "$ARG1 and $ARG2"
    ],
    "dataset*****manifold": [
        "$ARG1 ( Gong et al. , 2000 ) for which the subjects were asked to move in such a way as to cover the two-dimensional $ARG2"
    ],
    "structured probabilistic model*****deep learning": [
        "$ARG1 are a key ingredient of many of the most important research topics in $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2",
        "$ARG1 FOR $ARG2"
    ],
    "structured probabilistic model*****probability distribution": [
        "$ARG1 is a way of describing a $ARG2",
        "$ARG1 is that they allow us to dramatically reduce the cost of representing $ARG2"
    ],
    "probability distribution*****graph": [
        "$ARG1 , using a $ARG2",
        "$ARG1 using a $ARG2",
        "$ARG1 with a $ARG2",
        "$ARG1 with a $ARG2"
    ],
    "graph*****random variable": [
        "$ARG1 to describe which $ARG2",
        "$ARG1 G whose vertices are the $ARG2",
        "$ARG1 where we impose some ordering on the $ARG2",
        "$ARG1 corresponds to a $ARG2"
    ],
    "random variable*****probability distribution": [
        "$ARG1 in the $ARG2",
        "$ARG1 is just a description of the states that are possible ; it must be coupled with a $ARG2",
        "$ARG1 , we describe $ARG2",
        "$ARG1 x and y are independent if their $ARG2",
        "$ARG1 x vary as we sample di\ufb00erent values of x from its $ARG2",
        "$ARG1 means that the $ARG2"
    ],
    "graph*****graph theory": [
        "$ARG1 \u201d in the $ARG2",
        "$ARG1 \u201d in the sense of $ARG2"
    ],
    "graph theory*****set": [
        "$ARG1 sense\u2014a $ARG2",
        "$ARG1 : a $ARG2"
    ],
    "graph*****graphical model": [
        "$ARG1 , these models are often also referred to as The $ARG2"
    ],
    "graphical model*****inference": [
        "$ARG1 research community is large and has developed many di\ufb00erent models , training algorithms , and $ARG2",
        "$ARG1 typically aim to maintain the tractability of exact $ARG2",
        "$ARG1 approach is called structured variational $ARG2"
    ],
    "graphical model*****deep learning": [
        "$ARG1 , with an emphasis on the concepts that have proven most useful to the $ARG2",
        "$ARG1 are used for $ARG2",
        "$ARG1 are used for $ARG2"
    ],
    "deep learning*****graphical model": [
        "$ARG1 may bene\ufb01t from reading the \ufb01nal section of this chapter , section 16.7 , in which we highlight some of the unique ways that $ARG2",
        "$ARG1 practitioners place on speci\ufb01c approaches to $ARG2",
        "$ARG1 , we usually make di\ufb00erent design decisions about how to combine these tools , resulting in overall algorithms and models that have a very di\ufb00erent \ufb02avor from more traditional $ARG2",
        "$ARG1 does not always involve especially deep $ARG2",
        "$ARG1 approach to $ARG2",
        "$ARG1 approach to $ARG2",
        "$ARG1 usually arise from interactions between latent variables in a structured $ARG2",
        "$ARG1 are usually the result of interactions between latent variables in a structured $ARG2"
    ],
    "learning*****inference": [
        "$ARG1 algorithms and $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 of knowledge or $ARG2",
        "$ARG1 algorithms and $ARG2",
        "$ARG1 procedure similar to EM , in which we alternate between performing MAP $ARG2",
        "$ARG1 algorithms based on MAP $ARG2",
        "$ARG1 \u201d and \u201c variational $ARG2",
        "$ARG1 and $ARG2"
    ],
    "inference*****graphical model": [
        "$ARG1 procedures than are commonly used by the rest of the $ARG2",
        "$ARG1 problems are intractable , even when we use a structured $ARG2",
        "$ARG1 in $ARG2"
    ],
    "graph*****probability distribution": [
        "$ARG1 to describe the structure of a $ARG2",
        "$ARG1 and the factorization of $ARG2",
        "$ARG1 corresponds to $ARG2",
        "$ARG1 and the factorization of $ARG2",
        "$ARG1 corresponds to $ARG2"
    ],
    "graph*****problem": [
        "$ARG1 structures are most suitable for a given $ARG2"
    ],
    "graphical model*****challenge": [
        "$ARG1 in The $ARG2"
    ],
    "challenge*****deep learning": [
        "$ARG1 of Unstructured Modeling The goal of $ARG2"
    ],
    "machine learning*****challenges": [
        "$ARG1 to the kinds of $ARG2"
    ],
    "audio*****speech": [
        "$ARG1 waveforms representing $ARG2"
    ],
    "speech*****multiple": [
        "$ARG1 , and documents containing $ARG2"
    ],
    "classi\ufb01cation*****distribution": [
        "$ARG1 algorithms can take an input from such a rich high-dimensional $ARG2"
    ],
    "process*****classi\ufb01cation": [
        "$ARG1 of $ARG2"
    ],
    "classi\ufb01cation*****information": [
        "$ARG1 discards most of the $ARG2"
    ],
    "natural image*****image": [
        "$ARG1 is an $ARG2"
    ],
    "deep learning*****option": [
        "$ARG1 the input , with no $ARG2"
    ],
    "density estimation*****machine learning": [
        "$ARG1 : given an input x , the $ARG2"
    ],
    "system*****data generating distribution": [
        "$ARG1 returns an estimate of the true density p ( x ) under the $ARG2"
    ],
    "vector*****system": [
        "$ARG1 is unusual , the $ARG2"
    ],
    "element*****example": [
        "$ARG1 of the estimated clean $ARG2"
    ],
    "element*****distribution": [
        "$ARG1 drawn from the wrong $ARG2"
    ],
    "task*****natural image": [
        "$ARG1 using small $ARG2"
    ],
    "distribution*****random variable": [
        "$ARG1 over thousands or millions of $ARG2",
        "$ARG1 is clear from the context , we may simply write the name of the $ARG2",
        "$ARG1 over a single binary $ARG2",
        "$ARG1 is nearly deterministic , because the $ARG2",
        "$ARG1 is nearly deterministic , because the $ARG2"
    ],
    "vector*****probability": [
        "$ARG1 x containing n discrete variables capable of taking on k values each , then the naive approach of representing P ( x ) by storing a lookup table with one $ARG2",
        "$ARG1 of the autoencoder and points towards higher $ARG2"
    ],
    "deep learning*****natural image": [
        "$ARG1 Figure 16.1 : Probabilistic modeling of $ARG2"
    ],
    "example*****color images": [
        "$ARG1 32 \u00d7 32 pixel $ARG2"
    ],
    "color images*****dataset": [
        "$ARG1 from the CIFAR-10 $ARG2"
    ],
    "structured probabilistic model*****dataset": [
        "$ARG1 trained on this $ARG2"
    ],
    "memory*****distribution": [
        "$ARG1 : the cost of storing the representation : For all but very small values of n and k , representing the $ARG2"
    ],
    "model*****estimator": [
        "$ARG1 increases , so does the amount of training data needed to choose the values of those parameters using a statistical $ARG2"
    ],
    "example*****n-gram": [
        "$ARG1 , like in back-o\ufb00 or smoothed $ARG2"
    ],
    "problem*****interaction": [
        "$ARG1 with the table-based approach is that we are explicitly modeling every possible kind of $ARG2"
    ],
    "interaction*****subset": [
        "$ARG1 between every possible $ARG2"
    ],
    "model*****random variable": [
        "$ARG1 each of their \ufb01nishing times as a continuous $ARG2"
    ],
    "interaction*****model": [
        "$ARG1 between Alice and Carol from our $ARG2"
    ],
    "structured probabilistic model*****random variable": [
        "$ARG1 provide a formal framework for modeling only direct interactions between $ARG2"
    ],
    "model*****structured probabilistic model": [
        "$ARG1 Structure $ARG2"
    ],
    "structured probabilistic model*****graph theory": [
        "$ARG1 use graphs ( in the $ARG2"
    ],
    "graph theory*****random variable": [
        "$ARG1 sense of \u201c nodes \u201d or \u201c vertices \u201d connected by edges ) to represent interactions between $ARG2"
    ],
    "graphical model*****structured probabilistic model": [
        "$ARG1 can be largely divided into two categories : models based on directed acyclic graphs , and models based on One kind of $ARG2"
    ],
    "structured probabilistic model*****directed graphical model": [
        "$ARG1 is the $ARG2"
    ],
    "directed graphical model*****belief network": [
        "$ARG1 , otherwise known as the $ARG2"
    ],
    "belief network*****bayesian network": [
        "$ARG1 or $ARG2"
    ],
    "directed graphical model*****bayesian network": [
        "$ARG1 are called \u201c directed \u201d because their edges are directed , Judea Pearl suggested using the term \u201c $ARG2"
    ],
    "deep learning*****directed graphical model": [
        "$ARG1 Figure 16.2 : A $ARG2"
    ],
    "variable*****probability distribution": [
        "$ARG1 \u2019 s $ARG2"
    ],
    "directed graphical model*****graph": [
        "$ARG1 de\ufb01ned on variables x is de\ufb01ned by a directed acyclic $ARG2"
    ],
    "set*****local conditional probability distribution": [
        "$ARG1 of $ARG2"
    ],
    "local conditional probability distribution*****probability distribution": [
        "$ARG1 p ( xi | P aG ( x i ) ) where P aG ( xi ) gives the parents of x i in G. The $ARG2"
    ],
    "deep learning*****constraint": [
        "$ARG1 redundant by the $ARG2"
    ],
    "conditional probability distribution*****distribution": [
        "$ARG1 , then the $ARG2"
    ],
    "directed graphical model*****number": [
        "$ARG1 reduced our $ARG2"
    ],
    "number*****conditional probability distribution": [
        "$ARG1 of variables appearing ( on either side of the conditioning bar ) in a single $ARG2"
    ],
    "conditional probability distribution*****model": [
        "$ARG1 , then the cost of the tables for the directed $ARG2"
    ],
    "information*****graph": [
        "$ARG1 can and can not be encoded in the $ARG2",
        "$ARG1 from an undirected $ARG2"
    ],
    "directed graphical model*****constraint": [
        "$ARG1 syntax does not place any $ARG2"
    ],
    "directed graphical model*****structured probabilistic model": [
        "$ARG1 give us one language for describing $ARG2"
    ],
    "undirected model*****markov random \ufb01eld": [
        "$ARG1 , otherwise known as $ARG2"
    ],
    "markov random \ufb01eld*****markov network": [
        "$ARG1 ( MRFs ) or $ARG2"
    ],
    "pass*****other": [
        "$ARG1 it on to the $ARG2"
    ],
    "model*****base": [
        "$ARG1 the indirect transmission of a cold from your coworker to your roommate by modeling the transmission of the cold from your coworker to you and the transmission of the cold from you to your In this case , it is just as easy for you to cause your roommate to get sick as it is for your roommate to make you sick , so there is not a clean , uni-directional narrative on which to $ARG2"
    ],
    "base*****model": [
        "$ARG1 the $ARG2"
    ],
    "undirected model*****random variable": [
        "$ARG1 are connected by an edge , then the $ARG2"
    ],
    "undirected model*****conditional probability distribution": [
        "$ARG1 has no arrow , and is not associated with a $ARG2"
    ],
    "deep learning*****graph": [
        "$ARG1 Figure 16.3 : An undirected $ARG2",
        "$ARG1 This $ARG2",
        "$ARG1 Figure 16.5 : This $ARG2",
        "$ARG1 Figure 16.9 : From this $ARG2"
    ],
    "undirected graphical model*****structured probabilistic model": [
        "$ARG1 is a $ARG2"
    ],
    "structured probabilistic model*****graph": [
        "$ARG1 de\ufb01ned on an undirected $ARG2"
    ],
    "bayesian network*****probability distribution": [
        "$ARG1 , there is little structure to the de\ufb01nition of the cliques , so there is nothing to guarantee that multiplying them together will yield a valid $ARG2"
    ],
    "example*****contains": [
        "$ARG1 of the cold spreading between you , your roommate , and your colleague $ARG2"
    ],
    "graph*****subset": [
        "$ARG1 is a $ARG2"
    ],
    "probability*****probability distribution": [
        "$ARG1 p ( x ) = p\u0303 ( x ) where Z is the value that results in the $ARG2",
        "$ARG1 according to the implicitly estimated $ARG2"
    ],
    "probability distribution*****undirected model": [
        "$ARG1 of an $ARG2",
        "$ARG1 from the start , while $ARG2"
    ],
    "undirected model*****model": [
        "$ARG1 , the $ARG2",
        "$ARG1 can include substructures that no directed $ARG2",
        "$ARG1 to a directed $ARG2",
        "$ARG1 can not be converted directed to a directed $ARG2",
        "$ARG1 encodes two di\ufb00erent independences that no directed $ARG2",
        "$ARG1 to a directed $ARG2",
        "$ARG1 without \ufb01rst converting it to a directed $ARG2",
        "$ARG1 may be trained by computing such derivatives applied to samples from the $ARG2"
    ],
    "model*****integral": [
        "$ARG1 are continuous and the $ARG2"
    ],
    "integral*****distribution": [
        "$ARG1 A $ARG2"
    ],
    "distribution*****clique potential": [
        "$ARG1 de\ufb01ned by normalizing a product of $ARG2"
    ],
    "model*****scalar": [
        "$ARG1 a single $ARG2"
    ],
    "variable*****clique potential": [
        "$ARG1 x \u2208 R with a single $ARG2"
    ],
    "integral*****probability distribution": [
        "$ARG1 diverges , there is no $ARG2",
        "$ARG1 de\ufb01ning Z diverges and no $ARG2"
    ],
    "other*****undirected model": [
        "$ARG1 values of \u03b2 make \u03c6 impossible One key di\ufb00erence between directed modeling and $ARG2",
        "$ARG1 distributions can be represented more e\ufb03ciently using $ARG2",
        "$ARG1 words , directed models can encode some independences that $ARG2"
    ],
    "undirected model*****probability distribution": [
        "$ARG1 is that directed models are de\ufb01ned directly in terms of $ARG2",
        "$ARG1 are de\ufb01ned more loosely by \u03c6 functions that are then converted into $ARG2",
        "$ARG1 is that the domain of each of the variables has dramatic e\ufb00ect on the kind of $ARG2",
        "$ARG1 based on which approach can capture the most independences in the $ARG2"
    ],
    "example*****random variable": [
        "$ARG1 , consider an n-dimensional vector-valued $ARG2",
        "$ARG1 , x1 and x2 are both possible values that the $ARG2",
        "$ARG1 , suppose we have three $ARG2"
    ],
    "random variable*****undirected model": [
        "$ARG1 x and an $ARG2"
    ],
    "undirected model*****vector": [
        "$ARG1 parametrized by a $ARG2"
    ],
    "probability distribution*****information": [
        "$ARG1 does this result in ? The answer is that we do not have enough $ARG2"
    ],
    "variable*****set": [
        "$ARG1 in order to obtain complicated behavior from a relatively simple $ARG2"
    ],
    "undirected model*****assumption": [
        "$ARG1 depend on the $ARG2"
    ],
    "graph*****energy function": [
        "$ARG1 implies that p ( a , b , c , d , e , f ) can be written as c ) \u03c6a , d ( a , d ) \u03c6 b , e ( b , e ) \u03c6e , f ( e , f ) for an appropriate choice of the \u03c6 func\u03c6 Z a , b and E ( x ) is known as the $ARG2",
        "$ARG1 correspond to the di\ufb00erent terms of the $ARG2",
        "$ARG1 implies that E ( a , b , c , d , e , f ) can be written as Ea , b ( a , b ) + Eb , c ( b , c ) + Ea , d ( a , d ) + E b , e ( b , e ) + E e , f ( e , f ) for an appropriate choice of the per-clique $ARG2"
    ],
    "energy function*****probability": [
        "$ARG1 will result in a $ARG2"
    ],
    "energy function*****learning": [
        "$ARG1 makes $ARG2"
    ],
    "clique potential*****constrained optimization": [
        "$ARG1 directly , we would need to use $ARG2"
    ],
    "learning*****energy function": [
        "$ARG1 the $ARG2"
    ],
    "energy function*****energy-based model": [
        "$ARG1 , we can use unconstrained optimization.5 The probabilities in an $ARG2"
    ],
    "example*****boltzmann distribution": [
        "$ARG1 of a $ARG2"
    ],
    "model*****energy-based model": [
        "$ARG1 an $ARG2"
    ],
    "boltzmann machine*****markov random \ufb01eld": [
        "$ARG1 without latent variables are more often called $ARG2"
    ],
    "graph*****probability": [
        "$ARG1 correspond to factors of the unnormalized $ARG2"
    ],
    "other*****energy-based model": [
        "$ARG1 words , an $ARG2"
    ],
    "energy-based model*****markov network": [
        "$ARG1 is just a special kind of $ARG2"
    ],
    "markov network*****energy function": [
        "$ARG1 : the exponentiation makes each term in the $ARG2"
    ],
    "example*****constrained optimization": [
        "$ARG1 of how to read the For some models , we may still need to use $ARG2"
    ],
    "energy function*****graph": [
        "$ARG1 from an undirected $ARG2"
    ],
    "view*****energy-based model": [
        "$ARG1 an $ARG2"
    ],
    "energy-based model*****multiple": [
        "$ARG1 with $ARG2"
    ],
    "multiple*****energy function": [
        "$ARG1 terms in its $ARG2"
    ],
    "energy function*****product of experts": [
        "$ARG1 as being a $ARG2"
    ],
    "energy function*****probability distribution": [
        "$ARG1 corresponds to another factor in the $ARG2"
    ],
    "constraint*****random variable": [
        "$ARG1 that concerns only a low-dimensional projection of the $ARG2"
    ],
    "random variable*****multiplication": [
        "$ARG1 , but when combined by $ARG2"
    ],
    "multiplication*****constraint": [
        "$ARG1 of probabilities , the experts together enforce a complicated highdimensional $ARG2"
    ],
    "energy-based model*****machine learning": [
        "$ARG1 serves no functional purpose from a $ARG2"
    ],
    "machine learning*****view": [
        "$ARG1 point of $ARG2"
    ],
    "machine learning*****standard": [
        "$ARG1 researchers ( e.g. , Smolensky ( 1986 ) , who referred to negative energy as harmony ) have chosen to emit the negation , but this is not the $ARG2"
    ],
    "deep learning*****path": [
        "$ARG1 Figure 16.6 : ( a ) The $ARG2"
    ],
    "path*****random variable": [
        "$ARG1 between $ARG2"
    ],
    "conditional independence*****graph": [
        "$ARG1 in a $ARG2",
        "$ARG1 implied by the $ARG2",
        "$ARG1 that are implied by the $ARG2",
        "$ARG1 implied by an undirected $ARG2"
    ],
    "graph*****undirected model": [
        "$ARG1 is very simple in the case of $ARG2",
        "$ARG1 D into an $ARG2"
    ],
    "graph*****path": [
        "$ARG1 structure implies that A is independent from B given S. If two variables a and b are connected by a $ARG2"
    ],
    "deep learning*****example": [
        "$ARG1 Figure 16.7 : An $ARG2",
        "$ARG1 Figure 7.7 : An $ARG2"
    ],
    "path*****other": [
        "$ARG1 from a to c , we say that a and c are separated from each $ARG2"
    ],
    "undirected model*****graph": [
        "$ARG1 , we can examine the independences implied by the $ARG2",
        "$ARG1 , the complete $ARG2",
        "$ARG1 , we need to create a new $ARG2",
        "$ARG1 that consists of a bipartite undirected $ARG2"
    ],
    "example*****d-separation": [
        "$ARG1 of reading some properties from a It is important to remember that separation and $ARG2"
    ],
    "d-separation*****conditional independence": [
        "$ARG1 tell us only about those $ARG2"
    ],
    "graph*****independence": [
        "$ARG1 will never imply that an $ARG2",
        "$ARG1 may fail to encode an $ARG2"
    ],
    "deep learning*****random variable": [
        "$ARG1 Figure 16.8 : All of the kinds of active paths of length two that can exist between $ARG2",
        "$ARG1 frameworks and this \ufb01gure illustrates the common situation where the tasks share a common input but involve di\ufb00erent target $ARG2"
    ],
    "path*****example": [
        "$ARG1 in the relay race $ARG2"
    ],
    "variable*****measure": [
        "$ARG1 indicating whether or not there is a hurricane and a and b $ARG2",
        "$ARG1 in order to $ARG2"
    ],
    "expected*****change": [
        "$ARG1 wind at a ( for a hurricane ) would not $ARG2"
    ],
    "v-structure*****collider": [
        "$ARG1 or the $ARG2"
    ],
    "v-structure*****explaining away": [
        "$ARG1 causes a and b to be related by the $ARG2"
    ],
    "explaining away*****example": [
        "$ARG1 e\ufb00ect happens even if any descendant of s is observed ! For $ARG2"
    ],
    "probability*****turn": [
        "$ARG1 that she is not at work today , which in $ARG2"
    ],
    "path*****v-structure": [
        "$ARG1 through a $ARG2"
    ],
    "graph*****d-separation": [
        "$ARG1 , we can read out several $ARG2"
    ],
    "example*****sparse coding": [
        "$ARG1 , we typically refer to RBMs as undirected and $ARG2",
        "$ARG1 , this $ARG2"
    ],
    "subset*****task": [
        "$ARG1 of variables , or if we wish to perform a di\ufb00erent computational $ARG2"
    ],
    "model*****undirected model": [
        "$ARG1 ( described in section 16.3 ) while the $ARG2",
        "$ARG1 or by an $ARG2",
        "$ARG1 that can not be converted to an $ARG2"
    ],
    "undirected model*****approximate inference": [
        "$ARG1 formulation is often useful for deriving $ARG2"
    ],
    "approximate inference*****undirected model": [
        "$ARG1 procedures ( as we will see in chapter 19 , where the role of $ARG2"
    ],
    "random variable*****variable": [
        "$ARG1 , and each $ARG2",
        "$ARG1 is a $ARG2",
        "$ARG1 may be decomposed into conditional distributions over only one $ARG2"
    ],
    "immorality*****graphical model": [
        "$ARG1 \u201d may seem strange ; it was coined in the $ARG2"
    ],
    "graph*****conditional independence": [
        "$ARG1 D can not capture all of the $ARG2"
    ],
    "graph*****contains": [
        "$ARG1 U if U $ARG2"
    ],
    "contains*****loop": [
        "$ARG1 a $ARG2"
    ],
    "loop*****contains": [
        "$ARG1 also $ARG2"
    ],
    "contains*****chord": [
        "$ARG1 a $ARG2"
    ],
    "loop*****variable": [
        "$ARG1 is a sequence of variables connected by undirected edges , with the last $ARG2"
    ],
    "chord*****connection": [
        "$ARG1 is a $ARG2"
    ],
    "connection*****loop": [
        "$ARG1 between any two non-consecutive variables in the sequence de\ufb01ning a $ARG2"
    ],
    "chords*****model": [
        "$ARG1 before we can convert it to a directed $ARG2"
    ],
    "deep learning*****row": [
        "$ARG1 Figure 16.11 : Examples of converting directed models ( top $ARG2"
    ],
    "row*****undirected model": [
        "$ARG1 ) to $ARG2"
    ],
    "undirected model*****row": [
        "$ARG1 ( bottom $ARG2"
    ],
    "row*****moralized graph": [
        "$ARG1 ) by constructing $ARG2"
    ],
    "undirected model*****set": [
        "$ARG1 implies exactly the same $ARG2",
        "$ARG1 use graphs with undirected edges , and they represent factorizations into a $ARG2"
    ],
    "set*****conditional independence": [
        "$ARG1 of independences and $ARG2"
    ],
    "graph*****immorality": [
        "$ARG1 consists entirely of a single $ARG2"
    ],
    "dependence*****undirected model": [
        "$ARG1 , the $ARG2"
    ],
    "model*****loop": [
        "$ARG1 because it has a $ARG2"
    ],
    "loop*****chords": [
        "$ARG1 of length four with no $ARG2"
    ],
    "graph*****chord": [
        "$ARG1 , by ensuring that all loops of greater than length three have a $ARG2"
    ],
    "variable*****chords": [
        "$ARG1 names to impose these $ARG2"
    ],
    "chords*****independence": [
        "$ARG1 discards some of the $ARG2"
    ],
    "graph*****chords": [
        "$ARG1 formed by adding $ARG2"
    ],
    "chords*****triangulated graph": [
        "$ARG1 to U is known as a chordal or $ARG2"
    ],
    "graph*****chordal graph": [
        "$ARG1 D from the $ARG2"
    ],
    "factor graph*****undirected model": [
        "$ARG1 are another way of drawing $ARG2",
        "$ARG1 is a graphical representation of an $ARG2",
        "$ARG1 corresponding to the same $ARG2",
        "$ARG1 for the same $ARG2"
    ],
    "undirected model*****ambiguity": [
        "$ARG1 that resolve an $ARG2"
    ],
    "ambiguity*****standard": [
        "$ARG1 in the graphical representation of $ARG2"
    ],
    "undirected model*****function": [
        "$ARG1 , the scope of every \u03c6 $ARG2"
    ],
    "ambiguity*****example": [
        "$ARG1 arises because it is not clear if each clique actually has a corresponding factor whose scope encompasses the entire clique\u2014for $ARG2"
    ],
    "factor graph*****ambiguity": [
        "$ARG1 resolve this $ARG2",
        "$ARG1 can resolve $ARG2",
        "$ARG1 can resolve $ARG2"
    ],
    "ambiguity*****function": [
        "$ARG1 by explicitly representing the scope of each \u03c6 $ARG2"
    ],
    "random variable*****standard": [
        "$ARG1 as in a $ARG2"
    ],
    "variable*****unnormalized probability distribution": [
        "$ARG1 is one of the arguments to the factor in the $ARG2"
    ],
    "example*****factor graph": [
        "$ARG1 of how $ARG2",
        "$ARG1 of how a $ARG2"
    ],
    "ambiguity*****interpretation": [
        "$ARG1 in the $ARG2",
        "$ARG1 in the $ARG2"
    ],
    "interpretation*****example": [
        "$ARG1 of Figure 16.13 : An $ARG2"
    ],
    "learning*****factor graph": [
        "$ARG1 are all asymptotically cheaper in this $ARG2"
    ],
    "factor graph*****graph": [
        "$ARG1 depicted in the center , even though both require the same undirected $ARG2"
    ],
    "graphical model*****task": [
        "$ARG1 also facilitate the $ARG2"
    ],
    "directed graphical model*****ancestral sampling": [
        "$ARG1 is that a simple and e\ufb03cient procedure called $ARG2"
    ],
    "ancestral sampling*****distribution": [
        "$ARG1 can produce a sample from the joint $ARG2"
    ],
    "ancestral sampling*****directed graphical model": [
        "$ARG1 is that it only applies to $ARG2"
    ],
    "support*****operation": [
        "$ARG1 every conditional sampling $ARG2",
        "$ARG1 , since the $ARG2"
    ],
    "subset*****directed graphical model": [
        "$ARG1 of the variables in a $ARG2"
    ],
    "directed graphical model*****other": [
        "$ARG1 , given some $ARG2"
    ],
    "local conditional probability distribution*****model": [
        "$ARG1 speci\ufb01ed by the $ARG2"
    ],
    "undirected model*****inference": [
        "$ARG1 by converting them to directed models , but this often requires solving intractable $ARG2"
    ],
    "distribution*****graph": [
        "$ARG1 over the root nodes of the new directed $ARG2",
        "$ARG1 corresponds to the complete $ARG2"
    ],
    "variable*****process": [
        "$ARG1 , so there is no clear beginning point for the sampling $ARG2"
    ],
    "undirected graphical model*****process": [
        "$ARG1 is an expensive , multi-pass $ARG2"
    ],
    "graphical model*****vector": [
        "$ARG1 over an n-dimensional $ARG2"
    ],
    "vector*****random variable": [
        "$ARG1 of $ARG2"
    ],
    "pass*****graphical model": [
        "$ARG1 through the $ARG2"
    ],
    "undirected model*****structured probabilistic model": [
        "$ARG1 are an advanced topic , covered in more detail in Advantages of Structured Modeling The primary advantage of using $ARG2"
    ],
    "graphical model*****information": [
        "$ARG1 convey $ARG2"
    ],
    "model*****assumption": [
        "$ARG1 speci\ufb01es the $ARG2"
    ],
    "assumption*****model": [
        "$ARG1 that we do not need to $ARG2"
    ],
    "structured probabilistic model*****learning": [
        "$ARG1 is that they allow us to explicitly separate representation of knowledge from $ARG2"
    ],
    "deep learning*****bayesian network": [
        "$ARG1 have very large numbers of parents per node in a $ARG2"
    ],
    "bayesian network*****markov network": [
        "$ARG1 or very large cliques in a $ARG2"
    ],
    "machine learning*****structure learning": [
        "$ARG1 called $ARG2"
    ],
    "structure learning*****problem": [
        "$ARG1 is devoted to this $ARG2"
    ],
    "problem*****structure learning": [
        "$ARG1 For a good reference on $ARG2"
    ],
    "structure learning*****search": [
        "$ARG1 techniques are a form of greedy $ARG2"
    ],
    "number*****search": [
        "$ARG1 of edges added or removed are then proposed as the next step of the $ARG2"
    ],
    "search*****expected": [
        "$ARG1 proceeds to a new structure that is $ARG2"
    ],
    "sparse coding*****manifold": [
        "$ARG1 learn latent variables that can be used as input features for a classi\ufb01er , or as coordinates along a $ARG2"
    ],
    "feature learning*****learning": [
        "$ARG1 by $ARG2"
    ],
    "inference*****approximate inference": [
        "$ARG1 and $ARG2",
        "$ARG1 is usually not thought of as $ARG2",
        "$ARG1 as $ARG2",
        "$ARG1 as a form of $ARG2",
        "$ARG1 Using $ARG2",
        "$ARG1 This strategy of learned $ARG2"
    ],
    "approximate inference*****model": [
        "$ARG1 One of the main ways we can use a probabilistic $ARG2",
        "$ARG1 respect to L. Using the same approach on a di\ufb00erent $ARG2",
        "$ARG1 in a $ARG2"
    ],
    "other*****probability distribution": [
        "$ARG1 variables , or predict the $ARG2",
        "$ARG1 simpler $ARG2"
    ],
    "number*****deep learning": [
        "$ARG1 of parameters , but the graphs used for $ARG2"
    ],
    "deep learning*****inference": [
        "$ARG1 are usually not restrictive enough to also allow e\ufb03cient $ARG2",
        "$ARG1 , this usually refers to variational $ARG2"
    ],
    "marginal probability*****graphical model": [
        "$ARG1 of a general $ARG2"
    ],
    "graphical model*****problem": [
        "$ARG1 over the binary variables in a 3-SAT $ARG2"
    ],
    "reduction*****other": [
        "$ARG1 tree of latent variables , with each node in the tree reporting whether two $ARG2"
    ],
    "uniform distribution*****distribution": [
        "$ARG1 over the literals , the marginal $ARG2"
    ],
    "distribution*****reduction": [
        "$ARG1 over the root of the $ARG2"
    ],
    "reduction*****problem": [
        "$ARG1 tree speci\ufb01es what fraction of assignments satisfy the $ARG2"
    ],
    "example*****approximate inference": [
        "$ARG1 , NP hard graphs commonly arise in practical This motivates the use of $ARG2",
        "$ARG1 of learned $ARG2",
        "$ARG1 , in part III , we will see some $ARG2"
    ],
    "deep learning*****distribution": [
        "$ARG1 true $ARG2"
    ],
    "deep learning*****structured probabilistic model": [
        "$ARG1 Approach to $ARG2"
    ],
    "machine learning*****structured probabilistic model": [
        "$ARG1 practitioners who work with $ARG2"
    ],
    "model*****graphical model": [
        "$ARG1 in terms of the $ARG2"
    ],
    "graph*****computational graph": [
        "$ARG1 rather than the $ARG2"
    ],
    "variable*****path": [
        "$ARG1 hi as being at depth j if the shortest $ARG2"
    ],
    "deep learning*****distributed representation": [
        "$ARG1 essentially always makes use of the idea of $ARG2",
        "$ARG1 tend to connect each visible unit vi to very many hidden units hj , so that h can provide a $ARG2"
    ],
    "deep learning*****pretraining": [
        "$ARG1 purposes ( such as $ARG2"
    ],
    "contrast*****graphical model": [
        "$ARG1 , traditional $ARG2"
    ],
    "graphical model*****variable": [
        "$ARG1 have very few connections and the choice of connections for each $ARG2"
    ],
    "constraint*****approximate inference": [
        "$ARG1 is too limiting , a popular $ARG2"
    ],
    "algorithm*****loopy belief propagation": [
        "$ARG1 called $ARG2"
    ],
    "distributed representation*****view": [
        "$ARG1 have many advantages , but from the point of $ARG2"
    ],
    "distributed representation*****inference": [
        "$ARG1 have the disadvantage of usually yielding graphs that are not sparse enough for the traditional techniques of exact $ARG2"
    ],
    "inference*****loopy belief propagation": [
        "$ARG1 and $ARG2"
    ],
    "graphical model*****loopy belief propagation": [
        "$ARG1 community is that $ARG2"
    ],
    "loopy belief propagation*****deep learning": [
        "$ARG1 is almost never used for $ARG2"
    ],
    "gibbs sampling*****inference": [
        "$ARG1 or variational $ARG2"
    ],
    "matrix*****interaction": [
        "$ARG1 describing the $ARG2",
        "$ARG1 of parameters with a separate parameter describing the $ARG2"
    ],
    "algorithm*****matrix product": [
        "$ARG1 to be implemented with e\ufb03cient $ARG2"
    ],
    "matrix product*****block diagonal matrix": [
        "$ARG1 operations , or sparsely connected generalizations , like $ARG2"
    ],
    "example*****restricted boltzmann machine": [
        "$ARG1 : The $ARG2"
    ],
    "restricted boltzmann machine*****harmonium": [
        "$ARG1 ( RBM ) ( Smolensky , 1986 ) or $ARG2"
    ],
    "harmonium*****example": [
        "$ARG1 is the quintessential $ARG2"
    ],
    "example*****graphical model": [
        "$ARG1 of how $ARG2"
    ],
    "graphical model*****matrix": [
        "$ARG1 : its units are organized into large groups called layers , the connectivity between layers is described by a $ARG2"
    ],
    "model*****gibbs sampling": [
        "$ARG1 is designed to allow e\ufb03cient $ARG2",
        "$ARG1 trained on MNIST , drawn using $ARG2"
    ],
    "gibbs sampling*****model": [
        "$ARG1 , and the emphasis of the $ARG2",
        "$ARG1 from an RBM $ARG2"
    ],
    "interaction*****matrix": [
        "$ARG1 between them is described by a $ARG2"
    ],
    "model*****boltzmann machine": [
        "$ARG1 is that there are no direct interactions between any two visible units or between any two hidden units ( hence the \u201c restricted , \u201d a general $ARG2"
    ],
    "deep learning*****markov network": [
        "$ARG1 Figure 16.14 : An RBM drawn as a $ARG2"
    ],
    "energy function*****function": [
        "$ARG1 itself is just a linear $ARG2"
    ],
    "graphical model*****representation learning": [
        "$ARG1 : $ARG2"
    ],
    "deep learning*****weights": [
        "$ARG1 Figure 16.15 : Samples from a trained RBM , and its $ARG2"
    ],
    "row*****gibbs sampling": [
        "$ARG1 represents the output of another 1,000 steps of $ARG2"
    ],
    "other*****sparse coding": [
        "$ARG1 hand , the RBM posterior p ( h | v ) is factorial , while the $ARG2"
    ],
    "set*****challenge": [
        "$ARG1 of latent variables h. The $ARG2"
    ],
    "maximum likelihood*****graphical model": [
        "$ARG1 Many simple $ARG2"
    ],
    "graphical model*****hidden layer": [
        "$ARG1 with only one $ARG2"
    ],
    "restricted boltzmann machine*****probabilistic pca": [
        "$ARG1 and $ARG2"
    ],
    "probabilistic pca*****inference": [
        "$ARG1 , are de\ufb01ned in a way that makes $ARG2"
    ],
    "sparse coding*****problem": [
        "$ARG1 , have this $ARG2"
    ],
    "inference*****deep learning": [
        "$ARG1 problems in $ARG2",
        "$ARG1 problems in $ARG2",
        "$ARG1 is commonly used in $ARG2"
    ],
    "undirected model*****explaining away": [
        "$ARG1 or \u201c $ARG2"
    ],
    "approximate inference*****inference": [
        "$ARG1 Figure 19.1 : Intractable $ARG2",
        "$ARG1 MAP $ARG2",
        "$ARG1 Variational $ARG2",
        "$ARG1 To make this more concrete , we show how to apply variational $ARG2",
        "$ARG1 We have seen that $ARG2"
    ],
    "variable*****v-structure": [
        "$ARG1 to another , or due to longer paths that are activated when the child of a $ARG2"
    ],
    "deep boltzmann machine*****distribution": [
        "$ARG1 , organized into layers of variables without intra-layer connections , still has an intractable posterior $ARG2"
    ],
    "inference*****graph": [
        "$ARG1 over the latent variables despite having one of the $ARG2"
    ],
    "conditional probability distribution*****graph": [
        "$ARG1 are chosen to introduce additional independences beyond those described by the $ARG2"
    ],
    "example*****probabilistic pca": [
        "$ARG1 , $ARG2"
    ],
    "probabilistic pca*****graph": [
        "$ARG1 has the $ARG2"
    ],
    "graph*****inference": [
        "$ARG1 structure shown in the right , yet still has simple $ARG2"
    ],
    "approximate inference*****problem": [
        "$ARG1 Many approaches to confronting the $ARG2"
    ],
    "inference*****optimization problem": [
        "$ARG1 can be described as an $ARG2"
    ],
    "approximate inference*****optimization problem": [
        "$ARG1 algorithms may then be derived by approximating the underlying $ARG2",
        "$ARG1 algorithms that work by using algebraic solutions to $ARG2"
    ],
    "optimization problem*****model": [
        "$ARG1 , assume we have a probabilistic $ARG2"
    ],
    "evidence lower bound*****probability distribution": [
        "$ARG1 is de\ufb01ned to be L ( v , \u03b8 , q ) = log p ( v ; \u03b8 ) \u2212 DKL ( q ( h | v ) \ue06bp ( h | v ; \u03b8 ) ) where q is an arbitrary $ARG2"
    ],
    "divergence*****probability": [
        "$ARG1 is always non-negative , we can see that L always has at most the same value as the desired log $ARG2",
        "$ARG1 is minimized by choosing a single mode , in order to avoid putting $ARG2"
    ],
    "approximate inference*****optimization": [
        "$ARG1 by using approximate $ARG2"
    ],
    "optimization*****search": [
        "$ARG1 is allowed to $ARG2",
        "$ARG1 procedure to $ARG2"
    ],
    "search*****optimization": [
        "$ARG1 over or by using an imperfect $ARG2"
    ],
    "algorithm*****expectation maximization": [
        "$ARG1 we introduce based on maximizing a lower bound L is the $ARG2"
    ],
    "expectation maximization*****algorithm": [
        "$ARG1 ( EM ) $ARG2"
    ],
    "approximate inference*****learning": [
        "$ARG1 , but rather an approach to $ARG2",
        "$ARG1 as part of a $ARG2"
    ],
    "algorithm*****convergence": [
        "$ARG1 consists of alternating between two steps until $ARG2"
    ],
    "convergence*****e-step": [
        "$ARG1 : \u2022 The $ARG2"
    ],
    "e-step*****expectation": [
        "$ARG1 ( $ARG2"
    ],
    "set*****minibatch": [
        "$ARG1 q ( h ( i ) | v ) = p ( h ( i ) | v ( i ) ; \u03b8 ( 0 ) ) for all indices i of the training examples v ( i ) we want to train on ( both batch and $ARG2"
    ],
    "approximate inference*****optimization algorithm": [
        "$ARG1 with respect to \u03b8 using your $ARG2"
    ],
    "variable*****special case": [
        "$ARG1 models can be seen as a $ARG2"
    ],
    "e-step*****inference": [
        "$ARG1 involves exact $ARG2"
    ],
    "inference*****algorithm": [
        "$ARG1 , we can think of the EM $ARG2",
        "$ARG1 and training $ARG2"
    ],
    "algorithm*****approximate inference": [
        "$ARG1 as using $ARG2"
    ],
    "m-step*****e-step": [
        "$ARG1 moves further and further away from the value \u03b8 ( 0 ) used in the $ARG2"
    ],
    "e-step*****loop": [
        "$ARG1 reduces the gap to zero again as we enter the $ARG2"
    ],
    "gradient descent*****property": [
        "$ARG1 to maximize the log-likelihood also has this same $ARG2"
    ],
    "gradient*****distribution": [
        "$ARG1 computations require taking expectations with respect to the posterior $ARG2"
    ],
    "machine learning*****m-step": [
        "$ARG1 to derive large $ARG2"
    ],
    "deep learning*****solution": [
        "$ARG1 , most models are too complex to admit a tractable $ARG2"
    ],
    "solution*****m-step": [
        "$ARG1 for an optimal large $ARG2"
    ],
    "m-step*****algorithm": [
        "$ARG1 update , so this second insight which is more unique to the EM $ARG2"
    ],
    "inference*****sparse coding": [
        "$ARG1 and $ARG2",
        "$ARG1 to the binary $ARG2"
    ],
    "inference*****probability distribution": [
        "$ARG1 to refer to computing the $ARG2",
        "$ARG1 consists of maximizing L ( v , \u03b8 , q ) = E h\u223cq [ log p ( h , v ) ] + H ( q ) with respect to q over an unrestricted family of $ARG2"
    ],
    "approximate inference*****variable": [
        "$ARG1 of latent $ARG2"
    ],
    "process*****inference": [
        "$ARG1 based on maximizing L ( v , h , q ) , then it is helpful to think of MAP $ARG2",
        "$ARG1 as $ARG2"
    ],
    "approximate inference*****recall": [
        "$ARG1 , because it does not provide the optimal $ARG2"
    ],
    "recall*****inference": [
        "$ARG1 from section 19.1 that exact $ARG2"
    ],
    "probability distribution*****optimization algorithm": [
        "$ARG1 , using an exact $ARG2"
    ],
    "optimization problem*****inference": [
        "$ARG1 \u00b5\u2217 = arg max log p ( h = \u00b5 , v ) , which is equivalent to the MAP $ARG2"
    ],
    "distribution*****di\ufb00erential entropy": [
        "$ARG1 \u2019 s $ARG2"
    ],
    "deep learning*****feature": [
        "$ARG1 as both a $ARG2",
        "$ARG1 each input $ARG2"
    ],
    "recall*****sparse coding": [
        "$ARG1 from section 13.4 that $ARG2"
    ],
    "model*****covariance matrix": [
        "$ARG1 were Gaussian then these interactions could be modeled e\ufb03ciently via the $ARG2"
    ],
    "matrix*****sparse coding": [
        "$ARG1 V , then the $ARG2"
    ],
    "process*****sparse coding": [
        "$ARG1 consists of minimizing J ( H , W ) = |Hi , j | + Most applications of $ARG2"
    ],
    "learning*****evidence lower bound": [
        "$ARG1 We have seen how the $ARG2",
        "$ARG1 The $ARG2"
    ],
    "evidence lower bound*****inference": [
        "$ARG1 L ( v , \u03b8 , q ) is a lower bound on log p ( v ; \u03b8 ) , how $ARG2"
    ],
    "optimization problem*****probability distribution": [
        "$ARG1 determines the optimal $ARG2"
    ],
    "number*****distribution": [
        "$ARG1 of variables describing the q $ARG2",
        "$ARG1 of parameters must impose strict limits on the $ARG2",
        "$ARG1 of parameters needed to describe the $ARG2"
    ],
    "mathematics*****calculus of variations": [
        "$ARG1 called $ARG2"
    ],
    "calculus of variations*****optimization": [
        "$ARG1 to perform $ARG2"
    ],
    "optimization*****function": [
        "$ARG1 over a space of functions , and actually determine which $ARG2",
        "$ARG1 procedure that increases the value of a $ARG2",
        "$ARG1 ( \ufb01nding the value of an argument that minimizes or maximizes a $ARG2",
        "$ARG1 very di\ufb03cult , especially when the input to the $ARG2"
    ],
    "calculus of variations*****learning": [
        "$ARG1 is the origin of the names \u201c variational $ARG2",
        "$ARG1 if one wishes to develop a new form of variational $ARG2"
    ],
    "inference*****calculus of variations": [
        "$ARG1 , \u201d though these names apply even when the latent variables are discrete and $ARG2"
    ],
    "calculus of variations*****technique": [
        "$ARG1 is a powerful $ARG2"
    ],
    "technique*****human": [
        "$ARG1 that removes much of the responsibility from the $ARG2"
    ],
    "human*****model": [
        "$ARG1 designer of the $ARG2"
    ],
    "inference*****probability": [
        "$ARG1 procedure encourages q to have low $ARG2",
        "$ARG1 network on values of v that have high $ARG2"
    ],
    "principle*****optimization algorithm": [
        "$ARG1 the selection of q could be done with any $ARG2"
    ],
    "optimization*****loop": [
        "$ARG1 must occur in the inner $ARG2"
    ],
    "mean \ufb01eld*****model": [
        "$ARG1 applied to the $ARG2",
        "$ARG1 approximation is forced to choose one mode to $ARG2",
        "$ARG1 approximation for any probabilistic $ARG2"
    ],
    "derivation*****ambiguity": [
        "$ARG1 goes into considerable mathematical detail and is intended for the reader who wishes to fully resolve any $ARG2"
    ],
    "ambiguity*****inference": [
        "$ARG1 in the high-level conceptual description of variational $ARG2"
    ],
    "plan*****learning": [
        "$ARG1 to derive or implement variational $ARG2"
    ],
    "example*****list": [
        "$ARG1 are encouraged to review the $ARG2"
    ],
    "\u03c3*****set": [
        "$ARG1 ( bi ) p ( v | h ) = N ( v ; W h , \u03b2 \u22121 ) where b is a learnable $ARG2"
    ],
    "maximum likelihood*****derivative": [
        "$ARG1 requires taking the $ARG2"
    ],
    "approximate inference*****graph": [
        "$ARG1 Figure 19.2 : The $ARG2"
    ],
    "graph*****sparse coding": [
        "$ARG1 structure of a binary $ARG2"
    ],
    "model*****bernoulli distribution": [
        "$ARG1 m $ARG2"
    ],
    "bernoulli distribution*****vector": [
        "$ARG1 is with a $ARG2"
    ],
    "sparse coding*****vector": [
        "$ARG1 using an unrestricted $ARG2"
    ],
    "relation*****\u03c3": [
        "$ARG1 h\u0302 = $ARG2"
    ],
    "identity*****\u03c3": [
        "$ARG1 log $ARG2"
    ],
    "sigmoid*****softplus": [
        "$ARG1 and the $ARG2"
    ],
    "derivation*****learning": [
        "$ARG1 of variational $ARG2"
    ],
    "learning*****sparse coding": [
        "$ARG1 in the binary $ARG2"
    ],
    "mean \ufb01eld*****learning": [
        "$ARG1 approximation makes $ARG2"
    ],
    "evidence lower bound*****\u03c3": [
        "$ARG1 is given by L ( v , \u03b8 , q ) =Eh\u223cq [ log p ( h , v ) ] + H ( q ) =Eh\u223cq [ log p ( h ) + log p ( v | h ) \u2212 log q ( h | v ) ] log p ( hi ) + log p ( vi | h ) \u2212 log q ( h i | v ) h\u0302 i ( log $ARG2"
    ],
    "principle*****gradient": [
        "$ARG1 , we could simply run $ARG2"
    ],
    "gradient*****inference": [
        "$ARG1 ascent on both v and h and this would make a perfectly acceptable combined $ARG2"
    ],
    "approximate inference*****content": [
        "$ARG1 Second , we would like to be able to extract the features h\u0302 very quickly , in order to recognize the $ARG2"
    ],
    "gradient descent*****mean \ufb01eld": [
        "$ARG1 to compute the $ARG2"
    ],
    "convergence*****change": [
        "$ARG1 criteria include stopping when a full cycle of updates does not improve L by more than some tolerance amount , or when the cycle does not $ARG2"
    ],
    "change*****mean \ufb01eld": [
        "$ARG1 h\u0302 by more than Iterating $ARG2"
    ],
    "mean \ufb01eld*****technique": [
        "$ARG1 \ufb01xed point equations is a general $ARG2"
    ],
    "technique*****inference": [
        "$ARG1 that can provide fast variational $ARG2"
    ],
    "approximate inference*****1": [
        "$ARG1 =bi \u2212 log h\u0302i + log ( $ARG2"
    ],
    "rule*****\u03c3": [
        "$ARG1 , we solve for the h\u0302 i that sets equation 19.43 to 0 : h\u0302i = $ARG2"
    ],
    "connection*****recurrent neural network": [
        "$ARG1 between $ARG2"
    ],
    "recurrent neural network*****inference": [
        "$ARG1 and $ARG2"
    ],
    "mean \ufb01eld*****recurrent neural network": [
        "$ARG1 \ufb01xed point equations de\ufb01ned a $ARG2"
    ],
    "task*****inference": [
        "$ARG1 of this network is to perform $ARG2"
    ],
    "sparse coding*****recurrent network": [
        "$ARG1 , we can see that the $ARG2"
    ],
    "mean \ufb01eld*****explaining away": [
        "$ARG1 approximation \u2019 s attempt to capture the $ARG2"
    ],
    "explaining away*****sparse coding": [
        "$ARG1 interactions in the binary $ARG2"
    ],
    "explaining away*****other": [
        "$ARG1 e\ufb00ect actually should cause a multi-modal posterior , so that if we draw samples from the posterior , some samples will have one unit active , $ARG2"
    ],
    "explaining away*****mean \ufb01eld": [
        "$ARG1 interactions can not be modeled by the factorial q used for $ARG2"
    ],
    "example*****rule": [
        "$ARG1 , we have derived an update $ARG2",
        "$ARG1 , the simple $ARG2"
    ],
    "learning*****calculus of variations": [
        "$ARG1 : $ARG2",
        "$ARG1 by maximizing L. However , we must now use $ARG2"
    ],
    "probability density function*****random variable": [
        "$ARG1 over some $ARG2",
        "$ARG1 When working with continuous $ARG2"
    ],
    "partial derivative*****function": [
        "$ARG1 of a $ARG2",
        "$ARG1 of a $ARG2",
        "$ARG1 of g , or we can apply the test to a new $ARG2"
    ],
    "function*****functional derivatives": [
        "$ARG1 with respect to elements of its vectorvalued argument , we can take $ARG2"
    ],
    "functional derivatives*****variational derivatives": [
        "$ARG1 , also known as $ARG2"
    ],
    "variational derivatives*****function": [
        "$ARG1 , of a functional J [ f ] with respect to individual values of the $ARG2"
    ],
    "approximate inference*****identity": [
        "$ARG1 To gain some intuition for this $ARG2"
    ],
    "identity*****vector": [
        "$ARG1 , one can think of f ( x ) as being a $ARG2"
    ],
    "view*****identity": [
        "$ARG1 ) , the $ARG2"
    ],
    "identity*****functional derivatives": [
        "$ARG1 providing the $ARG2"
    ],
    "functional derivatives*****vector": [
        "$ARG1 is the same as we would obtain for a $ARG2"
    ],
    "element*****gradient": [
        "$ARG1 of the $ARG2",
        "$ARG1 i of the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "process*****problem": [
        "$ARG1 works , consider the $ARG2"
    ],
    "function*****di\ufb00erential entropy": [
        "$ARG1 over x \u2208 R that has maximal $ARG2"
    ],
    "recall*****probability distribution": [
        "$ARG1 that the entropy of a $ARG2"
    ],
    "function*****probability distribution": [
        "$ARG1 p ( x ) , because the result might not be a $ARG2",
        "$ARG1 placing exactly zero mass on all but two points does not integrate to one , and is not a valid $ARG2",
        "$ARG1 f ( x ) with respect to a $ARG2",
        "$ARG1 to represent a $ARG2"
    ],
    "lagrange multipliers*****constraint": [
        "$ARG1 to add a $ARG2"
    ],
    "solution*****constraint": [
        "$ARG1 , we add a $ARG2",
        "$ARG1 if that $ARG2",
        "$ARG1 where the $ARG2"
    ],
    "constraint*****distribution": [
        "$ARG1 that the mean of the $ARG2"
    ],
    "lagrangian*****optimization": [
        "$ARG1 functional for this $ARG2"
    ],
    "approximate inference*****\u03c3": [
        "$ARG1 \u03bb1 p ( x ) + \u03bb2 p ( x ) x + \u03bb3 p ( x ) ( x \u2212 \u00b5 ) 2 \u2212 p ( x ) log p ( x ) dx \u2212 \u03bb1 \u2212 \u00b5\u03bb 2 \u2212 $ARG2"
    ],
    "lagrangian*****set": [
        "$ARG1 with respect to p , we $ARG2"
    ],
    "set*****functional derivatives": [
        "$ARG1 the $ARG2"
    ],
    "functional derivatives*****1": [
        "$ARG1 equal to 0 : L = \u03bb1 + \u03bb2 x + \u03bb3 ( x \u2212 \u00b5 ) 2 \u2212 $ARG2"
    ],
    "gradient*****lagrangian": [
        "$ARG1 of the $ARG2"
    ],
    "normal distribution*****assumption": [
        "$ARG1 has the maximum entropy , we impose the least possible amount of structure by making this $ARG2"
    ],
    "lagrangian*****variance": [
        "$ARG1 functional for the entropy , we found only one critical point , corresponding to maximizing the entropy for \ufb01xed $ARG2"
    ],
    "probability*****\u03c3": [
        "$ARG1 density on the two points x = \u00b5 + $ARG2"
    ],
    "\u03c3*****probability": [
        "$ARG1 , and place less $ARG2"
    ],
    "method*****functional derivatives": [
        "$ARG1 of solving for a speci\ufb01c point where the $ARG2"
    ],
    "contains*****inference": [
        "$ARG1 continuous latent variables , we may still perform variational $ARG2"
    ],
    "mean \ufb01eld*****distribution": [
        "$ARG1 approximation q ( h | v ) = q ( hi | v ) , and \ufb01x q ( hj | v ) for all j \ue036 = i , then the optimal q ( h i | v ) may be obtained by normalizing the unnormalized $ARG2"
    ],
    "learning*****mean \ufb01eld": [
        "$ARG1 ; equation 19.56 yields the $ARG2"
    ],
    "approximate inference*****calculus of variations": [
        "$ARG1 interesting ; we have constructed it only to provide a simple demonstration of how $ARG2"
    ],
    "diagonal*****technique": [
        "$ARG1 \u03b2 are variational parameters that we can optimize using any $ARG2"
    ],
    "recall*****calculus of variations": [
        "$ARG1 that we did not ever assume that q would be Gaussian ; its Gaussian form was derived automatically by using $ARG2"
    ],
    "turn*****accuracy": [
        "$ARG1 a\ufb00ects the $ARG2"
    ],
    "accuracy*****inference": [
        "$ARG1 of the $ARG2"
    ],
    "inference*****optimization": [
        "$ARG1 can be thought of as an $ARG2"
    ],
    "function*****optimization": [
        "$ARG1 L. Explicitly performing $ARG2"
    ],
    "learning*****approximate inference": [
        "$ARG1 to perform $ARG2"
    ],
    "algorithm*****problem": [
        "$ARG1 ( Hinton et al. , 1995b ; Frey et al. , 1996 ) resolves this $ARG2"
    ],
    "model*****ancestral sampling": [
        "$ARG1 , this can be done cheaply by performing $ARG2"
    ],
    "ancestral sampling*****inference": [
        "$ARG1 beginning at h and ending at v. The $ARG2"
    ],
    "inference*****mapping": [
        "$ARG1 network can then be trained to perform the reverse $ARG2"
    ],
    "mapping*****inference": [
        "$ARG1 : predicting which h caused the present v. The main drawback to this approach is that we will only be able to train the $ARG2"
    ],
    "inference*****dream sleep": [
        "$ARG1 network will not have an opportunity to learn on samples that resemble In section 18.2 we saw that one possible explanation for the role of $ARG2"
    ],
    "dream sleep*****human": [
        "$ARG1 in $ARG2"
    ],
    "human*****negative phase": [
        "$ARG1 beings and animals is that dreams could provide the $ARG2"
    ],
    "positive phase*****gradient": [
        "$ARG1 of the $ARG2"
    ],
    "gradient*****negative phase": [
        "$ARG1 for several steps then with only the $ARG2"
    ],
    "approximate inference*****support": [
        "$ARG1 not readily apparent how this schedule could $ARG2"
    ],
    "support*****undirected model": [
        "$ARG1 Monte Carlo training of an $ARG2"
    ],
    "reinforcement learning*****model": [
        "$ARG1 rather than probabilistic modeling , by sampling synthetic experiences from the animal \u2019 s transition $ARG2"
    ],
    "model*****policy": [
        "$ARG1 , on which to train the animal \u2019 s $ARG2"
    ],
    "pass*****inference": [
        "$ARG1 in a learned $ARG2"
    ],
    "inference*****predictive sparse decomposition": [
        "$ARG1 network to output this re\ufb01ned estimate instead of its We have already seen in section 14.8 that the $ARG2"
    ],
    "encoder*****mean \ufb01eld": [
        "$ARG1 , PSD is not able to implement the kind of competition between units that we have seen in $ARG2"
    ],
    "problem*****encoder": [
        "$ARG1 can be remedied by training a deep $ARG2"
    ],
    "encoder*****approximate inference": [
        "$ARG1 to perform learned $ARG2"
    ],
    "approximate inference*****technique": [
        "$ARG1 , as in the ISTA $ARG2"
    ],
    "approximate inference*****variational autoencoder": [
        "$ARG1 has recently become one of the dominant approaches to generative modeling , in the form of the $ARG2"
    ],
    "linear algebra*****mathematics": [
        "$ARG1 is a branch of $ARG2",
        "$ARG1 is a form of continuous rather than discrete $ARG2"
    ],
    "linear algebra*****machine learning": [
        "$ARG1 is essential for understanding and working with many $ARG2"
    ],
    "linear algebra*****deep learning": [
        "$ARG1 topics that are not essential for understanding $ARG2",
        "$ARG1 is one of the fundamental mathematical disciplines that is necessary to understand $ARG2"
    ],
    "linear algebra*****scalar": [
        "$ARG1 involves several types of mathematical objects : \u2022 Scalars : A $ARG2"
    ],
    "number*****contrast": [
        "$ARG1 , in $ARG2"
    ],
    "contrast*****other": [
        "$ARG1 to most of the $ARG2"
    ],
    "other*****linear algebra": [
        "$ARG1 objects studied in $ARG2"
    ],
    "linear algebra*****multiple": [
        "$ARG1 , which are usually arrays of $ARG2"
    ],
    "example*****slope": [
        "$ARG1 , we might say \u201c Let s \u2208 R be the $ARG2"
    ],
    "slope*****scalar": [
        "$ARG1 of the line , \u201d while de\ufb01ning a real-valued $ARG2"
    ],
    "set*****cartesian product": [
        "$ARG1 formed by taking the $ARG2"
    ],
    "set*****vector": [
        "$ARG1 of elements of a $ARG2",
        "$ARG1 of vectors is linearly independent if no $ARG2",
        "$ARG1 , the new $ARG2",
        "$ARG1 the initial parameter $ARG2"
    ],
    "matrix*****element": [
        "$ARG1 is a 2-D array of numbers , so each $ARG2",
        "$ARG1 is sparse and each $ARG2"
    ],
    "linear algebra*****transpose": [
        "$ARG1 A = \uf8f0 A2,1 A2,2 \uf8fb \u21d2 A \ue021 = Figure 2.1 : The $ARG2"
    ],
    "matrix*****image": [
        "$ARG1 can be thought of as a mirror $ARG2",
        "$ARG1 is the mirror $ARG2"
    ],
    "element*****matrix": [
        "$ARG1 ( i , j ) of the $ARG2",
        "$ARG1 of a $ARG2",
        "$ARG1 of the weight $ARG2",
        "$ARG1 of the weight $ARG2"
    ],
    "number*****tensor": [
        "$ARG1 of axes is known as a $ARG2"
    ],
    "operation*****transpose": [
        "$ARG1 on matrices is the $ARG2"
    ],
    "image*****matrix": [
        "$ARG1 of the $ARG2"
    ],
    "diagonal*****main diagonal": [
        "$ARG1 line , called the $ARG2",
        "$ARG1 matrices consist mostly of zeros and have non-zero entries only along the $ARG2"
    ],
    "transpose*****vector": [
        "$ARG1 of a $ARG2"
    ],
    "matrix*****row": [
        "$ARG1 with only one $ARG2",
        "$ARG1 with b copied into each $ARG2",
        "$ARG1 is constrained to be equal to the $ARG2"
    ],
    "linear algebra*****vector": [
        "$ARG1 de\ufb01ne a $ARG2"
    ],
    "vector*****row": [
        "$ARG1 by writing out its elements in the text inline as a $ARG2",
        "$ARG1 b is added to each $ARG2"
    ],
    "matrix*****transpose": [
        "$ARG1 , then using the $ARG2",
        "$ARG1 that is equal to its own $ARG2",
        "$ARG1 of g to be the $ARG2"
    ],
    "transpose*****turn": [
        "$ARG1 operator to $ARG2"
    ],
    "turn*****standard": [
        "$ARG1 it into a $ARG2"
    ],
    "standard*****vector": [
        "$ARG1 column $ARG2"
    ],
    "scalar*****matrix": [
        "$ARG1 can be thought of as a $ARG2",
        "$ARG1 to a $ARG2"
    ],
    "scalar*****transpose": [
        "$ARG1 is its own $ARG2",
        "$ARG1 and therefore equal to its own $ARG2",
        "$ARG1 g ( c ) \ue03e x is equal to the $ARG2",
        "$ARG1 is its own $ARG2"
    ],
    "matrix*****scalar": [
        "$ARG1 by a $ARG2"
    ],
    "scalar*****operation": [
        "$ARG1 , just by performing that $ARG2"
    ],
    "operation*****element": [
        "$ARG1 on each $ARG2"
    ],
    "matrix product*****matrix": [
        "$ARG1 of matrices A and B is a third $ARG2"
    ],
    "standard*****matrix": [
        "$ARG1 product of two matrices is not just a $ARG2"
    ],
    "operation*****element-wise product": [
        "$ARG1 exists and is called the $ARG2"
    ],
    "element-wise product*****hadamard product": [
        "$ARG1 or $ARG2"
    ],
    "dot product*****matrix product": [
        "$ARG1 between two vectors x and y of the same dimensionality is the $ARG2"
    ],
    "matrix product*****dot product": [
        "$ARG1 C = AB as computing Ci , j as the $ARG2"
    ],
    "dot product*****row": [
        "$ARG1 between $ARG2"
    ],
    "matrix multiplication*****scalar": [
        "$ARG1 is not commutative ( the condition AB = BA does not always hold ) , unlike $ARG2"
    ],
    "transpose*****matrix product": [
        "$ARG1 of a $ARG2"
    ],
    "transpose*****linear algebra": [
        "$ARG1 : x\ue03e y = x \ue03ey Since the focus of this textbook is not $ARG2"
    ],
    "linear algebra*****list": [
        "$ARG1 , we do not attempt to develop a comprehensive $ARG2"
    ],
    "list*****matrix product": [
        "$ARG1 of useful properties of the $ARG2"
    ],
    "linear algebra*****system": [
        "$ARG1 notation to write down a $ARG2"
    ],
    "system*****matrix": [
        "$ARG1 of linear where A \u2208 Rm\u00d7n is a known $ARG2"
    ],
    "row*****element": [
        "$ARG1 of A and each $ARG2",
        "$ARG1 above shifted by one $ARG2"
    ],
    "element*****constraint": [
        "$ARG1 of b provide another $ARG2"
    ],
    "linear algebra*****1": [
        "$ARG1 \uf8f00 $ARG2",
        "$ARG1 = arg max Tr ( X \ue03eXdd \ue03e ) subject to d\ue03e d = $ARG2",
        "$ARG1 notation , we index into arrays using a $ARG2"
    ],
    "identity*****linear algebra": [
        "$ARG1 and Inverse Matrices $ARG2"
    ],
    "linear algebra*****matrix": [
        "$ARG1 o\ufb00ers a powerful tool called $ARG2"
    ],
    "matrix*****concept": [
        "$ARG1 inversion , we \ufb01rst need to de\ufb01ne the $ARG2"
    ],
    "concept*****identity matrix": [
        "$ARG1 of an $ARG2"
    ],
    "identity matrix*****matrix": [
        "$ARG1 is a $ARG2"
    ],
    "matrix*****change": [
        "$ARG1 that does not $ARG2"
    ],
    "change*****vector": [
        "$ARG1 any $ARG2"
    ],
    "identity matrix*****main diagonal": [
        "$ARG1 is simple : all of the entries along the $ARG2"
    ],
    "main diagonal*****1": [
        "$ARG1 are $ARG2"
    ],
    "matrix inverse*****matrix": [
        "$ARG1 of A is denoted as A \u22121 , and it is de\ufb01ned as the $ARG2"
    ],
    "dependence*****solution": [
        "$ARG1 and Span In order for A\u22121 to exist , equation 2.11 must have exactly one $ARG2"
    ],
    "1*****solution": [
        "$ARG1 \u2212 \u03b1 ) y is also a $ARG2"
    ],
    "view*****element": [
        "$ARG1 , each $ARG2"
    ],
    "operation*****linear combination": [
        "$ARG1 is called a $ARG2"
    ],
    "linear combination*****set": [
        "$ARG1 of some $ARG2"
    ],
    "set*****linear combination": [
        "$ARG1 of all points obtainable by $ARG2",
        "$ARG1 is a $ARG2",
        "$ARG1 that is a $ARG2"
    ],
    "linear algebra*****solution": [
        "$ARG1 Determining whether Ax = b has a $ARG2"
    ],
    "potential*****solution": [
        "$ARG1 value of b that has no $ARG2"
    ],
    "linear combination*****other": [
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "matrix*****solution": [
        "$ARG1 to have an inverse , we additionally need to ensure that equation 2.11 has at most one $ARG2"
    ],
    "method*****matrix": [
        "$ARG1 of $ARG2"
    ],
    "linear algebra*****matrix inverse": [
        "$ARG1 So far we have discussed $ARG2"
    ],
    "measure*****vector": [
        "$ARG1 the size of a $ARG2",
        "$ARG1 the size of a $ARG2",
        "$ARG1 the size of the $ARG2"
    ],
    "measure*****function": [
        "$ARG1 the size of vectors using a $ARG2",
        "$ARG1 of how much the values of a $ARG2"
    ],
    "function*****norm": [
        "$ARG1 called a $ARG2",
        "$ARG1 as the \u201c L 0 $ARG2",
        "$ARG1 being minimized simpli\ufb01es to ( x \u2212 g ( c ) ) \ue03e ( x \u2212 g ( c ) ) ( by the de\ufb01nition of the L 2 $ARG2"
    ],
    "lp norm*****1": [
        "$ARG1 for p \u2208 R , p \u2265 $ARG2"
    ],
    "lp norm*****mapping": [
        "$ARG1 , are functions $ARG2"
    ],
    "function*****triangle inequality": [
        "$ARG1 f that satis\ufb01es the following properties : \u2022 f ( x ) = 0 \u21d2 x = 0 \u2022 f ( x + y ) \u2264 f ( x ) + f ( y ) ( the $ARG2"
    ],
    "triangle inequality*****norm": [
        "$ARG1 ) \u2022 \u2200\u03b1 \u2208 R , f ( \u03b1x ) = |\u03b1|f ( x ) The L 2 $ARG2"
    ],
    "norm*****euclidean norm": [
        "$ARG1 , with p = 2 , is known as the $ARG2"
    ],
    "norm*****machine learning": [
        "$ARG1 is used so frequently in $ARG2",
        "$ARG1 is commonly used in $ARG2",
        "$ARG1 that commonly arises in $ARG2"
    ],
    "vector*****norm": [
        "$ARG1 using the squared L 2 $ARG2",
        "$ARG1 is not a $ARG2"
    ],
    "example*****norm": [
        "$ARG1 , the derivatives of the squared L2 $ARG2"
    ],
    "norm*****element": [
        "$ARG1 with respect to each $ARG2"
    ],
    "element*****norm": [
        "$ARG1 of x , while all of the derivatives of the L2 $ARG2"
    ],
    "function*****rate": [
        "$ARG1 that grows at the same $ARG2",
        "$ARG1 f whose $ARG2"
    ],
    "rate*****norm": [
        "$ARG1 in all locations , but retains mathematical simplicity : the L1 $ARG2"
    ],
    "norm*****1": [
        "$ARG1 may be simpli\ufb01ed to The L $ARG2",
        "$ARG1 of J x remains less than or equal to $ARG2",
        "$ARG1 of x exceeds $ARG2"
    ],
    "vector*****number": [
        "$ARG1 by counting its $ARG2"
    ],
    "vector*****change": [
        "$ARG1 by \u03b1 does not $ARG2"
    ],
    "change*****number": [
        "$ARG1 the $ARG2"
    ],
    "norm*****number": [
        "$ARG1 is often used as a substitute for the $ARG2"
    ],
    "machine learning*****norm": [
        "$ARG1 is the L \u221e $ARG2"
    ],
    "norm*****max norm": [
        "$ARG1 , also known as the $ARG2"
    ],
    "norm*****absolute value": [
        "$ARG1 simpli\ufb01es to the $ARG2"
    ],
    "absolute value*****element": [
        "$ARG1 of the $ARG2"
    ],
    "element*****magnitude": [
        "$ARG1 with the largest $ARG2"
    ],
    "magnitude*****vector": [
        "$ARG1 in the $ARG2"
    ],
    "measure*****matrix": [
        "$ARG1 the size of a $ARG2"
    ],
    "deep learning*****norm": [
        "$ARG1 , the most common way to do this is with the otherwise obscure A 2i , j , which is analogous to the L 2 $ARG2",
        "$ARG1 We have already seen , in section 5.2.2 , one of the simplest and most common kinds of parameter $ARG2",
        "$ARG1 limited L1 $ARG2"
    ],
    "example*****diagonal matrix": [
        "$ARG1 of a $ARG2"
    ],
    "diagonal matrix*****identity matrix": [
        "$ARG1 : the $ARG2"
    ],
    "identity matrix*****diagonal": [
        "$ARG1 , where all of the $ARG2"
    ],
    "diagonal*****1": [
        "$ARG1 entries are $ARG2",
        "$ARG1 entry is nonzero , and in that case , diag ( v ) \u22121 = diag ( [ 1/v $ARG2"
    ],
    "diagonal matrix*****diagonal": [
        "$ARG1 whose $ARG2"
    ],
    "diagonal*****vector": [
        "$ARG1 entries are given by the entries of the $ARG2"
    ],
    "diagonal*****diagonal matrix": [
        "$ARG1 matrices are of interest in part because multiplying by a $ARG2"
    ],
    "other*****diagonal matrix": [
        "$ARG1 words , diag ( v ) x = v \ue00c x. Inverting a square $ARG2"
    ],
    "algorithm*****diagonal": [
        "$ARG1 by restricting some matrices to be $ARG2"
    ],
    "diagonal matrix*****element": [
        "$ARG1 D , the product Dx will involve scaling each $ARG2"
    ],
    "symmetric matrix*****matrix": [
        "$ARG1 is any $ARG2"
    ],
    "matrix*****distance": [
        "$ARG1 of $ARG2",
        "$ARG1 D. To do so , we revisit the idea of minimizing the L 2 $ARG2"
    ],
    "unit vector*****vector": [
        "$ARG1 is a $ARG2"
    ],
    "vector*****unit norm": [
        "$ARG1 with $ARG2"
    ],
    "unit norm*****1": [
        "$ARG1 : ||x||2 = $ARG2"
    ],
    "norm*****other": [
        "$ARG1 , this means that they are at a 90 degree angle to each $ARG2"
    ],
    "unit norm*****orthogonal matrix": [
        "$ARG1 , we call them An $ARG2"
    ],
    "orthogonal matrix*****square matrix": [
        "$ARG1 is a $ARG2"
    ],
    "number*****change": [
        "$ARG1 12 will $ARG2"
    ],
    "change*****base": [
        "$ARG1 depending on whether we write it in $ARG2"
    ],
    "information*****matrix": [
        "$ARG1 about their functional properties that is not obvious from the representation of the $ARG2"
    ],
    "matrix decomposition*****eigendecomposition": [
        "$ARG1 is called $ARG2"
    ],
    "eigendecomposition*****matrix": [
        "$ARG1 , in which we decompose a $ARG2",
        "$ARG1 of a $ARG2",
        "$ARG1 involves analyzing a $ARG2"
    ],
    "set*****eigenvector": [
        "$ARG1 of eigenvectors and An $ARG2"
    ],
    "eigenvector*****square matrix": [
        "$ARG1 of a $ARG2"
    ],
    "square matrix*****vector": [
        "$ARG1 A is a non-zero $ARG2"
    ],
    "vector*****multiplication": [
        "$ARG1 v such that $ARG2"
    ],
    "scalar*****eigenvalue": [
        "$ARG1 \u03bb is known as the $ARG2"
    ],
    "eigenvalue*****eigenvector": [
        "$ARG1 corresponding to this $ARG2",
        "$ARG1 \u039bi , i is associated with the $ARG2",
        "$ARG1 of the Hessian corresponding to the $ARG2"
    ],
    "eigenvector*****vector": [
        "$ARG1 of A , then so is any rescaled $ARG2"
    ],
    "linear algebra*****example": [
        "$ARG1 Figure 2.3 : An $ARG2"
    ],
    "example*****eigenvectors and eigenvalues": [
        "$ARG1 of the e\ufb00ect of $ARG2"
    ],
    "1*****eigenvalue": [
        "$ARG1 ) with $ARG2"
    ],
    "set*****unit vector": [
        "$ARG1 of all $ARG2"
    ],
    "eigenvector*****1": [
        "$ARG1 per column : V = [ v ( $ARG2"
    ],
    "symmetric matrix*****orthogonal matrix": [
        "$ARG1 can be decomposed into an expression using only real-valued eigenvectors A = Q\u039bQ \ue03e , where Q is an $ARG2"
    ],
    "orthogonal matrix*****diagonal matrix": [
        "$ARG1 composed of eigenvectors of A , and \u039b is a $ARG2"
    ],
    "symmetric matrix*****eigendecomposition": [
        "$ARG1 A is guaranteed to have an $ARG2"
    ],
    "eigenvalue*****set": [
        "$ARG1 , then any $ARG2"
    ],
    "set*****eigenvalue": [
        "$ARG1 of orthogonal vectors lying in their span are also eigenvectors with that $ARG2"
    ],
    "convention*****eigendecomposition": [
        "$ARG1 , the $ARG2"
    ],
    "eigendecomposition*****symmetric matrix": [
        "$ARG1 of a real $ARG2"
    ],
    "symmetric matrix*****1": [
        "$ARG1 can also be used to optimize quadratic expressions of the form f ( x ) = x\ue03e Ax subject to ||x||2 = $ARG2"
    ],
    "constraint*****eigenvalue": [
        "$ARG1 region is the maximum $ARG2",
        "$ARG1 region is the minimum $ARG2"
    ],
    "eigenvalue*****constraint": [
        "$ARG1 and its minimum value within the $ARG2"
    ],
    "matrix*****negative de\ufb01nite": [
        "$ARG1 is $ARG2"
    ],
    "matrix*****eigenvectors and eigenvalues": [
        "$ARG1 into $ARG2"
    ],
    "matrix*****singular vector": [
        "$ARG1 , into $ARG2"
    ],
    "singular vector*****singular value": [
        "$ARG1 and $ARG2"
    ],
    "information*****eigendecomposition": [
        "$ARG1 as the $ARG2"
    ],
    "singular value decomposition*****eigenvalue": [
        "$ARG1 , but the same is not true of the $ARG2"
    ],
    "matrix*****eigendecomposition": [
        "$ARG1 is not square , the $ARG2"
    ],
    "eigendecomposition*****singular value decomposition": [
        "$ARG1 is not de\ufb01ned , and we must use a $ARG2"
    ],
    "recall*****eigendecomposition": [
        "$ARG1 that the $ARG2"
    ],
    "diagonal*****singular value": [
        "$ARG1 of D are known as the $ARG2"
    ],
    "singular value*****matrix": [
        "$ARG1 of the $ARG2"
    ],
    "singular value decomposition*****eigendecomposition": [
        "$ARG1 of A in terms of the $ARG2"
    ],
    "feature*****matrix": [
        "$ARG1 of the SVD is that we can use it to partially generalize $ARG2"
    ],
    "problem*****mapping": [
        "$ARG1 , it may not be possible to design a unique $ARG2"
    ],
    "multiple*****moore-penrose pseudoinverse": [
        "$ARG1 possible The $ARG2"
    ],
    "singular value decomposition*****diagonal matrix": [
        "$ARG1 of A , and the pseudoinverse D + of a $ARG2"
    ],
    "diagonal matrix*****transpose": [
        "$ARG1 D is obtained by taking the reciprocal of its non-zero elements then taking the $ARG2"
    ],
    "solution*****euclidean norm": [
        "$ARG1 x = A+ y with minimal $ARG2"
    ],
    "euclidean norm*****solution": [
        "$ARG1 ||x||2 among all possible When A has more rows than columns , it is possible for there to be no $ARG2"
    ],
    "trace operator*****diagonal": [
        "$ARG1 gives the sum of all of the $ARG2"
    ],
    "diagonal*****matrix": [
        "$ARG1 entries of a $ARG2",
        "$ARG1 entries of this $ARG2"
    ],
    "matrix product*****trace operator": [
        "$ARG1 and the $ARG2"
    ],
    "example*****trace operator": [
        "$ARG1 , the $ARG2",
        "$ARG1 , the $ARG2"
    ],
    "trace operator*****frobenius norm": [
        "$ARG1 provides an alternative way of writing the $ARG2"
    ],
    "frobenius norm*****matrix": [
        "$ARG1 of a $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the entire weight $ARG2"
    ],
    "trace operator*****invariant": [
        "$ARG1 is $ARG2"
    ],
    "invariant*****transpose": [
        "$ARG1 to the $ARG2"
    ],
    "trace*****square matrix": [
        "$ARG1 of a $ARG2"
    ],
    "square matrix*****invariant": [
        "$ARG1 composed of many factors is also $ARG2"
    ],
    "scalar*****trace": [
        "$ARG1 is its own $ARG2"
    ],
    "determinant*****square matrix": [
        "$ARG1 of a $ARG2"
    ],
    "square matrix*****function": [
        "$ARG1 , denoted det ( A ) , is a $ARG2"
    ],
    "determinant*****matrix": [
        "$ARG1 is equal to the product of all the eigenvalues of the $ARG2"
    ],
    "absolute value*****determinant": [
        "$ARG1 of the $ARG2"
    ],
    "determinant*****measure": [
        "$ARG1 can be thought of as a $ARG2"
    ],
    "measure*****multiplication": [
        "$ARG1 of how much $ARG2"
    ],
    "determinant*****dimension": [
        "$ARG1 is 0 , then space is contracted completely along at least one $ARG2"
    ],
    "determinant*****1": [
        "$ARG1 is $ARG2"
    ],
    "example*****principal components analysis": [
        "$ARG1 : $ARG2"
    ],
    "principal components analysis*****machine learning": [
        "$ARG1 One simple $ARG2"
    ],
    "algorithm*****principal components analysis": [
        "$ARG1 , $ARG2"
    ],
    "principal components analysis*****pca": [
        "$ARG1 or $ARG2"
    ],
    "pca*****linear algebra": [
        "$ARG1 can be derived using only knowledge of basic $ARG2"
    ],
    "collection*****1": [
        "$ARG1 of m points { x ( $ARG2"
    ],
    "memory*****precision": [
        "$ARG1 but may lose some $ARG2"
    ],
    "pca*****function": [
        "$ARG1 is de\ufb01ned by our choice of the decoding $ARG2"
    ],
    "decoder*****matrix multiplication": [
        "$ARG1 very simple , we choose to use $ARG2"
    ],
    "decoder*****problem": [
        "$ARG1 could be a di\ufb03cult $ARG2"
    ],
    "problem*****pca": [
        "$ARG1 easy , $ARG2"
    ],
    "pca*****other": [
        "$ARG1 constrains the columns of D to be orthogonal to each $ARG2"
    ],
    "orthogonal matrix*****problem": [
        "$ARG1 \u201d unless l = n ) With the $ARG2"
    ],
    "solution*****turn": [
        "$ARG1 , we constrain all of the columns of D to have unit In order to $ARG2"
    ],
    "turn*****algorithm": [
        "$ARG1 this basic idea into an $ARG2"
    ],
    "measure*****distance": [
        "$ARG1 this $ARG2",
        "$ARG1 the $ARG2"
    ],
    "distance*****norm": [
        "$ARG1 using a $ARG2"
    ],
    "algorithm*****norm": [
        "$ARG1 , we use the L2 $ARG2"
    ],
    "norm*****operation": [
        "$ARG1 is non-negative and the squaring $ARG2"
    ],
    "norm*****property": [
        "$ARG1 , equation 2.30 ) = x\ue03e x \u2212 x\ue03e g ( c ) \u2212 g ( c ) \ue03e x + g ( c ) \ue03eg ( c ) ( by the distributive $ARG2"
    ],
    "property*****scalar": [
        "$ARG1 ) = x\ue03e x \u2212 2x\ue03eg ( c ) + g ( c ) \ue03eg ( c ) ( because the $ARG2"
    ],
    "change*****function": [
        "$ARG1 the $ARG2",
        "$ARG1 each input by \ue00f , then a linear $ARG2"
    ],
    "orthogonality*****unit norm": [
        "$ARG1 and $ARG2"
    ],
    "unit norm*****optimization problem": [
        "$ARG1 constraints on D ) = arg min \u22122x\ue03eDc + c \ue03e c We can solve this $ARG2"
    ],
    "optimization problem*****vector": [
        "$ARG1 using $ARG2"
    ],
    "linear algebra*****algorithm": [
        "$ARG1 This makes the $ARG2"
    ],
    "vector*****encoder": [
        "$ARG1 , we apply the $ARG2"
    ],
    "matrix multiplication*****pca": [
        "$ARG1 , we can also de\ufb01ne the $ARG2"
    ],
    "matrix*****algorithm": [
        "$ARG1 of errors computed over all dimensions and all points : \ue058 \ue010 ( i ) D = arg min xj \u2212 r ( x ( i ) ) j subject to D\ue03eD = Il To derive the $ARG2"
    ],
    "vector*****problem": [
        "$ARG1 , d. Substituting equation 2.67 into equation 2.68 and simplifying D into d , the $ARG2"
    ],
    "transpose*****1": [
        "$ARG1 , as ||x ( i ) \u2212 x ( i ) \ue03edd||22 subject to ||d|| 2 = $ARG2"
    ],
    "linear algebra*****problem": [
        "$ARG1 At this point , it can be helpful to rewrite the $ARG2",
        "$ARG1 algorithms that can solve this $ARG2"
    ],
    "problem*****design matrix": [
        "$ARG1 in terms of a single $ARG2"
    ],
    "constraint*****frobenius norm": [
        "$ARG1 for the moment , we can simplify the $ARG2"
    ],
    "frobenius norm*****trace": [
        "$ARG1 portion as follows : arg min ||X \u2212 Xdd\ue03e || 2F = arg min Tr ( by equation 2.49 ) = arg min Tr ( X \ue03eX \u2212 X \ue03eXdd\ue03e \u2212 dd \ue03e X \ue03e X + dd \ue03eX \ue03e Xdd \ue03e ) = arg min Tr ( X \ue03e X ) \u2212 Tr ( X \ue03eXdd\ue03e ) \u2212 Tr ( dd\ue03eX \ue03eX ) + Tr ( dd\ue03eX \ue03e Xdd\ue03e ) = arg min \u2212 Tr ( X \ue03e Xdd\ue03e ) \u2212 Tr ( dd\ue03eX \ue03e X ) + Tr ( dd\ue03e X \ue03eXdd \ue03e ) ( because terms not involving d do not a\ufb00ect the arg min ) = arg min \u22122 Tr ( X \ue03e Xdd\ue03e ) + Tr ( dd\ue03e X \ue03e Xdd\ue03e ) ( because we can cycle the order of the matrices inside a $ARG2"
    ],
    "trace*****property": [
        "$ARG1 , equation 2.52 ) = arg min \u22122 Tr ( X \ue03e Xdd\ue03e ) + Tr ( X \ue03e Xdd\ue03e dd\ue03e ) ( using the same $ARG2"
    ],
    "property*****constraint": [
        "$ARG1 again ) At this point , we re-introduce the $ARG2"
    ],
    "1*****constraint": [
        "$ARG1 ( due to the $ARG2"
    ],
    "optimization problem*****eigendecomposition": [
        "$ARG1 may be solved using $ARG2"
    ],
    "eigenvector*****derivation": [
        "$ARG1 of X \ue03eX corresponding to the largest This $ARG2"
    ],
    "derivation*****1": [
        "$ARG1 is speci\ufb01c to the case of l = $ARG2"
    ],
    "area of mathematics*****machine learning": [
        "$ARG1 that is ubiquitous in $ARG2"
    ],
    "machine learning*****probability theory": [
        "$ARG1 is $ARG2",
        "$ARG1 makes heavy use of $ARG2"
    ],
    "decoder*****stochastic": [
        "$ARG1 beyond deterministic functions to $ARG2"
    ],
    "dimensionality reduction*****feature learning": [
        "$ARG1 or $ARG2"
    ],
    "special case*****minibatch": [
        "$ARG1 of feedforward networks , and may be trained with all of the same techniques , typically $ARG2"
    ],
    "back-propagation*****machine learning": [
        "$ARG1 , but is rarely used for $ARG2"
    ],
    "mapping*****encoder": [
        "$ARG1 an input x to an output ( called reconstruction ) r through an internal representation or code h. The autoencoder has two components : the $ARG2",
        "$ARG1 , sometimes called an $ARG2"
    ],
    "encoder*****mapping": [
        "$ARG1 f ( $ARG2",
        "$ARG1 itself , can approximate any $ARG2",
        "$ARG1 learns a $ARG2"
    ],
    "decoder*****mapping": [
        "$ARG1 g ( $ARG2"
    ],
    "task*****dimension": [
        "$ARG1 will result in h taking on useful One way to obtain useful features from the autoencoder is to constrain h to have smaller $ARG2"
    ],
    "process*****loss function": [
        "$ARG1 is described simply as minimizing a $ARG2"
    ],
    "decoder*****mean squared error": [
        "$ARG1 is linear and L is the $ARG2"
    ],
    "mean squared error*****pca": [
        "$ARG1 , an undercomplete autoencoder learns to span the same subspace as $ARG2"
    ],
    "decoder*****generalization": [
        "$ARG1 functions g can thus learn a more powerful nonlinear $ARG2"
    ],
    "generalization*****pca": [
        "$ARG1 of $ARG2"
    ],
    "decoder*****task": [
        "$ARG1 are allowed too much capacity , the autoencoder can learn to perform the copying $ARG2"
    ],
    "task*****information": [
        "$ARG1 without extracting useful $ARG2",
        "$ARG1 relies on preserving precise spatial $ARG2",
        "$ARG1 involves incorporating $ARG2"
    ],
    "encoder*****example": [
        "$ARG1 could learn to represent each training $ARG2"
    ],
    "dimension*****distribution": [
        "$ARG1 , can learn the most salient features of the data $ARG2"
    ],
    "problem*****dimension": [
        "$ARG1 occurs if the hidden code is allowed to have $ARG2"
    ],
    "decoder*****learning": [
        "$ARG1 can learn to copy the input to the output without $ARG2",
        "$ARG1 from $ARG2"
    ],
    "dimension*****encoder": [
        "$ARG1 and the capacity of the $ARG2"
    ],
    "decoder*****distribution": [
        "$ARG1 based on the complexity of $ARG2",
        "$ARG1 as providing a conditional $ARG2",
        "$ARG1 are not simple functions but instead involve some noise injection , meaning that their output can be seen as sampled from a $ARG2",
        "$ARG1 distributions are not necessarily conditional distributions compatible with a unique joint $ARG2"
    ],
    "decoder*****loss function": [
        "$ARG1 shallow and the code size small , regularized autoencoders use a $ARG2"
    ],
    "other*****derivative": [
        "$ARG1 properties include sparsity of the representation , smallness of the $ARG2"
    ],
    "derivative*****missing inputs": [
        "$ARG1 of the representation , and robustness to noise or to $ARG2"
    ],
    "model*****identity": [
        "$ARG1 capacity is great enough to learn a trivial $ARG2"
    ],
    "decoder*****encoder": [
        "$ARG1 output and typically we have h = f ( x ) , the $ARG2",
        "$ARG1 In general , the $ARG2"
    ],
    "encoder*****task": [
        "$ARG1 Sparse autoencoders are typically used to learn features for another $ARG2"
    ],
    "dataset*****identity": [
        "$ARG1 it has been trained on , rather than simply acting as an $ARG2"
    ],
    "unsupervised learning*****task": [
        "$ARG1 objective ) and possibly also perform some supervised $ARG2",
        "$ARG1 in your \ufb01rst attempt if the $ARG2"
    ],
    "other*****weight decay": [
        "$ARG1 regularizers such as $ARG2"
    ],
    "weight decay*****interpretation": [
        "$ARG1 , there is not a straightforward Bayesian $ARG2"
    ],
    "weight decay*****other": [
        "$ARG1 and $ARG2",
        "$ARG1 , there are $ARG2"
    ],
    "regularization*****map approximation": [
        "$ARG1 penalties can be interpreted as a $ARG2"
    ],
    "inference*****prior probability distribution": [
        "$ARG1 , with the added regularizing penalty corresponding to a $ARG2"
    ],
    "prior probability distribution*****model": [
        "$ARG1 over the $ARG2"
    ],
    "thinking*****task": [
        "$ARG1 of the sparsity penalty as a regularizer for the copying $ARG2"
    ],
    "encoder*****optimization": [
        "$ARG1 rather than the result of an $ARG2"
    ],
    "example*****absolute value": [
        "$ARG1 , the Laplace prior , pmodel ( hi ) = corresponds to an $ARG2"
    ],
    "absolute value*****model": [
        "$ARG1 penalty , we obtain \u2126 ( h ) = \u03bb = \u2126 ( h ) + const \u2212 log p $ARG2"
    ],
    "model*****constant": [
        "$ARG1 ( h ) = \u03bb|h i | \u2212 log where the $ARG2"
    ],
    "constant*****hyperparameter": [
        "$ARG1 term depends only on \u03bb and not h. We typically treat \u03bb as a $ARG2"
    ],
    "hyperparameter*****constant": [
        "$ARG1 and discard the $ARG2"
    ],
    "constant*****learning": [
        "$ARG1 term since it does not a\ufb00ect the parameter $ARG2",
        "$ARG1 and thus have the \ufb02exibility to capture linear trends in the training data while still $ARG2"
    ],
    "learning*****regularization": [
        "$ARG1 , the sparsity penalty is not a $ARG2"
    ],
    "connection*****maximum likelihood": [
        "$ARG1 between the sparsity penalty and the log Z term that arises when applying $ARG2"
    ],
    "connection*****correspondence": [
        "$ARG1 is on the level of an intuitive understanding of a general mechanism rather than a mathematical $ARG2"
    ],
    "absolute value*****average": [
        "$ARG1 penalty ) , one can thus indirectly control the $ARG2"
    ],
    "loss function*****norm": [
        "$ARG1 penalizing g ( f ( x ) ) for being dissimilar from x , such as the L2 $ARG2",
        "$ARG1 J a $ARG2"
    ],
    "learning*****identity": [
        "$ARG1 the $ARG2",
        "$ARG1 a useless $ARG2"
    ],
    "information*****contractive autoencoder": [
        "$ARG1 about the training An autoencoder regularized in this way is called a $ARG2"
    ],
    "denoising autoencoder*****manifold learning": [
        "$ARG1 , $ARG2"
    ],
    "theorem*****feedforward neural network": [
        "$ARG1 guarantees that a $ARG2"
    ],
    "hidden layer*****function": [
        "$ARG1 can represent an approximation of any $ARG2"
    ],
    "hidden layer*****identity": [
        "$ARG1 is able to represent the $ARG2"
    ],
    "hidden layer*****encoder": [
        "$ARG1 inside the $ARG2"
    ],
    "loss function*****distribution": [
        "$ARG1 of a feedforward network is to de\ufb01ne an output $ARG2"
    ],
    "distribution*****decoder": [
        "$ARG1 p $ARG2"
    ],
    "loss function*****change": [
        "$ARG1 will $ARG2"
    ],
    "bernoulli distribution*****sigmoid": [
        "$ARG1 whose parameters are given by a $ARG2"
    ],
    "probability distribution*****mixture density": [
        "$ARG1 is inexpensive to evaluate , but some techniques such as $ARG2"
    ],
    "mixture density*****stochastic": [
        "$ARG1 outputs allow tractable modeling of outputs pencoder ( h | x ) pdecoder ( x | h ) Figure 14.2 : The structure of a $ARG2"
    ],
    "stochastic*****encoder": [
        "$ARG1 autoencoder , in which both the $ARG2"
    ],
    "departure*****function": [
        "$ARG1 from the feedforward networks we have seen previously , we can also generalize the notion of an encoding $ARG2"
    ],
    "decoder*****denoising autoencoder": [
        "$ARG1 as a $ARG2"
    ],
    "computational graph*****cost function": [
        "$ARG1 of the $ARG2"
    ],
    "cost function*****denoising autoencoder": [
        "$ARG1 for a $ARG2"
    ],
    "encoder*****denoising autoencoder": [
        "$ARG1 is deterministic , the $ARG2"
    ],
    "denoising autoencoder*****other": [
        "$ARG1 is a feedforward network and may be trained with exactly the same techniques as any $ARG2"
    ],
    "other*****view": [
        "$ARG1 We can therefore $ARG2"
    ],
    "view*****stochastic gradient descent": [
        "$ARG1 the DAE as performing $ARG2"
    ],
    "stochastic gradient descent*****expectation": [
        "$ARG1 on the following $ARG2"
    ],
    "denoising autoencoder*****average": [
        "$ARG1 is trained to minimize the $ARG2"
    ],
    "vector*****manifold": [
        "$ARG1 g ( f ( x\u0303 ) ) \u2212 x\u0303 points approximately towards the nearest point on the $ARG2",
        "$ARG1 , with less dimensions than the \u201c ambient \u201d space of which the $ARG2",
        "$ARG1 that is tangent to the class $ARG2",
        "$ARG1 that is normal to the class $ARG2"
    ],
    "score matching*****maximum likelihood": [
        "$ARG1 ( Hyv\u00e4rinen , 2005 ) is an alternative to $ARG2"
    ],
    "estimator*****probability distribution": [
        "$ARG1 of $ARG2"
    ],
    "property*****vector": [
        "$ARG1 of DAEs is that their training criterion ( with conditionally Gaussian p ( x | h ) ) makes the autoencoder learn a $ARG2"
    ],
    "gaussian noise*****mean squared error": [
        "$ARG1 and $ARG2"
    ],
    "denoising score matching*****learning": [
        "$ARG1 ( Kingma and LeCun , 2010 ) , its $ARG2"
    ],
    "score matching*****estimator": [
        "$ARG1 is not a consistent $ARG2"
    ],
    "number*****consistency": [
        "$ARG1 of examples approaches in\ufb01nity , then $ARG2"
    ],
    "score matching*****cost function": [
        "$ARG1 applied to RBMs yields a $ARG2"
    ],
    "gradient*****contrastive divergence": [
        "$ARG1 provides an approximation to $ARG2"
    ],
    "contrastive divergence*****distribution": [
        "$ARG1 training of For continuous-valued x , the denoising criterion with Gaussian corruption and reconstruction $ARG2"
    ],
    "distribution*****estimator": [
        "$ARG1 yields an $ARG2"
    ],
    "estimator*****encoder": [
        "$ARG1 of the score that is applicable to general $ARG2"
    ],
    "\u03c3*****variance": [
        "$ARG1 = \u03c32 I ) with noise $ARG2",
        "$ARG1 , and the $ARG2"
    ],
    "vector*****denoising autoencoder": [
        "$ARG1 \ufb01eld learned by a $ARG2"
    ],
    "denoising autoencoder*****manifold": [
        "$ARG1 around a 1-D curved $ARG2"
    ],
    "function*****manifolds": [
        "$ARG1 ( on the data $ARG2"
    ],
    "manifolds*****function": [
        "$ARG1 ) and at minima of that density $ARG2",
        "$ARG1 , where the reconstruction $ARG2"
    ],
    "example*****spiral": [
        "$ARG1 , the $ARG2"
    ],
    "spiral*****manifold": [
        "$ARG1 arm forms a one-dimensional $ARG2"
    ],
    "norm*****probability": [
        "$ARG1 of reconstruction error ( shown by the length of the arrows ) is large , it means that $ARG2"
    ],
    "denoising autoencoder*****probability distribution": [
        "$ARG1 learns to represent a $ARG2"
    ],
    "sparse coding*****contractive autoencoder": [
        "$ARG1 , $ARG2"
    ],
    "contractive autoencoder*****other": [
        "$ARG1 and $ARG2"
    ],
    "hidden layer*****generalization": [
        "$ARG1 of a supervised MLP , with the objective to improve $ARG2"
    ],
    "method*****encoder": [
        "$ARG1 was based on a linear $ARG2"
    ],
    "manifolds*****other": [
        "$ARG1 with Autoencoders Like many $ARG2"
    ],
    "manifold*****set": [
        "$ARG1 or a small $ARG2",
        "$ARG1 is the $ARG2"
    ],
    "set*****manifolds": [
        "$ARG1 of such $ARG2"
    ],
    "set*****tangent plane": [
        "$ARG1 of its $ARG2"
    ],
    "manifold*****tangent plane": [
        "$ARG1 , the $ARG2",
        "$ARG1 has an n-dimensional $ARG2"
    ],
    "change*****manifold": [
        "$ARG1 x in\ufb01nitesimally while staying on the $ARG2",
        "$ARG1 rapidly as it moves in the direction normal to the $ARG2",
        "$ARG1 as it moves along the class $ARG2",
        "$ARG1 very much as x moves along the $ARG2"
    ],
    "example*****decoder": [
        "$ARG1 x such that x can be approximately recovered from h through a $ARG2"
    ],
    "constraint*****regularization": [
        "$ARG1 or $ARG2",
        "$ARG1 that limits the capacity of the autoencoder , or it can be a $ARG2"
    ],
    "information*****data generating distribution": [
        "$ARG1 about the structure of the $ARG2"
    ],
    "data generating distribution*****manifold": [
        "$ARG1 concentrates near a low-dimensional $ARG2"
    ],
    "manifold*****system": [
        "$ARG1 , this yields representations that implicitly capture a local coordinate $ARG2"
    ],
    "system*****manifold": [
        "$ARG1 for this $ARG2"
    ],
    "mapping*****manifold": [
        "$ARG1 that is only sensitive to changes along the $ARG2"
    ],
    "manifold learning*****other": [
        "$ARG1 , it is instructive to compare them to $ARG2"
    ],
    "illustration*****concept": [
        "$ARG1 of the $ARG2"
    ],
    "translation*****manifold": [
        "$ARG1 de\ufb01nes a coordinate along a one-dimensional $ARG2"
    ],
    "manifold*****path": [
        "$ARG1 that traces out a curved $ARG2"
    ],
    "path*****image": [
        "$ARG1 through $ARG2"
    ],
    "manifold*****pca": [
        "$ARG1 into two dimensional space using $ARG2",
        "$ARG1 estimated by local $ARG2"
    ],
    "function*****invariant": [
        "$ARG1 that is $ARG2",
        "$ARG1 the layer learns must be $ARG2"
    ],
    "invariant*****manifold": [
        "$ARG1 to small perturbations near the data points , it captures the $ARG2"
    ],
    "manifold*****collection": [
        "$ARG1 structure is a $ARG2"
    ],
    "diagonal*****identity": [
        "$ARG1 line indicates the $ARG2"
    ],
    "function*****identity": [
        "$ARG1 crosses the $ARG2"
    ],
    "vector*****base": [
        "$ARG1 at the $ARG2"
    ],
    "base*****manifold": [
        "$ARG1 of the arrow , in input space , always pointing towards the nearest \u201c $ARG2"
    ],
    "denoising autoencoder*****derivative": [
        "$ARG1 explicitly tries to make the $ARG2"
    ],
    "contractive autoencoder*****encoder": [
        "$ARG1 does the same for the $ARG2"
    ],
    "derivative*****manifold": [
        "$ARG1 in order to map corrupted points back onto the $ARG2"
    ],
    "example*****embedding": [
        "$ARG1 is also called its $ARG2"
    ],
    "manifold*****subset": [
        "$ARG1 is a low-dimensional $ARG2"
    ],
    "manifold learning*****embedding": [
        "$ARG1 algorithms , discussed below ) directly learn an $ARG2"
    ],
    "embedding*****example": [
        "$ARG1 for each training $ARG2"
    ],
    "example*****mapping": [
        "$ARG1 , while others learn a more general $ARG2"
    ],
    "function*****embedding": [
        "$ARG1 , that maps any point in the ambient space ( the input space ) to its $ARG2"
    ],
    "unsupervised learning*****manifolds": [
        "$ARG1 procedures that attempt to capture these $ARG2"
    ],
    "learning*****manifolds": [
        "$ARG1 nonlinear $ARG2"
    ],
    "manifolds*****graph": [
        "$ARG1 has focused on non-parametric methods based on the nearest-neighbor $ARG2"
    ],
    "manifold learning*****graph": [
        "$ARG1 procedures build a nearest neighbor $ARG2"
    ],
    "tangent plane*****graph": [
        "$ARG1 associated with a neighborhood of the $ARG2"
    ],
    "graph*****system": [
        "$ARG1 as well as a coordinate $ARG2"
    ],
    "vector*****embedding": [
        "$ARG1 position , or $ARG2"
    ],
    "number*****manifold": [
        "$ARG1 of examples is large enough to cover the curvature and twists of the $ARG2"
    ],
    "dataset*****tangent plane": [
        "$ARG1 ( Gong et al. , and Niyogi , 2003 ; Donoho and Grimes , 2003 ; Weinberger and Saul , 2004 ; Hinton and Roweis , 2003 ; van der Maaten and Hinton , 2008 ) associate each of nodes with a $ARG2"
    ],
    "tangent plane*****example": [
        "$ARG1 that spans the directions of variations associated with the di\ufb00erence vectors between the $ARG2"
    ],
    "system*****optimization": [
        "$ARG1 can then be obtained through an $ARG2"
    ],
    "optimization*****system": [
        "$ARG1 or solving a linear $ARG2"
    ],
    "manifold*****number": [
        "$ARG1 can be tiled by a large $ARG2"
    ],
    "manifold learning*****manifolds": [
        "$ARG1 , raised in Bengio and Monperrus ( 2005 ) : if the $ARG2"
    ],
    "manifolds*****number": [
        "$ARG1 are not very smooth ( they have many peaks and troughs and twists ) , one may need a very large $ARG2"
    ],
    "tangent plane*****system": [
        "$ARG1 ( see \ufb01gure 14.6 ) at each location are known , then they can be tiled to form a global coordinate $ARG2"
    ],
    "system*****variance": [
        "$ARG1 or as a locally \ufb02at Gaussian , or \u201c pancake , \u201d with a very small $ARG2"
    ],
    "variance*****system": [
        "$ARG1 in the directions de\ufb01ning the coordinate $ARG2"
    ],
    "manifold*****algorithm": [
        "$ARG1 Parzen window $ARG2"
    ],
    "manifold*****translation": [
        "$ARG1 resulting from $ARG2"
    ],
    "template*****manifolds": [
        "$ARG1 drives the complexity of the $ARG2"
    ],
    "manifolds*****image": [
        "$ARG1 that are generated by performing simple $ARG2"
    ],
    "distributed representation*****deep learning": [
        "$ARG1 and $ARG2"
    ],
    "deep learning*****manifold": [
        "$ARG1 for capturing $ARG2",
        "$ARG1 robust to small changes anywhere along the $ARG2"
    ],
    "frobenius norm*****jacobian matrix": [
        "$ARG1 ( sum of squared elements ) of the $ARG2"
    ],
    "jacobian matrix*****partial derivative": [
        "$ARG1 of $ARG2"
    ],
    "partial derivative*****encoder": [
        "$ARG1 associated with the $ARG2"
    ],
    "connection*****denoising autoencoder": [
        "$ARG1 between the $ARG2"
    ],
    "contractive autoencoder*****function": [
        "$ARG1 : Alain and Bengio ( 2013 ) showed that in the limit of small Gaussian input noise , the denoising reconstruction error is equivalent to a contractive penalty on the reconstruction $ARG2"
    ],
    "other*****denoising autoencoder": [
        "$ARG1 words , $ARG2"
    ],
    "denoising autoencoder*****function": [
        "$ARG1 make the reconstruction $ARG2"
    ],
    "function*****contractive autoencoder": [
        "$ARG1 resist small but \ufb01nite-sized perturbations of the input , while $ARG2"
    ],
    "contractive autoencoder*****feature": [
        "$ARG1 make the $ARG2"
    ],
    "manifolds*****example": [
        "$ARG1 ( see for $ARG2"
    ],
    "jacobian matrix*****encoder": [
        "$ARG1 J at a point x as approximating the nonlinear $ARG2"
    ],
    "change*****tangent plane": [
        "$ARG1 h , so these are likely to be directions which approximate the $ARG2"
    ],
    "singular value*****1": [
        "$ARG1 of J dropping below $ARG2",
        "$ARG1 remain above $ARG2"
    ],
    "singular value*****contractive autoencoder": [
        "$ARG1 are interpreted as the tangent directions that the $ARG2"
    ],
    "example*****tangent vector": [
        "$ARG1 , a CAE applied to images should learn $ARG2"
    ],
    "tangent vector*****image": [
        "$ARG1 that show how the $ARG2"
    ],
    "singular vector*****image": [
        "$ARG1 do seem to correspond to meaningful transformations of the input $ARG2"
    ],
    "regularization*****hidden layer": [
        "$ARG1 criterion is that although it is cheap to compute in the case of a single $ARG2"
    ],
    "pca*****illustration": [
        "$ARG1 ( no sharing across regions ) Figure 14.10 : $ARG2"
    ],
    "illustration*****tangent vector": [
        "$ARG1 of $ARG2"
    ],
    "tangent vector*****manifold": [
        "$ARG1 of the $ARG2"
    ],
    "pca*****contractive autoencoder": [
        "$ARG1 and by a $ARG2"
    ],
    "image*****dataset": [
        "$ARG1 of a dog drawn from the CIFAR-10 $ARG2"
    ],
    "tangent vector*****singular vector": [
        "$ARG1 are estimated by the leading $ARG2"
    ],
    "singular vector*****jacobian matrix": [
        "$ARG1 of the $ARG2"
    ],
    "jacobian matrix*****mapping": [
        "$ARG1 \u2202h \u2202x of the input-to-code $ARG2"
    ],
    "pca*****parameter sharing": [
        "$ARG1 and the CAE can capture local tangents , the CAE is able to form more accurate estimates from limited training data because it exploits $ARG2"
    ],
    "parameter sharing*****subset": [
        "$ARG1 across di\ufb00erent locations that share a $ARG2"
    ],
    "example*****encoder": [
        "$ARG1 , the $ARG2"
    ],
    "encoder*****constant": [
        "$ARG1 could consist of multiplying the input by a small $ARG2"
    ],
    "constant*****decoder": [
        "$ARG1 \ue00f and the $ARG2"
    ],
    "weights*****standard": [
        "$ARG1 of f and g. Both f and g are $ARG2"
    ],
    "predictive sparse decomposition*****model": [
        "$ARG1 ( PSD ) is a $ARG2"
    ],
    "feature learning*****object recognition": [
        "$ARG1 for $ARG2"
    ],
    "object recognition*****video": [
        "$ARG1 in images and $ARG2"
    ],
    "video*****audio": [
        "$ARG1 ( Kavukcuoglu et al. , 2009 , 2010 ; Jarrett et al. , 2009 ; Farabet et al. , 2011 ) , as well as for $ARG2"
    ],
    "sparse coding*****algorithm": [
        "$ARG1 , the training $ARG2"
    ],
    "sparse coding*****example": [
        "$ARG1 is an $ARG2"
    ],
    "encoder*****model": [
        "$ARG1 f is used to compute the learned features when the $ARG2"
    ],
    "dimensionality reduction*****information retrieval": [
        "$ARG1 and $ARG2",
        "$ARG1 is $ARG2"
    ],
    "dimensionality reduction*****representation learning": [
        "$ARG1 was one of the \ufb01rst applications of $ARG2"
    ],
    "dimensionality reduction*****other": [
        "$ARG1 place semantically related examples near each $ARG2",
        "$ARG1 that $ARG2"
    ],
    "mapping*****task": [
        "$ARG1 to the lower-dimensional space aid One $ARG2"
    ],
    "task*****dimensionality reduction": [
        "$ARG1 that bene\ufb01ts even more than usual from $ARG2",
        "$ARG1 derives the usual bene\ufb01ts from $ARG2"
    ],
    "information retrieval*****task": [
        "$ARG1 , the $ARG2"
    ],
    "task*****database": [
        "$ARG1 of \ufb01nding entries in a $ARG2"
    ],
    "other*****search": [
        "$ARG1 tasks do , but also derives the additional bene\ufb01t that $ARG2"
    ],
    "algorithm*****database": [
        "$ARG1 to produce a code that is lowdimensional and binary , then we can store all $ARG2"
    ],
    "database*****mapping": [
        "$ARG1 entries in a hash table $ARG2"
    ],
    "information retrieval*****database": [
        "$ARG1 by returning all $ARG2"
    ],
    "information retrieval*****dimensionality reduction": [
        "$ARG1 via $ARG2"
    ],
    "dimensionality reduction*****semantic hashing": [
        "$ARG1 and binarization is called $ARG2"
    ],
    "semantic hashing*****function": [
        "$ARG1 , one typically uses an encoding $ARG2"
    ],
    "information*****magnitude": [
        "$ARG1 as possible , the network must increase the $ARG2"
    ],
    "magnitude*****sigmoid function": [
        "$ARG1 of the inputs to the $ARG2"
    ],
    "function*****loss": [
        "$ARG1 has been further explored in several directions , including the idea of training the representations so as to optimize a $ARG2"
    ],
    "convolutional network*****convolutional neural network": [
        "$ARG1 ( LeCun , 1989 ) , also known as $ARG2"
    ],
    "convolutional neural network*****neural network": [
        "$ARG1 or CNNs , are a specialized kind of $ARG2"
    ],
    "convolutional neural network*****operation": [
        "$ARG1 \u201d indicates that the network employs a mathematical $ARG2"
    ],
    "operation*****convolution": [
        "$ARG1 called $ARG2",
        "$ARG1 is called $ARG2",
        "$ARG1 that consists of many applications of $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 to discrete $ARG2",
        "$ARG1 can be performed using the $ARG2"
    ],
    "convolution*****operation": [
        "$ARG1 is a specialized kind of linear $ARG2",
        "$ARG1 is an $ARG2",
        "$ARG1 with a stride length of two implemented in a single $ARG2",
        "$ARG1 , because it is a similar $ARG2",
        "$ARG1 is a linear $ARG2",
        "$ARG1 is one such $ARG2",
        "$ARG1 in some cases , but in the general case requires a third $ARG2"
    ],
    "convolution*****general matrix": [
        "$ARG1 in place of $ARG2"
    ],
    "convolution*****neural network": [
        "$ARG1 in a $ARG2",
        "$ARG1 , without requiring any further changes to the $ARG2",
        "$ARG1 in the context of $ARG2",
        "$ARG1 in the context of $ARG2"
    ],
    "pooling*****almost all": [
        "$ARG1 , which $ARG2"
    ],
    "operation*****convolutional neural network": [
        "$ARG1 used in a $ARG2"
    ],
    "convolutional neural network*****convolution": [
        "$ARG1 does not correspond precisely to the de\ufb01nition of $ARG2"
    ],
    "convolution*****other": [
        "$ARG1 as used in $ARG2",
        "$ARG1 is used simultaneously with $ARG2",
        "$ARG1 is not naturally equivariant to some $ARG2"
    ],
    "other*****mathematics": [
        "$ARG1 \ufb01elds such as engineering or pure $ARG2"
    ],
    "convolutional network*****example": [
        "$ARG1 stand out as an $ARG2",
        "$ARG1 terminology , the \ufb01rst argument ( in this $ARG2",
        "$ARG1 argument ( in this $ARG2",
        "$ARG1 Figure 9.1 : An $ARG2",
        "$ARG1 Figure 9.9 : $ARG2"
    ],
    "convolutional network*****deep learning": [
        "$ARG1 have played in the history of $ARG2",
        "$ARG1 have played an important role in the history of $ARG2"
    ],
    "operation*****example": [
        "$ARG1 is typically denoted with an asterisk : s ( t ) = ( x \u2217 w ) ( t ) In our $ARG2"
    ],
    "example*****probability density function": [
        "$ARG1 , w needs to be a valid $ARG2",
        "$ARG1 of a $ARG2"
    ],
    "probability density function*****average": [
        "$ARG1 , or the output is not a weighted $ARG2"
    ],
    "convolution*****integral": [
        "$ARG1 is de\ufb01ned for any functions for which the above $ARG2"
    ],
    "integral*****other": [
        "$ARG1 is de\ufb01ned , and may be used for $ARG2"
    ],
    "function*****convolution": [
        "$ARG1 x ) to the $ARG2",
        "$ARG1 that translates the input , i.e. , shifts it , then the $ARG2",
        "$ARG1 When discussing $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "convolution*****machine learning": [
        "$ARG1 : s ( t ) = ( x \u2217 w ) ( t ) = x ( a ) w ( t \u2212 a ) In $ARG2",
        "$ARG1 to be used alone in $ARG2",
        "$ARG1 leverages three important ideas that can help improve a $ARG2"
    ],
    "kernel*****learning": [
        "$ARG1 is usually a multidimensional array of parameters that are adapted by the $ARG2"
    ],
    "element*****kernel": [
        "$ARG1 of the input and $ARG2",
        "$ARG1 of a 3-element $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "image*****kernel": [
        "$ARG1 I as our input , we probably also want to use a two-dimensional $ARG2",
        "$ARG1 has width m and the $ARG2"
    ],
    "property*****convolution": [
        "$ARG1 of $ARG2"
    ],
    "kernel*****property": [
        "$ARG1 is to obtain the commutative $ARG2"
    ],
    "convolutional network*****property": [
        "$ARG1 is useful for writing proofs , it is not usually an important $ARG2",
        "$ARG1 capture this $ARG2"
    ],
    "function*****cross-correlation": [
        "$ARG1 called the $ARG2"
    ],
    "cross-correlation*****convolution": [
        "$ARG1 , which is the same as $ARG2",
        "$ARG1 but call it $ARG2"
    ],
    "machine learning*****cross-correlation": [
        "$ARG1 libraries implement $ARG2"
    ],
    "convention*****convolution": [
        "$ARG1 of calling both operations $ARG2"
    ],
    "kernel*****algorithm": [
        "$ARG1 in the appropriate place , so an $ARG2",
        "$ARG1 learned by an $ARG2"
    ],
    "algorithm*****convolution": [
        "$ARG1 based on $ARG2",
        "$ARG1 to 2-D $ARG2"
    ],
    "machine learning*****convolution": [
        "$ARG1 ; instead $ARG2"
    ],
    "other*****convolution": [
        "$ARG1 functions , and the combination of these functions does not commute regardless of whether the $ARG2",
        "$ARG1 , $ARG2",
        "$ARG1 extreme case , which MATLAB refers to as full $ARG2",
        "$ARG1 operations besides $ARG2"
    ],
    "operation*****kernel": [
        "$ARG1 \ufb02ips its $ARG2"
    ],
    "kernel*****example": [
        "$ARG1 or See \ufb01gure 9.1 for an $ARG2"
    ],
    "example*****convolution": [
        "$ARG1 of $ARG2",
        "$ARG1 , for univariate discrete $ARG2",
        "$ARG1 of 2-D $ARG2"
    ],
    "kernel*****tensor": [
        "$ARG1 \ufb02ipping ) applied to a 2-D $ARG2",
        "$ARG1 to the corresponding upper-left region of the input $ARG2"
    ],
    "convolution*****multiplication": [
        "$ARG1 can be viewed as $ARG2"
    ],
    "convolution*****row": [
        "$ARG1 , each $ARG2"
    ],
    "doubly block circulant matrix*****convolution": [
        "$ARG1 corresponds to $ARG2"
    ],
    "convolution*****sparse matrix": [
        "$ARG1 usually corresponds to a very $ARG2"
    ],
    "sparse matrix*****matrix": [
        "$ARG1 ( a $ARG2"
    ],
    "kernel*****image": [
        "$ARG1 is usually much smaller than the input $ARG2",
        "$ARG1 lies entirely within the $ARG2",
        "$ARG1 is contained entirely within the $ARG2",
        "$ARG1 stack K applied to multi-channel $ARG2",
        "$ARG1 moves over both the horizontal and vertical axes of the $ARG2"
    ],
    "algorithm*****matrix multiplication": [
        "$ARG1 that works with $ARG2"
    ],
    "matrix multiplication*****matrix": [
        "$ARG1 and does not depend on speci\ufb01c properties of the $ARG2",
        "$ARG1 by a $ARG2",
        "$ARG1 with a \ufb01xed-shape $ARG2",
        "$ARG1 would take 320 \u00d7 280\u00d7 319 \u00d7 280 , or over eight billion , entries in the $ARG2"
    ],
    "matrix*****convolution": [
        "$ARG1 structure should work with $ARG2",
        "$ARG1 , making $ARG2",
        "$ARG1 de\ufb01ned by $ARG2"
    ],
    "image*****convolution": [
        "$ARG1 , called \u201c valid \u201d $ARG2",
        "$ARG1 patches , then use each learned centroid as a $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "element*****tensor": [
        "$ARG1 of the output $ARG2"
    ],
    "tensor*****kernel": [
        "$ARG1 is formed by applying the $ARG2"
    ],
    "system*****parameter sharing": [
        "$ARG1 : sparse interactions , $ARG2"
    ],
    "convolution*****variable": [
        "$ARG1 provides a means for working with inputs of $ARG2",
        "$ARG1 for processing $ARG2",
        "$ARG1 does not make sense if the input has $ARG2"
    ],
    "matrix multiplication*****example": [
        "$ARG1 requires m \u00d7 n parameters and the algorithms used in practice have O ( m \u00d7 n ) runtime ( per $ARG2"
    ],
    "task*****magnitude": [
        "$ARG1 while keeping k several orders of $ARG2"
    ],
    "parameter sharing*****function": [
        "$ARG1 refers to using the same parameter for more than one $ARG2",
        "$ARG1 can dramatically improve the e\ufb03ciency of a linear $ARG2"
    ],
    "parameter sharing*****weights": [
        "$ARG1 , one can say that a network has tied $ARG2"
    ],
    "kernel*****boundary": [
        "$ARG1 is used at every position of the input ( except perhaps some of the $ARG2"
    ],
    "operation*****learning": [
        "$ARG1 means that rather than $ARG2"
    ],
    "receptive \ufb01eld*****convolutional network": [
        "$ARG1 of the units in the deeper layers of a $ARG2"
    ],
    "convolutional network*****receptive \ufb01eld": [
        "$ARG1 is larger than the $ARG2"
    ],
    "convolution*****pooling": [
        "$ARG1 ( \ufb01gure 9.12 ) or $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 following by $ARG2",
        "$ARG1 and $ARG2"
    ],
    "convolutional network*****parameter sharing": [
        "$ARG1 Figure 9.5 : $ARG2"
    ],
    "kernel*****model": [
        "$ARG1 in a convolutional $ARG2"
    ],
    "recall*****magnitude": [
        "$ARG1 that k is usually several orders of $ARG2"
    ],
    "magnitude*****convolution": [
        "$ARG1 less than m. Since m and n are usually roughly the same size , k is practically insigni\ufb01cant compared to m \u00d7 n. $ARG2"
    ],
    "matrix multiplication*****memory": [
        "$ARG1 in terms of the $ARG2"
    ],
    "example*****parameter sharing": [
        "$ARG1 of both of these \ufb01rst two principles in action , \ufb01gure 9.6 shows how sparse connectivity and $ARG2"
    ],
    "convolution*****parameter sharing": [
        "$ARG1 , the particular form of $ARG2"
    ],
    "parameter sharing*****property": [
        "$ARG1 causes the layer to have a $ARG2"
    ],
    "property*****equivariance": [
        "$ARG1 called $ARG2"
    ],
    "equivariance*****translation": [
        "$ARG1 to $ARG2"
    ],
    "mapping*****image": [
        "$ARG1 one $ARG2"
    ],
    "series*****convolution": [
        "$ARG1 data , this means that $ARG2"
    ],
    "number*****multiple": [
        "$ARG1 of neighboring pixels is useful when applied to $ARG2"
    ],
    "example*****look for": [
        "$ARG1 , if we are processing images that are cropped to be centered on an individual \u2019 s face , we probably want to extract di\ufb00erent features at di\ufb00erent locations\u2014the part of the network processing the top of the face needs to $ARG2"
    ],
    "function*****statistic": [
        "$ARG1 replaces the output of the net at a certain location with a summary $ARG2"
    ],
    "example*****max pooling": [
        "$ARG1 , the $ARG2"
    ],
    "image*****operation": [
        "$ARG1 , which can be a useful $ARG2",
        "$ARG1 had been cropped too tightly , with some of the digits of the address being removed by the cropping $ARG2"
    ],
    "operation*****object detection": [
        "$ARG1 for $ARG2"
    ],
    "kernel*****convolution": [
        "$ARG1 containing two elements , and requires 319 \u00d7 280 \u00d7 3 = 267 , 960 \ufb02oating point operations ( two multiplications and one addition per output pixel ) to compute using $ARG2",
        "$ARG1 is separable , naive $ARG2"
    ],
    "matrix multiplication algorithm*****convolution": [
        "$ARG1 performs over sixteen billion \ufb02oating point operations , making $ARG2"
    ],
    "convolution*****number": [
        "$ARG1 would require the same $ARG2"
    ],
    "convolutional network*****detector layer": [
        "$ARG1 e.g. , recti\ufb01ed linear $ARG2"
    ],
    "detector layer*****convolutional neural network": [
        "$ARG1 : Nonlinearity e.g. , recti\ufb01ed linear Figure 9.7 : The components of a typical $ARG2"
    ],
    "mapping*****kernel": [
        "$ARG1 between $ARG2"
    ],
    "convolutional network*****operation": [
        "$ARG1 and Chellappa , 1988 ) $ARG2"
    ],
    "other*****pooling": [
        "$ARG1 popular $ARG2",
        "$ARG1 Figure 9.10 : $ARG2"
    ],
    "pooling*****average": [
        "$ARG1 functions include the $ARG2"
    ],
    "average*****norm": [
        "$ARG1 of a rectangular neighborhood , the L2 $ARG2"
    ],
    "norm*****average": [
        "$ARG1 of a rectangular neighborhood , or a weighted $ARG2"
    ],
    "pooling*****invariant": [
        "$ARG1 helps to make the representation become approximately $ARG2",
        "$ARG1 is an in\ufb01nitely strong prior that each unit should be $ARG2"
    ],
    "invariance*****translation": [
        "$ARG1 to $ARG2",
        "$ARG1 to local $ARG2",
        "$ARG1 to $ARG2"
    ],
    "translation*****change": [
        "$ARG1 means that if we translate the input by a small amount , the values of most of the pooled outputs do not $ARG2"
    ],
    "translation*****property": [
        "$ARG1 can be a very useful $ARG2"
    ],
    "property*****feature": [
        "$ARG1 if we care more about whether some $ARG2"
    ],
    "contains*****accuracy": [
        "$ARG1 a face , we need not know the location of the eyes with pixel-perfect $ARG2"
    ],
    "other*****feature": [
        "$ARG1 contexts , it is more important to preserve the location of a $ARG2"
    ],
    "pooling*****function": [
        "$ARG1 can be viewed as adding an in\ufb01nitely strong prior that the $ARG2"
    ],
    "pooling*****invariance": [
        "$ARG1 over spatial regions produces $ARG2"
    ],
    "translation*****invariant": [
        "$ARG1 , but if we pool over the outputs of separately parametrized convolutions , the features can learn which transformations to become $ARG2"
    ],
    "pooling*****statistics": [
        "$ARG1 units than detector units , by reporting summary $ARG2",
        "$ARG1 layer of the network may be de\ufb01ned to output four sets of summary $ARG2"
    ],
    "statistics*****pooling": [
        "$ARG1 for $ARG2"
    ],
    "pooling*****1": [
        "$ARG1 regions spaced k pixels apart rather than $ARG2"
    ],
    "function*****matrix multiplication": [
        "$ARG1 of its input size ( such as when the next layer is fully connected and based on $ARG2"
    ],
    "matrix multiplication*****reduction": [
        "$ARG1 ) this $ARG2"
    ],
    "reduction*****memory": [
        "$ARG1 in the input size can also result in improved statistical e\ufb03ciency and reduced $ARG2",
        "$ARG1 in the $ARG2"
    ],
    "variable*****classi\ufb01cation": [
        "$ARG1 size , the input to the $ARG2"
    ],
    "pooling*****classi\ufb01cation": [
        "$ARG1 regions so that the $ARG2",
        "$ARG1 to 3x3 grid : 3x3x64 with stride 4 : with stride 4 : with stride 4 : with stride 4 : Figure 9.11 : Examples of architectures for $ARG2"
    ],
    "classi\ufb01cation*****number": [
        "$ARG1 layer always receives the same $ARG2"
    ],
    "number*****statistics": [
        "$ARG1 of summary $ARG2"
    ],
    "example*****pooling": [
        "$ARG1 , the \ufb01nal $ARG2",
        "$ARG1 of learned invariances : A $ARG2",
        "$ARG1 , we do not use any $ARG2"
    ],
    "statistics*****image": [
        "$ARG1 , one for each quadrant of an $ARG2"
    ],
    "convolutional network*****max pooling": [
        "$ARG1 Figure 9.8 : $ARG2"
    ],
    "max pooling*****invariance": [
        "$ARG1 introduces $ARG2"
    ],
    "row*****max pooling": [
        "$ARG1 shows the outputs of $ARG2",
        "$ARG1 have changed , because the $ARG2"
    ],
    "max pooling*****pooling": [
        "$ARG1 , with a stride of one pixel between $ARG2"
    ],
    "pooling*****multiple": [
        "$ARG1 unit that pools over $ARG2"
    ],
    "multiple*****invariant": [
        "$ARG1 features that are learned with separate parameters can learn to be $ARG2"
    ],
    "set*****max pooling": [
        "$ARG1 of three learned \ufb01lters and a $ARG2"
    ],
    "max pooling*****invariant": [
        "$ARG1 unit can learn to become $ARG2",
        "$ARG1 over spatial positions is naturally $ARG2"
    ],
    "principle*****maxout": [
        "$ARG1 is leveraged by $ARG2"
    ],
    "invariant*****translation": [
        "$ARG1 to $ARG2",
        "$ARG1 features and features that will not under\ufb01t when the $ARG2",
        "$ARG1 speci\ufb01cally to $ARG2",
        "$ARG1 to $ARG2"
    ],
    "translation*****learning": [
        "$ARG1 ; this multi-channel approach is only necessary for $ARG2"
    ],
    "set*****pooling": [
        "$ARG1 of $ARG2"
    ],
    "pooling*****image": [
        "$ARG1 regions for each $ARG2"
    ],
    "pooling*****neural network": [
        "$ARG1 can complicate some kinds of $ARG2"
    ],
    "neural network*****information": [
        "$ARG1 architectures that use top-down $ARG2"
    ],
    "information*****boltzmann machine": [
        "$ARG1 , such as $ARG2"
    ],
    "convolutional network*****classi\ufb01cation": [
        "$ARG1 architectures for $ARG2",
        "$ARG1 can be used to output a high-dimensional , structured object , rather than just predicting a class label for a $ARG2"
    ],
    "classi\ufb01cation*****convolution": [
        "$ARG1 using $ARG2"
    ],
    "pooling*****recall": [
        "$ARG1 as an In\ufb01nitely Strong $ARG2"
    ],
    "recall*****concept": [
        "$ARG1 the $ARG2"
    ],
    "concept*****prior probability distribution": [
        "$ARG1 of a $ARG2"
    ],
    "probability*****support": [
        "$ARG1 on some parameters and says that these parameter values are completely forbidden , regardless of how much $ARG2"
    ],
    "weights*****receptive \ufb01eld": [
        "$ARG1 must be zero , except for in the small , spatially contiguous $ARG2"
    ],
    "convolution*****prior probability distribution": [
        "$ARG1 as introducing an in\ufb01nitely strong $ARG2"
    ],
    "convolutional network*****softmax": [
        "$ARG1 Output of $ARG2"
    ],
    "softmax*****pooling": [
        "$ARG1 : multiply : 1,000 units multiply : 1,000 units Output of reshape to Output of reshape to with stride 4 : Output of $ARG2"
    ],
    "classi\ufb01cation*****convolutional network": [
        "$ARG1 with $ARG2"
    ],
    "pooling*****tensor": [
        "$ARG1 for a few layers , the $ARG2"
    ],
    "tensor*****feature": [
        "$ARG1 for the convolutional $ARG2"
    ],
    "feature*****softmax": [
        "$ARG1 map down to a single value provides the argument to the $ARG2"
    ],
    "convolutional network*****function": [
        "$ARG1 says that the $ARG2",
        "$ARG1 that copies its input x to an approximate reconstruction r using the $ARG2",
        "$ARG1 Their work helped to characterize many aspects of brain $ARG2"
    ],
    "function*****contains": [
        "$ARG1 the layer should learn $ARG2"
    ],
    "contains*****translation": [
        "$ARG1 only local interactions and is equivariant to $ARG2"
    ],
    "pooling*****other": [
        "$ARG1 on some channels but not on $ARG2"
    ],
    "other*****invariant": [
        "$ARG1 channels , in order to get both highly $ARG2"
    ],
    "information*****convolution": [
        "$ARG1 from very distant locations in the input , then the prior imposed by $ARG2"
    ],
    "convolution*****view": [
        "$ARG1 may be Another key insight from this $ARG2"
    ],
    "convolution*****image": [
        "$ARG1 would be able to learn even if we permuted all of the pixels in the $ARG2",
        "$ARG1 , in which enough zeroes are added for every pixel to be visited k times in each direction , resulting in an output $ARG2",
        "$ARG1 kernels U is used on each step to compute the hidden representation given the input $ARG2"
    ],
    "invariant*****concept": [
        "$ARG1 and must discover the $ARG2"
    ],
    "concept*****learning": [
        "$ARG1 of topology via $ARG2"
    ],
    "neural network*****standard": [
        "$ARG1 , we usually do not refer exactly to the $ARG2"
    ],
    "standard*****convolution": [
        "$ARG1 discrete $ARG2"
    ],
    "neural network*****operation": [
        "$ARG1 , we usually actually mean an $ARG2"
    ],
    "convolutional network*****convolution": [
        "$ARG1 usually use multi-channel $ARG2",
        "$ARG1 Figure 9.12 : $ARG2",
        "$ARG1 In some cases , we do not actually want to use $ARG2",
        "$ARG1 Figure 9.14 : Comparison of local connections , $ARG2",
        "$ARG1 implemented using a $ARG2",
        "$ARG1 that incorporates strided $ARG2",
        "$ARG1 Note that the use of $ARG2",
        "$ARG1 without ever using $ARG2"
    ],
    "tensor*****element": [
        "$ARG1 K with $ARG2"
    ],
    "element*****connection": [
        "$ARG1 Ki , j , k , l giving the $ARG2"
    ],
    "element*****row": [
        "$ARG1 Vi , j , k giving the value of the input unit within channel i at $ARG2"
    ],
    "format*****tensor": [
        "$ARG1 as V. If Z is produced by convolving K across V without \ufb02ipping K , then Zi , j , k = V l , j +m\u22121 , k +n\u22121 Ki , l , m , n l , m , n where the summation over l , m and n is over all values for which the $ARG2"
    ],
    "feature*****convolutional network": [
        "$ARG1 of any $ARG2"
    ],
    "kernel*****1": [
        "$ARG1 has width k , the output will be of width m \u2212 k + $ARG2"
    ],
    "dimension*****1": [
        "$ARG1 of the network will eventually drop to $ARG2"
    ],
    "image*****1": [
        "$ARG1 of width m + k \u2212 $ARG2"
    ],
    "convolutional network*****kernel": [
        "$ARG1 with a $ARG2"
    ],
    "pooling*****convolution": [
        "$ARG1 , so only the $ARG2"
    ],
    "matrix*****graph": [
        "$ARG1 in the $ARG2"
    ],
    "graph*****connection": [
        "$ARG1 of our MLP is the same , but every $ARG2"
    ],
    "connection*****tensor": [
        "$ARG1 has its own weight , speci\ufb01ed by a 6-D $ARG2"
    ],
    "image*****look for": [
        "$ARG1 is a picture of a face , we only need to $ARG2"
    ],
    "look for*****convolution": [
        "$ARG1 the mouth in the bottom half of the It can also be useful to make versions of $ARG2"
    ],
    "convolution*****example": [
        "$ARG1 or locally connected layers in which the connectivity is further restricted , for $ARG2"
    ],
    "tiled convolution*****standard": [
        "$ARG1 , and $ARG2",
        "$ARG1 , and $ARG2"
    ],
    "convolutional network*****tiled convolution": [
        "$ARG1 Figure 9.16 : A comparison of locally connected layers , $ARG2",
        "$ARG1 To de\ufb01ne $ARG2"
    ],
    "tiled convolution*****set": [
        "$ARG1 has a $ARG2"
    ],
    "convolution*****tiled convolution": [
        "$ARG1 is equivalent to $ARG2"
    ],
    "tiled convolution*****1": [
        "$ARG1 with t = $ARG2"
    ],
    "kernel*****weights": [
        "$ARG1 with $ARG2"
    ],
    "tiled convolution*****tensor": [
        "$ARG1 algebraically , let k be a 6-D $ARG2"
    ],
    "set*****kernel": [
        "$ARG1 of t di\ufb00erent choices of $ARG2",
        "$ARG1 A wider $ARG2"
    ],
    "tiled convolution*****interaction": [
        "$ARG1 layers have an interesting $ARG2"
    ],
    "convolution*****convolutional network": [
        "$ARG1 are usually necessary to implement a $ARG2"
    ],
    "gradient*****kernel": [
        "$ARG1 with respect to the $ARG2"
    ],
    "kernel*****gradient": [
        "$ARG1 , given the $ARG2"
    ],
    "1*****property": [
        "$ARG1 , do not have this $ARG2"
    ],
    "recall*****convolution": [
        "$ARG1 that $ARG2"
    ],
    "operation*****matrix multiplication": [
        "$ARG1 and can thus be described as a $ARG2"
    ],
    "matrix multiplication*****tensor": [
        "$ARG1 ( if we \ufb01rst reshape the input $ARG2"
    ],
    "kernel*****matrix": [
        "$ARG1 is copied to several elements of the $ARG2"
    ],
    "other*****multiplication": [
        "$ARG1 operations needed to implement a convolutional $ARG2"
    ],
    "multiplication*****transpose": [
        "$ARG1 by the $ARG2",
        "$ARG1 by the $ARG2"
    ],
    "operation*****convolutional network": [
        "$ARG1 needed to back-propagate error derivatives through a convolutional layer , so it is needed to train $ARG2"
    ],
    "convolutional network*****hidden layer": [
        "$ARG1 that have more than one $ARG2"
    ],
    "operation*****sparse coding": [
        "$ARG1 commonly used in the models described in part III of this book , such as autoencoders , RBMs , and $ARG2"
    ],
    "operation*****forward propagation": [
        "$ARG1 with the $ARG2",
        "$ARG1 , as well as the size of the $ARG2"
    ],
    "operation*****policy": [
        "$ARG1 should return depends on the zero padding $ARG2"
    ],
    "policy*****forward propagation": [
        "$ARG1 and stride of the $ARG2"
    ],
    "multiple*****forward propagation": [
        "$ARG1 sizes of input to $ARG2"
    ],
    "forward propagation*****transpose": [
        "$ARG1 can result in the same size of output map , so the $ARG2"
    ],
    "backprop*****weights": [
        "$ARG1 from output to $ARG2"
    ],
    "weights*****backprop": [
        "$ARG1 , and $ARG2"
    ],
    "backprop*****convolutional network": [
        "$ARG1 from output to inputs\u2014are su\ufb03cient to compute all of the gradients needed to train any depth of feedforward $ARG2"
    ],
    "convolutional network*****transpose": [
        "$ARG1 with reconstruction functions based on the $ARG2"
    ],
    "forward propagation*****cost function": [
        "$ARG1 , we will need to use c itself to output Z , which is then propagated through the rest of the network and used to compute the $ARG2"
    ],
    "back-propagation*****tensor": [
        "$ARG1 , we will receive a $ARG2"
    ],
    "weights*****kernel": [
        "$ARG1 in the $ARG2"
    ],
    "matrix*****pca": [
        "$ARG1 just as $ARG2"
    ],
    "function*****transpose": [
        "$ARG1 h to perform the $ARG2"
    ],
    "tensor*****decoder": [
        "$ARG1 E. To train the $ARG2"
    ],
    "decoder*****gradient": [
        "$ARG1 , we need to obtain the $ARG2"
    ],
    "encoder*****gradient": [
        "$ARG1 , we need to obtain the $ARG2"
    ],
    "algorithm*****standard": [
        "$ARG1 on any $ARG2"
    ],
    "bias*****tiled convolution": [
        "$ARG1 , and for $ARG2"
    ],
    "bias*****convolution": [
        "$ARG1 per channel of the output and share it across all locations within each $ARG2"
    ],
    "tensor*****standard": [
        "$ARG1 , emitted by a $ARG2"
    ],
    "model*****tensor": [
        "$ARG1 might emit a $ARG2"
    ],
    "tensor*****probability": [
        "$ARG1 S , where Si , j , k is the $ARG2"
    ],
    "convolutional network*****1": [
        "$ARG1 H ( $ARG2",
        "$ARG1 layer is designed to capture three properties of V1 : $ARG2"
    ],
    "tensor*****image": [
        "$ARG1 X , with axes corresponding to $ARG2",
        "$ARG1 has axes corresponding to $ARG2"
    ],
    "tensor*****probability distribution": [
        "$ARG1 of labels Y\u0302 , with a $ARG2"
    ],
    "tensor*****convolution": [
        "$ARG1 of $ARG2"
    ],
    "classi\ufb01cation*****image": [
        "$ARG1 of a single object in an $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "image*****reduction": [
        "$ARG1 , the greatest $ARG2"
    ],
    "reduction*****pooling": [
        "$ARG1 in the spatial dimensions of the network comes from using $ARG2"
    ],
    "principle*****pooling": [
        "$ARG1 , one could use a $ARG2"
    ],
    "process*****image": [
        "$ARG1 these predictions in order to obtain a segmentation of the $ARG2"
    ],
    "convolutional network*****graphical model": [
        "$ARG1 can be trained to maximize an approximation of the $ARG2"
    ],
    "convolutional network*****video": [
        "$ARG1 applied to $ARG2"
    ],
    "collection*****image": [
        "$ARG1 of images , where each $ARG2"
    ],
    "kernel*****number": [
        "$ARG1 is simply applied a di\ufb00erent $ARG2",
        "$ARG1 width increases the $ARG2"
    ],
    "number*****convolution": [
        "$ARG1 of times depending on the size of the input , and the output of the $ARG2",
        "$ARG1 of hidden units Adding implicit zeros before $ARG2"
    ],
    "kernel*****doubly block circulant matrix": [
        "$ARG1 induces a di\ufb00erent size of $ARG2"
    ],
    "pooling*****number": [
        "$ARG1 regions scale in size proportional to the size of the input , in order to maintain a \ufb01xed $ARG2"
    ],
    "audio*****fourier transform": [
        "$ARG1 data that has been preprocessed with a $ARG2"
    ],
    "fourier transform*****audio": [
        "$ARG1 : We can transform the $ARG2"
    ],
    "audio*****tensor": [
        "$ARG1 waveform into a 2D $ARG2"
    ],
    "convolution*****model": [
        "$ARG1 in the time makes the $ARG2",
        "$ARG1 across the frequency axis makes the $ARG2"
    ],
    "point in time*****character": [
        "$ARG1 , the pose of the $ARG2"
    ],
    "kernel*****fourier transform": [
        "$ARG1 to the frequency domain using a $ARG2"
    ],
    "fourier transform*****multiplication": [
        "$ARG1 , performing point-wise $ARG2"
    ],
    "multiplication*****fourier transform": [
        "$ARG1 of the two signals , and converting back to the time domain using an inverse $ARG2"
    ],
    "problem*****convolution": [
        "$ARG1 sizes , this can be faster than the naive implementation of discrete $ARG2"
    ],
    "vector*****dimension": [
        "$ARG1 per $ARG2"
    ],
    "dimension*****kernel": [
        "$ARG1 , the $ARG2"
    ],
    "kernel*****dimension": [
        "$ARG1 is w elements wide in each $ARG2",
        "$ARG1 results in a narrower output $ARG2"
    ],
    "dimension*****convolution": [
        "$ARG1 , then naive multidimensional $ARG2"
    ],
    "convolution*****separable convolution": [
        "$ARG1 requires O ( wd ) runtime and parameter storage space , while $ARG2"
    ],
    "convolution*****accuracy": [
        "$ARG1 without harming the $ARG2"
    ],
    "convolutional network*****learning": [
        "$ARG1 training is $ARG2"
    ],
    "k-means*****image": [
        "$ARG1 clustering to small $ARG2"
    ],
    "learning*****convex optimization": [
        "$ARG1 the last layer is then typically a $ARG2"
    ],
    "problem*****logistic regression": [
        "$ARG1 , assuming the last layer is something like $ARG2"
    ],
    "logistic regression*****svm": [
        "$ARG1 or an $ARG2"
    ],
    "invariant*****weights": [
        "$ARG1 when assigned random $ARG2"
    ],
    "multilayer perceptron*****pretraining": [
        "$ARG1 , we use greedy layer-wise $ARG2"
    ],
    "8*****pretraining": [
        "$ARG1 has described how to perform supervised greedy layer-wise $ARG2"
    ],
    "example*****pretraining": [
        "$ARG1 of greedy layer-wise $ARG2"
    ],
    "convolutional network*****pretraining": [
        "$ARG1 us the opportunity to take the $ARG2"
    ],
    "pretraining*****multilayer perceptron": [
        "$ARG1 strategy one step further than is possible with $ARG2"
    ],
    "unsupervised learning*****convolutional network": [
        "$ARG1 to train a $ARG2"
    ],
    "convolution*****process": [
        "$ARG1 during the training $ARG2"
    ],
    "convolutional network*****back-propagation": [
        "$ARG1 are trained in a purely supervised fashion , using full forward and $ARG2",
        "$ARG1 Lang and Hinton ( 1988 ) introduced the use of $ARG2",
        "$ARG1 succeeded when general $ARG2"
    ],
    "convolutional network*****arti\ufb01cial intelligence": [
        "$ARG1 are perhaps the greatest success story of biologically inspired $ARG2"
    ],
    "convolutional network*****other": [
        "$ARG1 have been guided by many $ARG2",
        "$ARG1 had been used to win $ARG2",
        "$ARG1 trained on ImageNet to solve $ARG2"
    ],
    "neural network*****neuroscience": [
        "$ARG1 were drawn from $ARG2"
    ],
    "deep learning*****activity": [
        "$ARG1 models were based on recording the $ARG2"
    ],
    "deep learning*****view": [
        "$ARG1 , we can focus on a simpli\ufb01ed , cartoon $ARG2",
        "$ARG1 From the point of $ARG2"
    ],
    "view*****primary visual cortex": [
        "$ARG1 , we focus on a part of the brain called V1 , also known as the $ARG2"
    ],
    "preprocessing*****image": [
        "$ARG1 of the $ARG2"
    ],
    "contains*****simple cell": [
        "$ARG1 many $ARG2"
    ],
    "simple cell*****activity": [
        "$ARG1 \u2019 s $ARG2"
    ],
    "activity*****function": [
        "$ARG1 can to some extent be characterized by a linear $ARG2"
    ],
    "image*****receptive \ufb01eld": [
        "$ARG1 in a small , spatially localized $ARG2"
    ],
    "convolutional network*****simple cell": [
        "$ARG1 are designed to emulate these properties of $ARG2"
    ],
    "contains*****complex cell": [
        "$ARG1 many $ARG2"
    ],
    "simple cell*****complex cell": [
        "$ARG1 , but $ARG2",
        "$ARG1 \u201d and \u201c $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 are roughly linear and selective for certain features , $ARG2"
    ],
    "complex cell*****invariant": [
        "$ARG1 are $ARG2",
        "$ARG1 are also $ARG2",
        "$ARG1 are more nonlinear and become $ARG2",
        "$ARG1 is $ARG2"
    ],
    "invariant*****feature": [
        "$ARG1 to small shifts in the position of the $ARG2"
    ],
    "invariant*****pooling": [
        "$ARG1 to some changes in lighting that can not be captured simply by $ARG2"
    ],
    "convolutional network*****maxout": [
        "$ARG1 , such as $ARG2"
    ],
    "system*****pooling": [
        "$ARG1 , the basic strategy of detection followed by $ARG2"
    ],
    "multiple*****concept": [
        "$ARG1 anatomical layers of the brain , we eventually \ufb01nd cells that respond to some speci\ufb01c $ARG2",
        "$ARG1 inputs : f : Rn \u2192 R. For the $ARG2",
        "$ARG1 inputs , we must make use of the $ARG2"
    ],
    "concept*****invariant": [
        "$ARG1 and are $ARG2"
    ],
    "analog*****convolutional network": [
        "$ARG1 to a $ARG2"
    ],
    "convolutional network*****system": [
        "$ARG1 and the mammalian vision $ARG2"
    ],
    "list*****human": [
        "$ARG1 : \u2022 The $ARG2"
    ],
    "human*****fovea": [
        "$ARG1 eye is mostly very low resolution , except for a tiny patch called the $ARG2"
    ],
    "attention*****deep learning": [
        "$ARG1 mechanisms into $ARG2"
    ],
    "deep learning*****attention": [
        "$ARG1 , $ARG2"
    ],
    "attention*****natural language processing": [
        "$ARG1 mechanisms have been most successful for $ARG2"
    ],
    "human*****system": [
        "$ARG1 visual $ARG2",
        "$ARG1 visual $ARG2"
    ],
    "information*****convolutional network": [
        "$ARG1 as $ARG2"
    ],
    "complex cell*****simple cell": [
        "$ARG1 \u201d might create a nonexistent distinction ; $ARG2"
    ],
    "complex cell*****continuum": [
        "$ARG1 might both be the same kind of cell but with their \u201c parameters \u201d enabling a $ARG2"
    ],
    "neuroscience*****convolutional network": [
        "$ARG1 has told us relatively little about how to train $ARG2"
    ],
    "parameter sharing*****multiple": [
        "$ARG1 across $ARG2"
    ],
    "multiple*****back-propagation": [
        "$ARG1 spatial locations date back to early connectionist models of vision ( Marr and Poggio , 1976 ) , but these models did not use the modern $ARG2"
    ],
    "algorithm*****gradient descent": [
        "$ARG1 and $ARG2",
        "$ARG1 such as $ARG2",
        "$ARG1 to minimize f ( x ) = 12 ||Ax \u2212 b||22 with respect to x using $ARG2"
    ],
    "example*****neocognitron": [
        "$ARG1 , the $ARG2"
    ],
    "back-propagation*****time-delay neural network": [
        "$ARG1 to train $ARG2"
    ],
    "convolutional network*****series": [
        "$ARG1 applied to time $ARG2"
    ],
    "convolutional network*****algorithm": [
        "$ARG1 by applying the same training $ARG2"
    ],
    "invariant*****simple cell": [
        "$ARG1 to some transformations of these $ARG2"
    ],
    "simple cell*****invariance": [
        "$ARG1 features , and stacks of layers that alternate between selectivity and $ARG2"
    ],
    "simple cell*****function": [
        "$ARG1 in the \ufb01rst layer are easier to analyze , because their responses are driven by a linear $ARG2"
    ],
    "arti\ufb01cial neural network*****image": [
        "$ARG1 , we can just display an $ARG2"
    ],
    "neural network*****weights": [
        "$ARG1 , we do not have access to the $ARG2"
    ],
    "linear model*****weights": [
        "$ARG1 to these responses in order to obtain an approximation of the neuron \u2019 s $ARG2"
    ],
    "correlation*****weights": [
        "$ARG1 shows us that most V1 cells have $ARG2"
    ],
    "weights*****gabor function": [
        "$ARG1 that are described by $ARG2"
    ],
    "gabor function*****image": [
        "$ARG1 describes the weight at a 2-D point in the $ARG2"
    ],
    "image*****function": [
        "$ARG1 as being a $ARG2"
    ],
    "simple cell*****image": [
        "$ARG1 as sampling the $ARG2",
        "$ARG1 to an $ARG2",
        "$ARG1 will respond to $ARG2",
        "$ARG1 are most excited when the wave of brightness in the $ARG2",
        "$ARG1 are most inhibited when the wave of brightness is fully out of phase with the weights\u2014when the $ARG2"
    ],
    "weights*****function": [
        "$ARG1 that are also a $ARG2"
    ],
    "view*****simple cell": [
        "$ARG1 , the response of a $ARG2",
        "$ARG1 of $ARG2"
    ],
    "simple cell*****other": [
        "$ARG1 will only respond to values near where x \ue030 and y\ue030 are both zero , in $ARG2"
    ],
    "other*****receptive \ufb01eld": [
        "$ARG1 words , near the center of the cell \u2019 s $ARG2"
    ],
    "magnitude*****simple cell": [
        "$ARG1 of the $ARG2"
    ],
    "simple cell*****receptive \ufb01eld": [
        "$ARG1 \u2019 s response , while \u03b2 x and \u03b2 y control how quickly its $ARG2"
    ],
    "image*****weights": [
        "$ARG1 has the same phase as the $ARG2",
        "$ARG1 is bright where the $ARG2",
        "$ARG1 is dark where the $ARG2"
    ],
    "weights*****view": [
        "$ARG1 are The cartoon $ARG2"
    ],
    "view*****complex cell": [
        "$ARG1 of a $ARG2"
    ],
    "complex cell*****norm": [
        "$ARG1 is that it computes \ue070the L $ARG2"
    ],
    "vector*****simple cell": [
        "$ARG1 containing two $ARG2"
    ],
    "special case*****set": [
        "$ARG1 occurs when s1 has all of the same parameters as s 0 except for \u03c6 , and \u03c6 is $ARG2"
    ],
    "complex cell*****image": [
        "$ARG1 de\ufb01ned in this way responds when the Gaussian reweighted $ARG2"
    ],
    "other*****complex cell": [
        "$ARG1 words , the $ARG2"
    ],
    "invariant*****image": [
        "$ARG1 to small translations of the $ARG2"
    ],
    "convolutional network*****gabor function": [
        "$ARG1 Figure 9.18 : $ARG2"
    ],
    "gabor function*****system": [
        "$ARG1 with di\ufb00erent values of the parameters that control the coordinate $ARG2"
    ],
    "neuroscience*****machine learning": [
        "$ARG1 and $ARG2"
    ],
    "sparse coding*****receptive \ufb01eld": [
        "$ARG1 , learns features with $ARG2"
    ],
    "receptive \ufb01eld*****simple cell": [
        "$ARG1 similar to those of $ARG2"
    ],
    "learning*****natural image": [
        "$ARG1 algorithms learn features with Gabor-like functions when applied to $ARG2"
    ],
    "algorithm*****natural image": [
        "$ARG1 does not learn some sort of edge detector when applied to $ARG2"
    ],
    "convolutional network*****machine learning": [
        "$ARG1 Figure 9.19 : Many $ARG2"
    ],
    "machine learning*****natural image": [
        "$ARG1 algorithms learn features that detect edges or speci\ufb01c colors of edges when applied to $ARG2"
    ],
    "feature*****gabor function": [
        "$ARG1 detectors are reminiscent of the $ARG2"
    ],
    "gabor function*****primary visual cortex": [
        "$ARG1 known to be present in $ARG2"
    ],
    "weights*****unsupervised learning": [
        "$ARG1 learned by an $ARG2"
    ],
    "sparse coding*****image": [
        "$ARG1 ) applied to small $ARG2"
    ],
    "convolution*****maxout": [
        "$ARG1 kernels learned by the \ufb01rst layer of a fully supervised convolutional $ARG2"
    ],
    "neural network*****group": [
        "$ARG1 research $ARG2"
    ],
    "group*****convolutional network": [
        "$ARG1 at AT & T developed a $ARG2"
    ],
    "challenge*****convolutional network": [
        "$ARG1 , but $ARG2"
    ],
    "machine learning*****computer vision": [
        "$ARG1 and $ARG2"
    ],
    "convolutional network*****multiple": [
        "$ARG1 were more computationally e\ufb03cient than fully connected networks , so it was easier to run $ARG2"
    ],
    "multiple*****hyperparameters": [
        "$ARG1 experiments with them and tune their implementation and $ARG2"
    ],
    "solution*****process": [
        "$ARG1 via an iterative $ARG2"
    ],
    "process*****solution": [
        "$ARG1 , rather than analytically deriving a formula providing a symbolic expression for the correct $ARG2"
    ],
    "function*****memory": [
        "$ARG1 involves real numbers , which can not be represented precisely using a \ufb01nite amount of $ARG2"
    ],
    "almost all*****number": [
        "$ARG1 real numbers , we incur some approximation error when we represent the $ARG2"
    ],
    "function*****softmax function": [
        "$ARG1 that must be stabilized against under\ufb02ow and over\ufb02ow is the $ARG2"
    ],
    "softmax function*****multinoulli distribution": [
        "$ARG1 is often used to predict the probabilities associated with a $ARG2"
    ],
    "softmax*****constant": [
        "$ARG1 ( x ) i = \ue050n j=1 exp ( xj ) Consider what happens when all of the xi are equal to some $ARG2"
    ],
    "softmax function*****scalar": [
        "$ARG1 is not changed analytically by adding or subtracting a $ARG2"
    ],
    "eigenvalue*****condition number": [
        "$ARG1 decomposition , its $ARG2"
    ],
    "sensitivity*****property": [
        "$ARG1 is an intrinsic $ARG2"
    ],
    "property*****matrix": [
        "$ARG1 of the $ARG2"
    ],
    "cost function*****loss function": [
        "$ARG1 , $ARG2"
    ],
    "loss function*****error function": [
        "$ARG1 , or $ARG2"
    ],
    "illustration*****gradient descent": [
        "$ARG1 of how the $ARG2"
    ],
    "technique*****gradient descent": [
        "$ARG1 is called $ARG2"
    ],
    "example*****technique": [
        "$ARG1 of this $ARG2",
        "$ARG1 , a $ARG2"
    ],
    "derivative*****information": [
        "$ARG1 provides no $ARG2"
    ],
    "multiple*****function": [
        "$ARG1 global minima of the $ARG2"
    ],
    "deep learning*****saddle points": [
        "$ARG1 , we optimize functions that may have many local minima that are not optimal , and many $ARG2"
    ],
    "concept*****scalar": [
        "$ARG1 of \u201c minimization \u201d to make sense , there must still be only one ( $ARG2"
    ],
    "scalar*****multiple": [
        "$ARG1 ) For functions with $ARG2"
    ],
    "partial derivative*****variable": [
        "$ARG1 \u2202x $ARG2"
    ],
    "gradient*****derivative": [
        "$ARG1 generalizes the notion of $ARG2"
    ],
    "derivative*****vector": [
        "$ARG1 is with respect to a $ARG2"
    ],
    "gradient*****partial derivative": [
        "$ARG1 is the $ARG2",
        "$ARG1 : Jacobian and Hessian Matrices Sometimes we need to \ufb01nd all of the $ARG2"
    ],
    "optimization algorithm*****multiple": [
        "$ARG1 may fail to \ufb01nd a global minimum when there are $ARG2"
    ],
    "deep learning*****cost function": [
        "$ARG1 , we generally accept such solutions even though they are not truly minimal , so long as they correspond to signi\ufb01cantly low values of the $ARG2"
    ],
    "directional derivative*****unit vector": [
        "$ARG1 in direction u ( a $ARG2"
    ],
    "unit vector*****slope": [
        "$ARG1 ) is the $ARG2"
    ],
    "other*****directional derivative": [
        "$ARG1 words , the $ARG2"
    ],
    "directional derivative*****derivative": [
        "$ARG1 is the $ARG2"
    ],
    "directional derivative*****gradient": [
        "$ARG1 : u , u\ue03e u=1 u\ue03e \u2207x f ( x ) min ||u||2 ||\u2207x f ( x ) ||2 cos \u03b8 u , u \ue03e u=1 where \u03b8 is the angle between u and the $ARG2"
    ],
    "method of steepest descent*****gradient descent": [
        "$ARG1 or $ARG2"
    ],
    "learning rate*****scalar": [
        "$ARG1 , a positive $ARG2"
    ],
    "set*****constant": [
        "$ARG1 \ue00f to a small $ARG2"
    ],
    "steepest descent*****element": [
        "$ARG1 converges when every $ARG2"
    ],
    "optimization*****concept": [
        "$ARG1 in continuous spaces , the general $ARG2"
    ],
    "objective function*****hill climbing": [
        "$ARG1 of discrete parameters is called $ARG2"
    ],
    "matrix*****partial derivative": [
        "$ARG1 containing all such $ARG2"
    ],
    "partial derivative*****jacobian matrix": [
        "$ARG1 is known as a $ARG2"
    ],
    "function*****jacobian matrix": [
        "$ARG1 f : R m \u2192 Rn , then the $ARG2"
    ],
    "second derivative*****derivative": [
        "$ARG1 tells us how the \ufb01rst $ARG2",
        "$ARG1 f \ue030\ue030 ( x ) > 0 , the \ufb01rst $ARG2"
    ],
    "derivative*****change": [
        "$ARG1 will $ARG2"
    ],
    "function*****second derivative": [
        "$ARG1 has a $ARG2"
    ],
    "second derivative*****function": [
        "$ARG1 is negative , the $ARG2",
        "$ARG1 is positive , the $ARG2",
        "$ARG1 determines the curvature of a $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "expected*****cost function": [
        "$ARG1 and eventually begins to increase , so steps that are too large can actually increase the \ufb01gure 4.4 to see how di\ufb00erent forms of curvature a\ufb00ect the relationship between the value of the $ARG2"
    ],
    "multiple*****second derivative": [
        "$ARG1 input dimensions , there are many $ARG2",
        "$ARG1 dimensions , we need to examine all of the $ARG2",
        "$ARG1 dimensions , there is a di\ufb00erent $ARG2"
    ],
    "matrix*****hessian matrix": [
        "$ARG1 called the $ARG2"
    ],
    "deep learning*****almost everywhere": [
        "$ARG1 have a symmetric Hessian $ARG2"
    ],
    "hessian matrix*****set": [
        "$ARG1 is real and symmetric , we can decompose it into a $ARG2"
    ],
    "second derivative*****unit vector": [
        "$ARG1 in a speci\ufb01c direction represented by a $ARG2"
    ],
    "eigenvector*****second derivative": [
        "$ARG1 of H , the $ARG2"
    ],
    "second derivative*****eigenvalue": [
        "$ARG1 in that direction is given by the corresponding $ARG2",
        "$ARG1 and the minimum $ARG2",
        "$ARG1 test is inconclusive in the cross section corresponding to the zero $ARG2"
    ],
    "other*****second derivative": [
        "$ARG1 directions of d , the directional $ARG2"
    ],
    "second derivative*****average": [
        "$ARG1 is a weighted $ARG2"
    ],
    "average*****weights": [
        "$ARG1 of all of the eigenvalues , with $ARG2"
    ],
    "eigenvalue*****second derivative": [
        "$ARG1 determines the maximum $ARG2",
        "$ARG1 determines the minimum $ARG2"
    ],
    "second derivative*****gradient descent": [
        "$ARG1 tells us how well we can expect a $ARG2"
    ],
    "series*****function": [
        "$ARG1 approximation to the $ARG2",
        "$ARG1 approximation of the $ARG2"
    ],
    "expected*****slope": [
        "$ARG1 improvement due to the $ARG2"
    ],
    "function*****eigenvector": [
        "$ARG1 the most yields In the worst case , when g aligns with the $ARG2"
    ],
    "eigenvalue*****function": [
        "$ARG1 \u03bb max , then this optimal step size is given by \u03bb max extent that the $ARG2"
    ],
    "learning*****second derivative": [
        "$ARG1 The $ARG2"
    ],
    "other*****slope": [
        "$ARG1 words , as we move right , the $ARG2"
    ],
    "eigendecomposition*****hessian matrix": [
        "$ARG1 of the $ARG2"
    ],
    "hessian matrix*****second derivative": [
        "$ARG1 , we can generalize the $ARG2"
    ],
    "second derivative*****multiple": [
        "$ARG1 test to $ARG2"
    ],
    "multiple*****saddle points": [
        "$ARG1 dimensions , it is actually possible to \ufb01nd positive evidence of $ARG2"
    ],
    "condition number*****second derivative": [
        "$ARG1 of the Hessian at this point measures how much the $ARG2"
    ],
    "second derivative*****other": [
        "$ARG1 di\ufb00er from each $ARG2"
    ],
    "condition number*****gradient descent": [
        "$ARG1 , $ARG2"
    ],
    "gradient descent*****change": [
        "$ARG1 is unaware of this $ARG2"
    ],
    "change*****derivative": [
        "$ARG1 in the $ARG2"
    ],
    "information*****hessian matrix": [
        "$ARG1 from the $ARG2",
        "$ARG1 contained in the $ARG2"
    ],
    "dimension*****eigenvalue": [
        "$ARG1 , it is not necessary to have an $ARG2",
        "$ARG1 , the $ARG2"
    ],
    "gradient descent*****information": [
        "$ARG1 fails to exploit the curvature $ARG2"
    ],
    "hessian matrix*****condition number": [
        "$ARG1 has $ARG2"
    ],
    "path*****gradient descent": [
        "$ARG1 followed by $ARG2"
    ],
    "gradient descent*****feature": [
        "$ARG1 wastes time repeatedly descending canyon walls , because they are the steepest $ARG2"
    ],
    "eigenvector*****directional derivative": [
        "$ARG1 pointed in this direction indicates that this $ARG2"
    ],
    "directional derivative*****optimization algorithm": [
        "$ARG1 is rapidly increasing , so an $ARG2"
    ],
    "optimization algorithm*****search": [
        "$ARG1 based on the Hessian could predict that the steepest direction is not actually a promising $ARG2"
    ],
    "method*****series": [
        "$ARG1 is based on using a second-order Taylor $ARG2"
    ],
    "positive de\ufb01nite*****function": [
        "$ARG1 quadratic $ARG2"
    ],
    "method*****function": [
        "$ARG1 consists of applying equation 4.12 once to jump to the minimum of the $ARG2"
    ],
    "saddle points*****gradient": [
        "$ARG1 unless the $ARG2"
    ],
    "gradient descent*****optimization algorithm": [
        "$ARG1 , are called \ufb01rst-order $ARG2"
    ],
    "optimization algorithm*****hessian matrix": [
        "$ARG1 that also use the $ARG2"
    ],
    "hessian matrix*****method": [
        "$ARG1 , such as Newton \u2019 s $ARG2"
    ],
    "method*****optimization algorithm": [
        "$ARG1 , are called second-order $ARG2"
    ],
    "optimization algorithm*****deep learning": [
        "$ARG1 for a limited family In the context of $ARG2"
    ],
    "rate*****change": [
        "$ARG1 of $ARG2"
    ],
    "change*****lipschitz constant": [
        "$ARG1 is bounded by a $ARG2"
    ],
    "property*****assumption": [
        "$ARG1 is useful because it allows us to quantify our $ARG2"
    ],
    "change*****algorithm": [
        "$ARG1 in the input made by an $ARG2"
    ],
    "constraint*****optimization problem": [
        "$ARG1 , and many $ARG2"
    ],
    "deep learning*****minor": [
        "$ARG1 can be made Lipschitz continuous with relatively $ARG2"
    ],
    "deep learning*****convex optimization": [
        "$ARG1 are di\ufb03cult to express in terms of $ARG2"
    ],
    "convex optimization*****deep learning": [
        "$ARG1 is used only as a subroutine of some $ARG2",
        "$ARG1 is greatly diminished in the context of $ARG2"
    ],
    "convergence*****deep learning": [
        "$ARG1 of $ARG2"
    ],
    "information*****convex optimization": [
        "$ARG1 about $ARG2"
    ],
    "set*****constrained optimization": [
        "$ARG1 S. This is known as $ARG2",
        "$ARG1 S are called feasible points in $ARG2",
        "$ARG1 of properties describe the optimal points of $ARG2"
    ],
    "constrained optimization*****gradient descent": [
        "$ARG1 is simply to modify $ARG2"
    ],
    "gradient descent*****constraint": [
        "$ARG1 taking the $ARG2"
    ],
    "constant*****gradient descent": [
        "$ARG1 step size \ue00f , we can make $ARG2"
    ],
    "gradient descent*****line search": [
        "$ARG1 steps , then project the result back into S. If we use a $ARG2"
    ],
    "line search*****search": [
        "$ARG1 , we can $ARG2"
    ],
    "search*****constraint": [
        "$ARG1 only over step sizes \ue00f that yield new x points that are feasible , or we can project each point on the line back into the $ARG2"
    ],
    "optimization problem*****solution": [
        "$ARG1 whose $ARG2"
    ],
    "solution*****constrained optimization": [
        "$ARG1 to the original , $ARG2",
        "$ARG1 to $ARG2"
    ],
    "norm*****solution": [
        "$ARG1 , we can instead minimize g ( \u03b8 ) = f ( [ cos \u03b8 , sin \u03b8 ] \ue03e ) with respect to \u03b8 , then return [ cos \u03b8 , sin \u03b8 ] as the $ARG2"
    ],
    "optimization problem*****karush\u2013kuhn\u2013tucker": [
        "$ARG1 must be designed speci\ufb01cally for each case we The $ARG2"
    ],
    "karush\u2013kuhn\u2013tucker*****solution": [
        "$ARG1 ( KKT ) approach1 provides a very general $ARG2"
    ],
    "function*****generalized lagrangian": [
        "$ARG1 called the $ARG2"
    ],
    "generalized lagrangian*****lagrangian": [
        "$ARG1 or generalized Lagrange To de\ufb01ne the $ARG2"
    ],
    "equality constraint*****inequality constraint": [
        "$ARG1 and the inequalities involving h ( j ) are called $ARG2",
        "$ARG1 but not $ARG2"
    ],
    "optimization*****generalized lagrangian": [
        "$ARG1 of the $ARG2"
    ],
    "method*****lagrange multipliers": [
        "$ARG1 of $ARG2"
    ],
    "lagrange multipliers*****equality constraint": [
        "$ARG1 which allows $ARG2"
    ],
    "generalized lagrange function*****optimization problem": [
        "$ARG1 of \u2212f ( x ) , which leads to this $ARG2"
    ],
    "problem*****loop": [
        "$ARG1 with maximization in the outer $ARG2"
    ],
    "equality constraint*****matter": [
        "$ARG1 does not $ARG2"
    ],
    "matter*****optimization": [
        "$ARG1 ; we may de\ufb01ne it with addition or subtraction as we wish , because the $ARG2"
    ],
    "problem*****constraint": [
        "$ARG1 found using that $ARG2",
        "$ARG1 could have better local stationary points excluded by a $ARG2"
    ],
    "constraint*****other": [
        "$ARG1 excludes $ARG2"
    ],
    "problem*****subset": [
        "$ARG1 with an entire region of globally optimal points ( a wide , \ufb02at , region of equal cost ) could have a $ARG2",
        "$ARG1 by choosing which $ARG2"
    ],
    "subset*****problem": [
        "$ARG1 of this region eliminated by constraints , or a non-convex $ARG2"
    ],
    "constraint*****convergence": [
        "$ARG1 that is inactive at $ARG2"
    ],
    "solution*****boundary": [
        "$ARG1 is on the $ARG2"
    ],
    "boundary*****solution": [
        "$ARG1 imposed by the inequality and we must use its KKT multiplier to in\ufb02uence the $ARG2"
    ],
    "conditions*****gradient": [
        "$ARG1 are : \u2022 The $ARG2"
    ],
    "gradient*****generalized lagrangian": [
        "$ARG1 of the $ARG2"
    ],
    "problem*****method": [
        "$ARG1 using Newton \u2019 s $ARG2"
    ],
    "function*****constraint": [
        "$ARG1 , but subject to the $ARG2",
        "$ARG1 representing whether the $ARG2"
    ],
    "lagrangian*****1": [
        "$ARG1 L ( x , \u03bb ) = f ( x ) + \u03bb x\ue03e x \u2212 $ARG2"
    ],
    "problem*****moore-penrose pseudoinverse": [
        "$ARG1 may be found using the $ARG2"
    ],
    "magnitude*****constraint": [
        "$ARG1 of \u03bb must be chosen such that the result obeys the $ARG2"
    ],
    "1*****derivative": [
        "$ARG1 , this $ARG2"
    ],
    "derivative*****lagrangian": [
        "$ARG1 uphill and increase the $ARG2"
    ],
    "solution*****norm": [
        "$ARG1 with smaller $ARG2"
    ],
    "process*****norm": [
        "$ARG1 of solving the linear equation and adjusting \u03bb continues until x has the correct $ARG2"
    ],
    "norm*****derivative": [
        "$ARG1 and the $ARG2"
    ],
    "derivative*****machine learning": [
        "$ARG1 on \u03bb is This concludes the mathematical preliminaries that we use to develop $ARG2"
    ],
    "generalization*****bias": [
        "$ARG1 , under\ufb01tting , over\ufb01tting , $ARG2"
    ],
    "variance*****regularization": [
        "$ARG1 and $ARG2"
    ],
    "standard*****machine learning": [
        "$ARG1 concepts in $ARG2"
    ],
    "generalization*****regularization": [
        "$ARG1 error but not its training error. \u201d There are many $ARG2",
        "$ARG1 error can follow this curve or be complicated by $ARG2"
    ],
    "objective function*****constraint": [
        "$ARG1 that can be thought of as corresponding to a soft $ARG2"
    ],
    "other*****regularization": [
        "$ARG1 forms of $ARG2",
        "$ARG1 academic communities , L 2 $ARG2",
        "$ARG1 forms of $ARG2"
    ],
    "regularization*****multiple": [
        "$ARG1 , known as ensemble methods , combine $ARG2"
    ],
    "deep learning*****regularization": [
        "$ARG1 , most $ARG2",
        "$ARG1 of $ARG2"
    ],
    "regularization*****estimator": [
        "$ARG1 of an $ARG2"
    ],
    "1*****data generating process": [
        "$ARG1 ) excluded the true $ARG2"
    ],
    "data generating process*****bias": [
        "$ARG1 to under\ufb01tting and inducing $ARG2"
    ],
    "bias*****data generating process": [
        "$ARG1 , or ( 2 ) matched the true $ARG2"
    ],
    "data generating process*****process": [
        "$ARG1 , or ( 3 ) included the generating $ARG2"
    ],
    "regularization*****model": [
        "$ARG1 is to take a $ARG2",
        "$ARG1 on the $ARG2",
        "$ARG1 strategies of comparable power impose more severe restrictions on the architecture of the $ARG2",
        "$ARG1 e\ufb00ect compared to training the entire network as a single $ARG2"
    ],
    "deep learning*****data generating process": [
        "$ARG1 algorithms are to domains where the true $ARG2"
    ],
    "deep learning*****audio": [
        "$ARG1 algorithms are typically applied to extremely complicated domains such as images , $ARG2"
    ],
    "audio*****process": [
        "$ARG1 sequences and text , for which the true generation $ARG2"
    ],
    "model*****matter": [
        "$ARG1 is not a simple $ARG2"
    ],
    "matter*****model": [
        "$ARG1 of \ufb01nding the $ARG2"
    ],
    "logistic regression*****regularization": [
        "$ARG1 allow simple , straightforward , and e\ufb00ective $ARG2"
    ],
    "neural network*****linear regression": [
        "$ARG1 , $ARG2"
    ],
    "logistic regression*****norm": [
        "$ARG1 , by adding a parameter $ARG2"
    ],
    "norm*****objective function": [
        "$ARG1 penalty \u2126 ( \u03b8 ) to the $ARG2"
    ],
    "objective function*****hyperparameter": [
        "$ARG1 by J : J\u02dc ( \u03b8 ; X , y ) = J ( \u03b8 ; X , y ) + \u03b1\u2126 ( \u03b8 ) where \u03b1 \u2208 [ 0 , \u221e ) is a $ARG2"
    ],
    "hyperparameter*****weights": [
        "$ARG1 that $ARG2"
    ],
    "norm*****standard": [
        "$ARG1 penalty term , \u2126 , relative to the $ARG2"
    ],
    "algorithm*****objective function": [
        "$ARG1 minimizes the regularized $ARG2",
        "$ARG1 or $ARG2",
        "$ARG1 and $ARG2"
    ],
    "objective function*****measure": [
        "$ARG1 J\u02dc it will decrease both the original objective J on the training data and some $ARG2"
    ],
    "measure*****subset": [
        "$ARG1 of the size of the parameters \u03b8 ( or some $ARG2"
    ],
    "neural network*****norm": [
        "$ARG1 , we typically choose to use a parameter $ARG2"
    ],
    "norm*****weights": [
        "$ARG1 penalty \u2126 that penalizes only the $ARG2",
        "$ARG1 , then the $ARG2",
        "$ARG1 , then the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of each column separately prevents any one hidden unit from having very large $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "weights*****a\ufb03ne": [
        "$ARG1 of the $ARG2"
    ],
    "bias*****variable": [
        "$ARG1 controls only a single $ARG2"
    ],
    "vector*****neural network": [
        "$ARG1 \u03b8 denotes all of the parameters , including both w and the unregularized In the context of $ARG2"
    ],
    "search*****multiple": [
        "$ARG1 for the correct value of $ARG2"
    ],
    "hyperparameters*****weight decay": [
        "$ARG1 , it is still reasonable to use the same $ARG2",
        "$ARG1 such as $ARG2"
    ],
    "norm*****weight decay": [
        "$ARG1 penalty commonly known as $ARG2"
    ],
    "regularization*****weights": [
        "$ARG1 strategy drives the $ARG2",
        "$ARG1 encourages the parameters to go to regions of parameter space where small perturbations of the $ARG2",
        "$ARG1 , we examine a simple setting where the only parameters are linear $ARG2"
    ],
    "regularization*****ridge regression": [
        "$ARG1 is also known as $ARG2"
    ],
    "ridge regression*****weight decay": [
        "$ARG1 or We can gain some insight into the behavior of $ARG2"
    ],
    "regularization*****gradient": [
        "$ARG1 by studying the $ARG2",
        "$ARG1 contribution to the $ARG2"
    ],
    "gradient*****objective function": [
        "$ARG1 of the regularized $ARG2"
    ],
    "rule*****vector": [
        "$ARG1 to multiplicatively shrink the weight $ARG2"
    ],
    "vector*****constant": [
        "$ARG1 by a $ARG2"
    ],
    "objective function*****weights": [
        "$ARG1 in the neighborhood of the value of the $ARG2"
    ],
    "objective function*****linear regression": [
        "$ARG1 is truly quadratic , as in the case of \ufb01tting a $ARG2"
    ],
    "model*****regularization": [
        "$ARG1 with More generally , we could regularize the parameters to be near any speci\ufb01c point in space and , surprisingly , still get a $ARG2",
        "$ARG1 or improve $ARG2"
    ],
    "model*****special case": [
        "$ARG1 parameters towards zero , we will focus on this $ARG2"
    ],
    "weight decay*****eigenvector": [
        "$ARG1 is to rescale w \u2217 along the axes de\ufb01ned by the eigenvectors of H. Speci\ufb01cally , the component of w \u2217 that is aligned with the i-th $ARG2"
    ],
    "deep learning*****illustration": [
        "$ARG1 Figure 7.1 : An $ARG2",
        "$ARG1 Figure 7.4 : An $ARG2",
        "$ARG1 Figure 7.9 : $ARG2"
    ],
    "illustration*****weight decay": [
        "$ARG1 of the e\ufb00ect of L2 ( or $ARG2"
    ],
    "weight decay*****regularization": [
        "$ARG1 ) $ARG2",
        "$ARG1 controls the strength of the $ARG2",
        "$ARG1 coe\ufb03cients , or by adding $ARG2"
    ],
    "dimension*****objective function": [
        "$ARG1 , the $ARG2"
    ],
    "weight decay*****objective function": [
        "$ARG1 a\ufb00ects the position of w2 relatively Only directions along which the parameters contribute signi\ufb01cantly to reducing the $ARG2"
    ],
    "objective function*****eigenvalue": [
        "$ARG1 , a small $ARG2"
    ],
    "eigenvalue*****gradient": [
        "$ARG1 of the Hessian tells us that movement in this direction will not signi\ufb01cantly increase the $ARG2"
    ],
    "vector*****regularization": [
        "$ARG1 corresponding to such unimportant directions are decayed away through the use of the $ARG2"
    ],
    "weight decay*****optimization": [
        "$ARG1 in terms of its e\ufb00ect on the $ARG2"
    ],
    "machine learning*****linear regression": [
        "$ARG1 in particular ? We can \ufb01nd out by studying $ARG2",
        "$ARG1 , including $ARG2"
    ],
    "special case*****solution": [
        "$ARG1 of the same results , but with the $ARG2"
    ],
    "linear regression*****cost function": [
        "$ARG1 , the $ARG2"
    ],
    "normal equations*****solution": [
        "$ARG1 for the $ARG2"
    ],
    "covariance matrix*****regularization": [
        "$ARG1 m Using L2 $ARG2"
    ],
    "regularization*****matrix": [
        "$ARG1 replaces this $ARG2"
    ],
    "matrix*****variance": [
        "$ARG1 correspond to the $ARG2"
    ],
    "variance*****feature": [
        "$ARG1 of each input $ARG2"
    ],
    "weights*****covariance": [
        "$ARG1 on features whose $ARG2"
    ],
    "covariance*****variance": [
        "$ARG1 with the output target is low compared to this added $ARG2",
        "$ARG1 give the $ARG2"
    ],
    "option*****1": [
        "$ARG1 is to use L $ARG2"
    ],
    "model*****absolute value": [
        "$ARG1 parameter w is de\ufb01ned as : \u2126 ( \u03b8 ) = ||w||1 = that is , as the sum of $ARG2"
    ],
    "absolute value*****regularization": [
        "$ARG1 of the individual parameters.2 We will now discuss the e\ufb00ect of L1 $ARG2"
    ],
    "regularization*****linear regression": [
        "$ARG1 on the simple $ARG2"
    ],
    "bias parameter*****regularization": [
        "$ARG1 , that we studied in our analysis of L2 $ARG2"
    ],
    "regularization*****1": [
        "$ARG1 would zero , but instead towards some parameter value introduce the term \u2126 ( \u03b8 ) = ||w \u2212 w ( o ) || $ARG2"
    ],
    "gradient*****constant": [
        "$ARG1 no longer scales linearly with each wi ; instead it is a $ARG2"
    ],
    "gradient*****linear model": [
        "$ARG1 is that we will not necessarily see clean algebraic solutions to quadratic approximations of J ( X , y ; w ) as we did for L 2 Our simple $ARG2"
    ],
    "cost function*****series": [
        "$ARG1 that we can represent via its Taylor $ARG2"
    ],
    "assumption*****diagonal": [
        "$ARG1 that the Hessian is $ARG2",
        "$ARG1 of a $ARG2"
    ],
    "assumption*****linear regression": [
        "$ARG1 holds if the data for the $ARG2"
    ],
    "problem*****correlation": [
        "$ARG1 has been preprocessed to remove all $ARG2"
    ],
    "correlation*****pca": [
        "$ARG1 between the input features , which may be accomplished using $ARG2"
    ],
    "problem*****cost function": [
        "$ARG1 of minimizing this approximate $ARG2"
    ],
    "solution*****dimension": [
        "$ARG1 ( for each $ARG2"
    ],
    "regularization*****distance": [
        "$ARG1 does not move the optimal value of wi to zero but instead it just shifts it in that direction by a $ARG2"
    ],
    "regularization*****solution": [
        "$ARG1 results in a $ARG2"
    ],
    "solution*****regularization": [
        "$ARG1 w\u0303 for L2 $ARG2"
    ],
    "diagonal*****positive de\ufb01nite": [
        "$ARG1 and $ARG2"
    ],
    "positive de\ufb01nite*****regularization": [
        "$ARG1 Hessian H that we introduced for our analysis of L1 $ARG2"
    ],
    "property*****1": [
        "$ARG1 induced by L $ARG2"
    ],
    "regularization*****feature selection": [
        "$ARG1 has been used extensively as a $ARG2"
    ],
    "feature selection*****machine learning": [
        "$ARG1 simpli\ufb01es a $ARG2"
    ],
    "model*****linear model": [
        "$ARG1 integrates an L1 penalty with a $ARG2"
    ],
    "1*****subset": [
        "$ARG1 penalty causes a $ARG2"
    ],
    "subset*****weights": [
        "$ARG1 of the $ARG2"
    ],
    "regularization*****cost function": [
        "$ARG1 , the penalty \u03b1\u2126 ( w ) = \u03b1 i |wi | used to regularize a $ARG2",
        "$ARG1 without requiring the addition of penalty terms to the $ARG2"
    ],
    "cost function*****inference": [
        "$ARG1 is equivalent to the log-prior term that is maximized by MAP Bayesian $ARG2"
    ],
    "inference*****isotropic": [
        "$ARG1 when the prior is an $ARG2"
    ],
    "norm*****constrained optimization": [
        "$ARG1 Penalties as $ARG2"
    ],
    "constrained optimization*****cost function": [
        "$ARG1 Consider the $ARG2"
    ],
    "cost function*****norm": [
        "$ARG1 regularized by a parameter $ARG2"
    ],
    "recall*****function": [
        "$ARG1 from section 4.4 that we can minimize a $ARG2"
    ],
    "function*****generalized lagrange function": [
        "$ARG1 subject to constraints by constructing a $ARG2"
    ],
    "generalized lagrange function*****objective function": [
        "$ARG1 , consisting of the original $ARG2"
    ],
    "karush\u2013kuhn\u2013tucker*****function": [
        "$ARG1 ( KKT ) multiplier , and a $ARG2"
    ],
    "constant*****generalized lagrange function": [
        "$ARG1 k , we could construct a $ARG2"
    ],
    "linear regression*****constraint": [
        "$ARG1 with an L 2 $ARG2"
    ],
    "constraint*****view": [
        "$ARG1 , we can \ufb01x \u03b1 \u2217 and $ARG2"
    ],
    "view*****problem": [
        "$ARG1 the $ARG2"
    ],
    "norm*****constraint": [
        "$ARG1 penalty as imposing a $ARG2",
        "$ARG1 limitation is always implemented as an explicit $ARG2"
    ],
    "constraint*****weights": [
        "$ARG1 on the $ARG2"
    ],
    "constraint*****weight decay": [
        "$ARG1 region that we impose by using $ARG2"
    ],
    "weight decay*****principle": [
        "$ARG1 with coe\ufb03cient \u03b1\u2217 because the value of \u03b1\u2217 does not directly tell us the value of k. In $ARG2"
    ],
    "weights*****constraint": [
        "$ARG1 become large and attempt to leave the $ARG2"
    ],
    "constraint*****optimization": [
        "$ARG1 Finally , explicit constraints with reprojection can be useful because they impose some stability on the $ARG2"
    ],
    "learning rate*****loop": [
        "$ARG1 , it is possible to encounter a positive feedback $ARG2"
    ],
    "loop*****weights": [
        "$ARG1 in which large $ARG2"
    ],
    "loop*****magnitude": [
        "$ARG1 from continuing to increase the $ARG2"
    ],
    "magnitude*****weights": [
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "learning rate*****exploration": [
        "$ARG1 to allow rapid $ARG2"
    ],
    "norm*****matrix": [
        "$ARG1 of each column of the weight $ARG2"
    ],
    "deep learning*****frobenius norm": [
        "$ARG1 of a neural net layer , rather than constraining the $ARG2"
    ],
    "constraint*****function": [
        "$ARG1 into a penalty in a Lagrange $ARG2"
    ],
    "function*****weight decay": [
        "$ARG1 , it would be similar to L2 $ARG2"
    ],
    "linear model*****machine learning": [
        "$ARG1 in $ARG2"
    ],
    "linear regression*****pca": [
        "$ARG1 and $ARG2"
    ],
    "pca*****matrix": [
        "$ARG1 , depend on inverting the $ARG2"
    ],
    "matrix*****data generating distribution": [
        "$ARG1 can be singular whenever the $ARG2"
    ],
    "data generating distribution*****variance": [
        "$ARG1 truly has no $ARG2"
    ],
    "logistic regression*****problem": [
        "$ARG1 applied to a $ARG2"
    ],
    "vector*****classi\ufb01cation": [
        "$ARG1 w is able to achieve perfect $ARG2"
    ],
    "stochastic gradient descent*****magnitude": [
        "$ARG1 will continually increase the $ARG2"
    ],
    "gradient descent*****weights": [
        "$ARG1 will eventually reach su\ufb03ciently large $ARG2"
    ],
    "regularization*****convergence": [
        "$ARG1 are able to guarantee the $ARG2"
    ],
    "weight decay*****gradient descent": [
        "$ARG1 will cause $ARG2"
    ],
    "gradient descent*****magnitude": [
        "$ARG1 to quit increasing the $ARG2"
    ],
    "weights*****slope": [
        "$ARG1 when the $ARG2"
    ],
    "slope*****weight decay": [
        "$ARG1 of the likelihood is equal to the $ARG2"
    ],
    "deep learning*****moore-penrose pseudoinverse": [
        "$ARG1 the $ARG2"
    ],
    "recall*****matrix": [
        "$ARG1 that one de\ufb01nition of the pseudoinverse X + of a $ARG2"
    ],
    "task*****density estimation": [
        "$ARG1 unless we have already solved the $ARG2"
    ],
    "dataset augmentation*****technique": [
        "$ARG1 has been a particularly e\ufb00ective $ARG2"
    ],
    "technique*****classi\ufb01cation": [
        "$ARG1 for a speci\ufb01c $ARG2"
    ],
    "problem*****object recognition": [
        "$ARG1 : $ARG2",
        "$ARG1 falls into an \u201c AI-complete \u201d category like $ARG2"
    ],
    "invariant*****convolution": [
        "$ARG1 by using the $ARG2"
    ],
    "deep learning*****invariant": [
        "$ARG1 There are also transformations that we would like our classi\ufb01ers to be $ARG2"
    ],
    "dataset augmentation*****speech recognition": [
        "$ARG1 is e\ufb00ective for $ARG2"
    ],
    "unsupervised learning*****denoising autoencoder": [
        "$ARG1 algorithms such as the $ARG2"
    ],
    "dataset augmentation*****multiple": [
        "$ARG1 at $ARG2"
    ],
    "multiple*****abstraction": [
        "$ARG1 levels of $ARG2"
    ],
    "dropout*****regularization": [
        "$ARG1 , a powerful $ARG2",
        "$ARG1 is a $ARG2",
        "$ARG1 and larger models may outweigh the bene\ufb01t of $ARG2",
        "$ARG1 boosting show almost no $ARG2"
    ],
    "regularization*****process": [
        "$ARG1 strategy that will be described in section 7.12 , can be seen as a $ARG2"
    ],
    "machine learning*****dataset augmentation": [
        "$ARG1 benchmark results , it is important to take the e\ufb00ect of $ARG2",
        "$ARG1 algorithms that inject noise into the input are performing a form of $ARG2"
    ],
    "algorithm*****dataset augmentation": [
        "$ARG1 B , it is necessary to make sure that both algorithms were evaluated using the same hand-designed $ARG2",
        "$ARG1 A performs poorly with no $ARG2"
    ],
    "dataset augmentation*****algorithm": [
        "$ARG1 and $ARG2"
    ],
    "gaussian noise*****machine learning": [
        "$ARG1 to the input ) are considered part of the $ARG2"
    ],
    "algorithm*****image": [
        "$ARG1 , while operations that are speci\ufb01c to one application domain ( such as randomly cropping an $ARG2"
    ],
    "deep learning*****dataset augmentation": [
        "$ARG1 Section 7.4 has motivated the use of noise applied to the inputs as a $ARG2"
    ],
    "variance*****model": [
        "$ARG1 at the input of the $ARG2"
    ],
    "weights*****bishop": [
        "$ARG1 ( $ARG2"
    ],
    "technique*****recurrent neural network": [
        "$ARG1 has been used primarily in the context of $ARG2"
    ],
    "stochastic*****inference": [
        "$ARG1 implementation of Bayesian $ARG2"
    ],
    "weights*****probability distribution": [
        "$ARG1 to be uncertain and representable via a $ARG2"
    ],
    "stochastic*****weights": [
        "$ARG1 way to re\ufb02ect Noise applied to the $ARG2",
        "$ARG1 approaches to training exponentially large ensembles of models that share $ARG2"
    ],
    "regularization*****function": [
        "$ARG1 , encouraging stability of the $ARG2",
        "$ARG1 term collapses into \u03b7Ep ( x ) \ue06bx\ue06b2 , which is not a $ARG2"
    ],
    "set*****scalar": [
        "$ARG1 of features x to a $ARG2"
    ],
    "scalar*****cost function": [
        "$ARG1 using the least-squares $ARG2"
    ],
    "covariance*****regularization": [
        "$ARG1 \u03b7I ) is equivalent to minimization of J with an additional $ARG2"
    ],
    "linear regression*****regularization": [
        "$ARG1 ( where , for instance , y\u0302 ( x ) = w \ue03ex + b ) , this $ARG2"
    ],
    "example*****constant": [
        "$ARG1 , we can assume that for some small $ARG2"
    ],
    "constant*****set": [
        "$ARG1 \ue00f , the training $ARG2"
    ],
    "assumption*****cost function": [
        "$ARG1 is easy to incorporate into the $ARG2"
    ],
    "example*****label smoothing": [
        "$ARG1 , $ARG2"
    ],
    "label smoothing*****model": [
        "$ARG1 regularizes a $ARG2"
    ],
    "learning*****softmax": [
        "$ARG1 with a $ARG2"
    ],
    "1*****weights": [
        "$ARG1 , so it will continue to learn larger and larger $ARG2"
    ],
    "label smoothing*****classi\ufb01cation": [
        "$ARG1 has the advantage of preventing the pursuit of hard probabilities without discouraging correct $ARG2"
    ],
    "semi-supervised learning*****deep learning": [
        "$ARG1 , both unlabeled examples from P ( x ) and labeled examples from P ( x , y ) are used to estimate P ( y | x ) or predict y from In the context of $ARG2"
    ],
    "deep learning*****semi-supervised learning": [
        "$ARG1 , $ARG2"
    ],
    "semi-supervised learning*****learning": [
        "$ARG1 usually refers to $ARG2"
    ],
    "unsupervised learning*****group": [
        "$ARG1 can provide useful cues for how to $ARG2"
    ],
    "principal components analysis*****model": [
        "$ARG1 as a pre-processing step before applying a classi\ufb01er ( on the projected Instead of having separate unsupervised and supervised components in the $ARG2"
    ],
    "solution*****supervised learning": [
        "$ARG1 to the $ARG2"
    ],
    "learning*****kernel": [
        "$ARG1 the $ARG2"
    ],
    "information*****semi-supervised learning": [
        "$ARG1 about $ARG2"
    ],
    "multi-task learning*****generalization": [
        "$ARG1 ( Caruana , 1993 ) is a way to improve $ARG2"
    ],
    "generalization*****pooling": [
        "$ARG1 by $ARG2"
    ],
    "model*****multi-task learning": [
        "$ARG1 is more constrained towards good values ( assuming the sharing is justi\ufb01ed ) , often yielding Figure 7.2 illustrates a very common form of $ARG2"
    ],
    "task*****generalization": [
        "$ARG1 to achieve good $ARG2"
    ],
    "1*****multi-task learning": [
        "$ARG1 ) h ( 2 ) h ( 3 ) h ( shared ) Figure 7.2 : $ARG2"
    ],
    "multi-task learning*****deep learning": [
        "$ARG1 can be cast in several ways in $ARG2"
    ],
    "assumption*****task": [
        "$ARG1 is that there exists a common pool of factors that explain the variations in the input x , while each $ARG2"
    ],
    "task*****1": [
        "$ARG1 ( respectively predicting y ( $ARG2"
    ],
    "unsupervised learning*****1": [
        "$ARG1 context , it makes sense for some of the top-level factors to be associated with none of the output tasks ( h ( 3 ) ) : these are the factors that explain some of the input variations but are not relevant for predicting y ( $ARG2"
    ],
    "learning*****loss": [
        "$ARG1 curves showing how the negative log-likelihood $ARG2"
    ],
    "representational capacity*****task": [
        "$ARG1 to over\ufb01t the $ARG2"
    ],
    "set*****point in time": [
        "$ARG1 error ) by returning to the parameter setting at the $ARG2"
    ],
    "point in time*****set": [
        "$ARG1 with the lowest validation $ARG2"
    ],
    "algorithm*****early stopping": [
        "$ARG1 7.1 The $ARG2",
        "$ARG1 7.2 A meta-algorithm for using $ARG2",
        "$ARG1 7.3 Meta-algorithm using $ARG2"
    ],
    "number*****early stopping": [
        "$ARG1 of training steps is i\u2217 This strategy is known as $ARG2",
        "$ARG1 of steps as the $ARG2"
    ],
    "early stopping*****hyperparameter": [
        "$ARG1 is as a very e\ufb03cient $ARG2"
    ],
    "hyperparameter*****algorithm": [
        "$ARG1 selection $ARG2"
    ],
    "number*****hyperparameter": [
        "$ARG1 of training steps is just another $ARG2"
    ],
    "early stopping*****e\ufb00ective capacity": [
        "$ARG1 , we are controlling the $ARG2"
    ],
    "e\ufb00ective capacity*****model": [
        "$ARG1 of the $ARG2",
        "$ARG1 of a $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "hyperparameters*****process": [
        "$ARG1 must be chosen using an expensive guess and check $ARG2"
    ],
    "set*****hyperparameter": [
        "$ARG1 a $ARG2",
        "$ARG1 of values for each individual $ARG2",
        "$ARG1 of values for each $ARG2",
        "$ARG1 error , then propose new $ARG2",
        "$ARG1 error for each $ARG2"
    ],
    "hyperparameter*****early stopping": [
        "$ARG1 automatically via $ARG2"
    ],
    "early stopping*****set": [
        "$ARG1 is running the validation $ARG2",
        "$ARG1 requires a validation $ARG2",
        "$ARG1 typically involves monitoring the validation $ARG2"
    ],
    "process*****gpu": [
        "$ARG1 on a separate machine , separate CPU , or separate $ARG2"
    ],
    "gpu*****process": [
        "$ARG1 from the main training $ARG2"
    ],
    "memory*****example": [
        "$ARG1 ( for $ARG2"
    ],
    "early stopping*****regularization": [
        "$ARG1 is a very unobtrusive form of $ARG2",
        "$ARG1 is a $ARG2",
        "$ARG1 automatically determines the correct amount of $ARG2"
    ],
    "regularization*****change": [
        "$ARG1 , in that it requires almost no $ARG2"
    ],
    "change*****objective function": [
        "$ARG1 in the underlying training procedure , the $ARG2"
    ],
    "early stopping*****learning": [
        "$ARG1 without damaging the $ARG2"
    ],
    "contrast*****weight decay": [
        "$ARG1 to $ARG2"
    ],
    "weight decay*****solution": [
        "$ARG1 and trap the network in a bad local minimum corresponding to a $ARG2"
    ],
    "solution*****early stopping": [
        "$ARG1 with pathologically $ARG2"
    ],
    "early stopping*****other": [
        "$ARG1 may be used either alone or in conjunction with $ARG2"
    ],
    "objective function*****generalization": [
        "$ARG1 to encourage better $ARG2"
    ],
    "pass*****number": [
        "$ARG1 , we train for the same $ARG2"
    ],
    "early stopping*****pass": [
        "$ARG1 procedure determined was optimal in the \ufb01rst $ARG2"
    ],
    "pass*****dataset": [
        "$ARG1 through the $ARG2"
    ],
    "early stopping*****algorithm": [
        "$ARG1 ( $ARG2",
        "$ARG1 ( $ARG2"
    ],
    "loss function*****set": [
        "$ARG1 on the validation $ARG2"
    ],
    "set*****early stopping": [
        "$ARG1 objective at which the $ARG2"
    ],
    "reduction*****number": [
        "$ARG1 in cost due to limiting the $ARG2"
    ],
    "early stopping*****model": [
        "$ARG1 regularizes the $ARG2"
    ],
    "model*****bishop": [
        "$ARG1 ? $ARG2"
    ],
    "bishop*****early stopping": [
        "$ARG1 ( 1995a ) and Sj\u00f6berg and Ljung ( 1995 ) argued that $ARG2"
    ],
    "early stopping*****optimization": [
        "$ARG1 has the e\ufb00ect of restricting the $ARG2"
    ],
    "optimization*****learning rate": [
        "$ARG1 steps ( corresponding to \u03c4 training iterations ) and with $ARG2"
    ],
    "view*****measure": [
        "$ARG1 the product \ue00f\u03c4 as a $ARG2"
    ],
    "measure*****e\ufb00ective capacity": [
        "$ARG1 of $ARG2"
    ],
    "linear model*****error function": [
        "$ARG1 with a quadratic $ARG2"
    ],
    "error function*****gradient descent": [
        "$ARG1 and simple $ARG2"
    ],
    "gradient descent*****regularization": [
        "$ARG1 stopping is equivalent to L 2 In order to compare with classical L2 $ARG2"
    ],
    "weights*****hessian matrix": [
        "$ARG1 w\u2217 : J\u02c6 ( \u03b8 ) = J ( w \u2217 ) + ( w \u2212 w\u2217 ) \ue03e H ( w \u2212 w\u2217 ) , where H is the $ARG2"
    ],
    "series*****gradient": [
        "$ARG1 approximation , the $ARG2"
    ],
    "illustration*****early stopping": [
        "$ARG1 of the e\ufb00ect of $ARG2"
    ],
    "illustration*****regularization": [
        "$ARG1 of the e\ufb00ect of L 2 $ARG2"
    ],
    "eigendecomposition*****diagonal matrix": [
        "$ARG1 of H : H = Q\u039bQ \ue03e , where \u039b is a $ARG2"
    ],
    "neural network*****symmetry": [
        "$ARG1 , to obtain $ARG2"
    ],
    "regularization*****hyperparameters": [
        "$ARG1 can be rearranged as : Q\ue03e w\u0303 = ( \u039b + \u03b1I ) \u22121\u039bQ \ue03ew \u2217 Q\ue03e w\u0303 = [ I \u2212 ( \u039b + \u03b1I ) \u22121 \u03b1 ] Q\ue03ew \u2217 Comparing equation 7.40 and equation 7.42 , we see that if the $ARG2",
        "$ARG1 , by adjusting $ARG2"
    ],
    "hyperparameters*****regularization": [
        "$ARG1 \ue00f , \u03b1 , and \u03c4 are chosen such that ( I \u2212 \ue00f\u039b ) \u03c4 = ( \u039b + \u03b1I ) \u22121 \u03b1 , then L 2 $ARG2"
    ],
    "regularization*****early stopping": [
        "$ARG1 and $ARG2"
    ],
    "early stopping*****objective function": [
        "$ARG1 can be seen to be equivalent ( at least under the quadratic approximation of the $ARG2"
    ],
    "series*****1": [
        "$ARG1 expansion for log ( $ARG2"
    ],
    "weight decay*****early stopping": [
        "$ARG1 that $ARG2"
    ],
    "weight decay*****hyperparameter": [
        "$ARG1 requires many training experiments with di\ufb00erent values of its $ARG2"
    ],
    "parameter tying*****parameter sharing": [
        "$ARG1 and $ARG2"
    ],
    "weight decay*****model": [
        "$ARG1 ) penalizes $ARG2",
        "$ARG1 acts by placing a penalty directly on the $ARG2",
        "$ARG1 coe\ufb03cient frees the $ARG2"
    ],
    "method*****regularization": [
        "$ARG1 of $ARG2"
    ],
    "regularization*****parameter sharing": [
        "$ARG1 is often referred to as $ARG2"
    ],
    "parameter sharing*****norm": [
        "$ARG1 over regularizing the parameters to be close ( via a $ARG2"
    ],
    "norm*****subset": [
        "$ARG1 penalty ) is that only a $ARG2"
    ],
    "convolutional neural network*****reduction": [
        "$ARG1 can lead to signi\ufb01cant $ARG2"
    ],
    "convolutional neural network*****parameter sharing": [
        "$ARG1 By far the most popular and extensive use of $ARG2"
    ],
    "parameter sharing*****convolutional neural network": [
        "$ARG1 occurs in $ARG2"
    ],
    "convolutional neural network*****computer vision": [
        "$ARG1 ( CNNs ) applied to $ARG2"
    ],
    "natural image*****invariant": [
        "$ARG1 have many statistical properties that are $ARG2"
    ],
    "property*****multiple": [
        "$ARG1 into account by sharing parameters across $ARG2"
    ],
    "view*****1": [
        "$ARG1 of this distinction can be illustrated in the context of 4 0 0 \u22122 0 \uf8ef 0 0 \u22121 0 \uf8fa \uf8ef 3 \uf8fa \uf8fa \uf8ef \u22122 \uf8fa \uf8ef 15 \uf8fa = \uf8ef 0 5 0 \uf8fa \uf8ef \u22125 \uf8fa \uf8f0 $ARG2"
    ],
    "linear regression*****sparse representation": [
        "$ARG1 with a $ARG2"
    ],
    "function*****information": [
        "$ARG1 of x that , in some sense , represents the $ARG2"
    ],
    "information*****vector": [
        "$ARG1 present in x , but does so with a sparse $ARG2",
        "$ARG1 THEORY parametrized by a $ARG2"
    ],
    "norm*****regularization": [
        "$ARG1 penalty $ARG2",
        "$ARG1 penalty term , with larger values of \u03b1 corresponding to more $ARG2"
    ],
    "regularization*****loss function": [
        "$ARG1 of representations is performed by adding to the $ARG2"
    ],
    "loss function*****weights": [
        "$ARG1 by J : J\u02dc ( \u03b8 ; X , y ) = J ( \u03b8 ; X , y ) + \u03b1\u2126 ( h ) where \u03b1 \u2208 [ 0 , \u221e ) $ARG2"
    ],
    "1*****sparse representation": [
        "$ARG1 penalty is only one choice of penalty that can result in a $ARG2"
    ],
    "average*****vector": [
        "$ARG1 activation across several examples , m1 i h ( i ) , to be near some target value , such as a $ARG2"
    ],
    "other*****constraint": [
        "$ARG1 approaches obtain representational sparsity with a hard $ARG2"
    ],
    "example*****orthogonal matching pursuit": [
        "$ARG1 , $ARG2"
    ],
    "orthogonal matching pursuit*****number": [
        "$ARG1 ( Pati et al. , 1993 ) encodes an input x with the representation h that solves the constrained arg min \ue06bx \u2212 W h\ue06b2 , h , \ue06bh\ue06b0 < k where \ue06bh\ue06b0 is the $ARG2"
    ],
    "omp-k*****number": [
        "$ARG1 with the value of k speci\ufb01ed to indicate the $ARG2"
    ],
    "regularization*****bagging": [
        "$ARG1 used in a variety of $ARG2"
    ],
    "bagging*****other": [
        "$ARG1 and $ARG2"
    ],
    "other*****bagging": [
        "$ARG1 Ensemble Methods $ARG2"
    ],
    "bagging*****technique": [
        "$ARG1 ( short for bootstrap aggregating ) is a $ARG2"
    ],
    "technique*****generalization": [
        "$ARG1 for reducing $ARG2"
    ],
    "machine learning*****model averaging": [
        "$ARG1 called $ARG2",
        "$ARG1 contests are usually won by methods using $ARG2"
    ],
    "model averaging*****set": [
        "$ARG1 works is that di\ufb00erent models will usually not make all the same errors on the test $ARG2"
    ],
    "set*****regression model": [
        "$ARG1 of k $ARG2"
    ],
    "example*****multivariate normal distribution": [
        "$ARG1 , with the errors drawn from a zero-mean $ARG2"
    ],
    "multivariate normal distribution*****average": [
        "$ARG1 with variances E [ \ue00f 2i ] = v and covariances E [ \ue00fi \ue00fj ] = Then the error made by the $ARG2"
    ],
    "expected*****mean squared error": [
        "$ARG1 squared error of the ensemble predictor is \ue00fi \uf8fb = 2 E \uf8f0 \ue00fi \ue00f j \uf8f8\uf8fb In the case where the errors are perfectly correlated and c = v , the $ARG2"
    ],
    "mean squared error*****model averaging": [
        "$ARG1 reduces to v , so the $ARG2"
    ],
    "other*****average": [
        "$ARG1 words , on $ARG2"
    ],
    "deep learning*****bagging": [
        "$ARG1 Figure 7.5 : A cartoon depiction of how $ARG2"
    ],
    "8*****dataset": [
        "$ARG1 detector on the $ARG2"
    ],
    "dataset*****8": [
        "$ARG1 depicted above , containing an $ARG2",
        "$ARG1 omits the 9 and repeats the $ARG2"
    ],
    "dataset*****loop": [
        "$ARG1 , the detector learns that a $ARG2"
    ],
    "loop*****8": [
        "$ARG1 on top of the digit corresponds to an $ARG2",
        "$ARG1 on the bottom of the digit corresponds to an $ARG2"
    ],
    "average*****8": [
        "$ARG1 their output then the detector is robust , achieving maximal con\ufb01dence only when both loops of the $ARG2"
    ],
    "bagging*****method": [
        "$ARG1 is a $ARG2"
    ],
    "method*****model": [
        "$ARG1 that allows the same kind of $ARG2"
    ],
    "probability*****dataset": [
        "$ARG1 , each $ARG2"
    ],
    "contains*****average": [
        "$ARG1 several duplicate examples ( on $ARG2"
    ],
    "average*****dataset": [
        "$ARG1 around 2/3 of the examples from the original $ARG2"
    ],
    "neural network*****solution": [
        "$ARG1 reach a wide enough variety of $ARG2"
    ],
    "solution*****model averaging": [
        "$ARG1 points that they can often bene\ufb01t from $ARG2"
    ],
    "model averaging*****dataset": [
        "$ARG1 even if all of the models are trained on the same $ARG2"
    ],
    "initialization*****hyperparameters": [
        "$ARG1 , random selection of minibatches , di\ufb00erences in $ARG2"
    ],
    "hyperparameters*****neural network": [
        "$ARG1 , or di\ufb00erent outcomes of non-deterministic implementations of $ARG2"
    ],
    "model averaging*****method": [
        "$ARG1 is an extremely powerful and reliable $ARG2"
    ],
    "method*****generalization": [
        "$ARG1 for reducing $ARG2"
    ],
    "algorithm*****model averaging": [
        "$ARG1 can bene\ufb01t substantially from $ARG2"
    ],
    "model averaging*****memory": [
        "$ARG1 at the price of increased computation and $ARG2"
    ],
    "example*****net\ufb02ix grand prize": [
        "$ARG1 is the $ARG2"
    ],
    "dropout*****method": [
        "$ARG1 ( Srivastava et al. , 2014 ) provides a computationally inexpensive but powerful $ARG2",
        "$ARG1 can be thought of as a $ARG2",
        "$ARG1 remains the most widely used implicit ensemble $ARG2"
    ],
    "method*****bagging": [
        "$ARG1 of making $ARG2"
    ],
    "bagging*****neural network": [
        "$ARG1 practical for ensembles of very many large $ARG2"
    ],
    "bagging*****multiple": [
        "$ARG1 involves training $ARG2"
    ],
    "dropout*****base": [
        "$ARG1 trains the ensemble consisting of all sub-networks that can be formed by removing non-output units from an underlying $ARG2",
        "$ARG1 trains an ensemble consisting of all sub-networks that can be constructed by removing non-output units from an underlying $ARG2"
    ],
    "neural network*****series": [
        "$ARG1 , based on a $ARG2"
    ],
    "series*****a\ufb03ne": [
        "$ARG1 of $ARG2"
    ],
    "algorithm*****multiplication": [
        "$ARG1 in terms of $ARG2"
    ],
    "multiplication*****other": [
        "$ARG1 by zero for simplicity , but it can be trivially modi\ufb01ed to work with $ARG2"
    ],
    "other*****recall": [
        "$ARG1 operations that remove a unit from the $ARG2"
    ],
    "recall*****bagging": [
        "$ARG1 that to learn with $ARG2"
    ],
    "bagging*****set": [
        "$ARG1 , we de\ufb01ne k di\ufb00erent models , construct k di\ufb00erent datasets by sampling from the training $ARG2"
    ],
    "dropout*****process": [
        "$ARG1 aims to approximate this $ARG2"
    ],
    "process*****number": [
        "$ARG1 , but with an exponentially large $ARG2"
    ],
    "dropout*****learning": [
        "$ARG1 , we use a minibatch-based $ARG2"
    ],
    "probability*****hyperparameter": [
        "$ARG1 of sampling a mask value of one ( causing a unit to be included ) is a $ARG2"
    ],
    "forward propagation*****vector": [
        "$ARG1 More formally , suppose that a mask $ARG2"
    ],
    "vector*****model": [
        "$ARG1 \u00b5 speci\ufb01es which units to include , and J ( \u03b8 , \u00b5 ) de\ufb01nes the cost of the $ARG2"
    ],
    "contains*****unbiased": [
        "$ARG1 exponentially many terms but we can obtain an $ARG2"
    ],
    "dropout*****bagging": [
        "$ARG1 training is not quite the same as $ARG2",
        "$ARG1 follows the $ARG2",
        "$ARG1 is analogous to $ARG2",
        "$ARG1 as $ARG2",
        "$ARG1 as $ARG2",
        "$ARG1 purely as a means of performing e\ufb03cient , approximate $ARG2"
    ],
    "model*****subset": [
        "$ARG1 inheriting a di\ufb00erent $ARG2"
    ],
    "bagging*****model": [
        "$ARG1 , each $ARG2",
        "$ARG1 , each $ARG2"
    ],
    "model*****convergence": [
        "$ARG1 is trained to $ARG2"
    ],
    "deep learning*****dropout": [
        "$ARG1 Figure 7.6 : $ARG2"
    ],
    "problem*****probability": [
        "$ARG1 becomes insigni\ufb01cant for networks with wider layers , where the $ARG2"
    ],
    "example*****forward propagation": [
        "$ARG1 of $ARG2"
    ],
    "forward propagation*****dropout": [
        "$ARG1 through a feedforward network using $ARG2",
        "$ARG1 with $ARG2"
    ],
    "dropout*****vector": [
        "$ARG1 , we randomly sample a $ARG2",
        "$ARG1 , each sub-model de\ufb01ned by mask $ARG2"
    ],
    "1*****hyperparameter": [
        "$ARG1 is a $ARG2"
    ],
    "hyperparameter*****hidden layer": [
        "$ARG1 , usually 0.5 for the $ARG2"
    ],
    "bagging*****dropout": [
        "$ARG1 and $ARG2"
    ],
    "arithmetic mean*****1": [
        "$ARG1 of all of these distributions , $ARG2"
    ],
    "arithmetic mean*****probability distribution": [
        "$ARG1 over all masks is given p ( \u00b5 ) p ( y | x , \u00b5 ) where p ( \u00b5 ) is the $ARG2"
    ],
    "change*****arithmetic mean": [
        "$ARG1 to using the geometric mean rather than the $ARG2"
    ],
    "unnormalized probability distribution*****number": [
        "$ARG1 de\ufb01ned directly by the geometric mean is given by p\u0303ensemble ( y | x ) = 2d p ( y | x , \u00b5 ) where d is the $ARG2"
    ],
    "weights*****probability": [
        "$ARG1 going out of unit i multiplied by the $ARG2"
    ],
    "accuracy*****approximate inference": [
        "$ARG1 of this $ARG2"
    ],
    "rule*****weights": [
        "$ARG1 usually amounts to dividing the $ARG2"
    ],
    "expected*****average": [
        "$ARG1 total input to that unit at train time , even though half the units at train time are missing on $ARG2"
    ],
    "example*****softmax": [
        "$ARG1 , consider a $ARG2",
        "$ARG1 , classi\ufb01ers based on a $ARG2"
    ],
    "multiplication*****vector": [
        "$ARG1 of the input with a binary $ARG2"
    ],
    "deep learning*****rule": [
        "$ARG1 To see that the weight scaling $ARG2"
    ],
    "rule*****softmax": [
        "$ARG1 is exact , we can simplify P\u02dcensemble : P ( y = y | v ; d ) P\u0303ensemble ( y = y | v ) = 2n d\u2208 { 0,1 } n d\u2208 { 0,1 } n d\u2208 { 0,1 } n $ARG2"
    ],
    "softmax*****multiplication": [
        "$ARG1 ( W \ue03e ( d \ue00c v ) + b ) y \ue03e ( d \ue00c v ) + b exp Wy , : y \ue030 exp W y\ue030 , : ( d \ue00c v ) + b y \ue030 \ue03e ( d \ue00c v ) + b exp Wy , : d\u2208 { 0,1 } n y \ue030 exp W y\ue030 , : ( d \ue00c v ) + by \ue030 d\u2208 { 0,1 } n Because P\u0303 will be normalized , we can safely ignore $ARG2"
    ],
    "multiplication*****constant": [
        "$ARG1 by factors that are $ARG2"
    ],
    "softmax*****weights": [
        "$ARG1 classi\ufb01er with $ARG2"
    ],
    "weights*****rule": [
        "$ARG1 The weight scaling $ARG2"
    ],
    "rule*****other": [
        "$ARG1 is also exact in $ARG2"
    ],
    "other*****hidden layer": [
        "$ARG1 settings , including regression networks with conditionally normal outputs , and deep networks that have $ARG2"
    ],
    "dropout*****other": [
        "$ARG1 is more e\ufb00ective than $ARG2",
        "$ARG1 may also be combined with $ARG2",
        "$ARG1 has inspired $ARG2"
    ],
    "standard*****weight decay": [
        "$ARG1 computationally inexpensive regularizers , such as $ARG2"
    ],
    "weight decay*****norm": [
        "$ARG1 , \ufb01lter $ARG2"
    ],
    "norm*****activity": [
        "$ARG1 constraints and sparse $ARG2"
    ],
    "dropout*****example": [
        "$ARG1 during training requires only O ( n ) computation per $ARG2"
    ],
    "model*****dropout": [
        "$ARG1 has the same cost per-example as if $ARG2",
        "$ARG1 is negligible , the cost of using $ARG2",
        "$ARG1 capacity , such as by adding $ARG2"
    ],
    "dropout*****weights": [
        "$ARG1 were not used , though we must pay the cost of dividing the $ARG2"
    ],
    "inference*****dropout": [
        "$ARG1 on Another signi\ufb01cant advantage of $ARG2"
    ],
    "distributed representation*****stochastic gradient descent": [
        "$ARG1 and can be trained with $ARG2"
    ],
    "feedforward neural network*****restricted boltzmann machine": [
        "$ARG1 , probabilistic models such as $ARG2"
    ],
    "restricted boltzmann machine*****recurrent neural network": [
        "$ARG1 ( Srivastava et al. , 2014 ) , and $ARG2"
    ],
    "dropout*****system": [
        "$ARG1 in a complete $ARG2"
    ],
    "technique*****e\ufb00ective capacity": [
        "$ARG1 , it reduces the $ARG2"
    ],
    "set*****dropout": [
        "$ARG1 error is much lower when using $ARG2"
    ],
    "regularization*****reduction": [
        "$ARG1 confers little $ARG2"
    ],
    "reduction*****generalization": [
        "$ARG1 in $ARG2"
    ],
    "neural network*****dropout": [
        "$ARG1 ( Neal , 1996 ) outperform $ARG2",
        "$ARG1 problems , but has not yet yielded a signi\ufb01cant improvement or been applied to a Just as stochasticity is not necessary to achieve the regularizing e\ufb00ect of $ARG2"
    ],
    "dropout*****dataset": [
        "$ARG1 on the Alternative Splicing $ARG2"
    ],
    "feature learning*****dropout": [
        "$ARG1 can gain an advantage over $ARG2"
    ],
    "linear regression*****dropout": [
        "$ARG1 , $ARG2"
    ],
    "dropout*****weight decay": [
        "$ARG1 is equivalent to L2 $ARG2",
        "$ARG1 is not equivalent to $ARG2",
        "$ARG1 or $ARG2"
    ],
    "magnitude*****feature": [
        "$ARG1 of each $ARG2"
    ],
    "feature*****weight decay": [
        "$ARG1 \u2019 s $ARG2"
    ],
    "weight decay*****variance": [
        "$ARG1 coe\ufb03cient is determined by its $ARG2"
    ],
    "dropout*****convergence": [
        "$ARG1 resulted in faster $ARG2"
    ],
    "convergence*****gradient": [
        "$ARG1 time due to the reduced stochasticity in the computation of the $ARG2"
    ],
    "method*****average": [
        "$ARG1 can also be applied at test time , as a more principled ( but also more computationally expensive ) approximation to the $ARG2"
    ],
    "dropout*****standard": [
        "$ARG1 has been used to nearly match the performance of $ARG2"
    ],
    "dropout*****neural network": [
        "$ARG1 on small $ARG2"
    ],
    "method*****dropout": [
        "$ARG1 called $ARG2"
    ],
    "dropout*****set": [
        "$ARG1 boosting trains the entire ensemble to jointly maximize the log-likelihood on the training $ARG2"
    ],
    "interpretation*****dropout": [
        "$ARG1 of $ARG2",
        "$ARG1 of $ARG2"
    ],
    "bagging*****interpretation": [
        "$ARG1 has value beyond the $ARG2"
    ],
    "regularization*****other": [
        "$ARG1 e\ufb00ect of the bagged ensemble is only achieved when the stochastically sampled ensemble members are trained to perform well independently of each $ARG2"
    ],
    "dropconnect*****special case": [
        "$ARG1 is a $ARG2"
    ],
    "special case*****dropout": [
        "$ARG1 of $ARG2"
    ],
    "dropout*****scalar": [
        "$ARG1 where each product between a single $ARG2"
    ],
    "stochastic pooling*****pooling": [
        "$ARG1 is a form of randomized $ARG2"
    ],
    "dropout*****stochastic": [
        "$ARG1 is that training a network with $ARG2"
    ],
    "stochastic*****multiple": [
        "$ARG1 behavior and making predictions by averaging over $ARG2"
    ],
    "stochastic*****bagging": [
        "$ARG1 decisions implements a form of $ARG2"
    ],
    "bagging*****parameter sharing": [
        "$ARG1 with $ARG2"
    ],
    "1*****dropout": [
        "$ARG1 , I ) can outperform $ARG2"
    ],
    "standard*****approximate inference": [
        "$ARG1 network automatically implements $ARG2"
    ],
    "view*****dropout": [
        "$ARG1 of $ARG2"
    ],
    "organism*****model": [
        "$ARG1 or $ARG2"
    ],
    "dropout*****feature": [
        "$ARG1 thus regularizes each hidden unit to be not merely a good $ARG2"
    ],
    "dropout*****generalization": [
        "$ARG1 o\ufb00ers additional improvements to $ARG2"
    ],
    "image*****magnitude": [
        "$ARG1 of a face unless the $ARG2"
    ],
    "magnitude*****information": [
        "$ARG1 of the noise is so great that nearly all of the $ARG2"
    ],
    "algorithm*****batch normalization": [
        "$ARG1 , $ARG2"
    ],
    "batch normalization*****optimization": [
        "$ARG1 is to improve $ARG2",
        "$ARG1 can have a dramatic e\ufb00ect on $ARG2",
        "$ARG1 from the very \ufb01rst baseline , it should be introduced quickly if $ARG2"
    ],
    "optimization*****dropout": [
        "$ARG1 , but the noise can have a regularizing e\ufb00ect , and sometimes makes $ARG2"
    ],
    "neural network*****human": [
        "$ARG1 have begun to reach $ARG2",
        "$ARG1 that perform at $ARG2"
    ],
    "task*****search": [
        "$ARG1 , we can $ARG2"
    ],
    "search*****model": [
        "$ARG1 for examples that the $ARG2",
        "$ARG1 for an input x\ue030 near a data point x such that the $ARG2"
    ],
    "human*****accuracy": [
        "$ARG1 level $ARG2"
    ],
    "rate*****optimization": [
        "$ARG1 on examples that are intentionally constructed by using an $ARG2"
    ],
    "example*****adversarial example": [
        "$ARG1 and the $ARG2"
    ],
    "adversarial example*****example": [
        "$ARG1 have many implications , for $ARG2"
    ],
    "example*****security": [
        "$ARG1 , in computer $ARG2"
    ],
    "regularization*****rate": [
        "$ARG1 because one can reduce the error $ARG2"
    ],
    "set*****adversarial training": [
        "$ARG1 via $ARG2"
    ],
    "adversarial training*****set": [
        "$ARG1 on adversarially perturbed examples from the training $ARG2"
    ],
    "deep learning*****adversarial example": [
        "$ARG1 y = \u201c panda \u201d sign ( \u2207 xJ ( \u03b8 , x , y ) ) \ue00f sign ( \u2207xJ ( \u03b8 , x , y ) ) Figure 7.8 : A demonstration of $ARG2"
    ],
    "cost function*****change": [
        "$ARG1 with respect to the input , we can $ARG2"
    ],
    "change*****classi\ufb01cation": [
        "$ARG1 GoogLeNet \u2019 s $ARG2"
    ],
    "adversarial training*****constant": [
        "$ARG1 discourages this highly sensitive locally linear behavior by encouraging the network to be locally $ARG2"
    ],
    "adversarial training*****function": [
        "$ARG1 helps to illustrate the power of using a large $ARG2"
    ],
    "function*****regularization": [
        "$ARG1 family in combination with aggressive $ARG2"
    ],
    "logistic regression*****adversarial example": [
        "$ARG1 , are not able to resist $ARG2"
    ],
    "neural network*****constant": [
        "$ARG1 are able to represent functions that can range from nearly linear to nearly locally $ARG2"
    ],
    "adversarial example*****semi-supervised learning": [
        "$ARG1 also provide a means of accomplishing $ARG2"
    ],
    "model*****quality": [
        "$ARG1 is high $ARG2"
    ],
    "quality*****probability": [
        "$ARG1 , then y\u0302 has a high $ARG2"
    ],
    "adversarial example*****model": [
        "$ARG1 generated using not the true label but a label provided by a trained $ARG2"
    ],
    "model*****virtual adversarial examples": [
        "$ARG1 are called $ARG2"
    ],
    "assumption*****manifolds": [
        "$ARG1 motivating this approach is that di\ufb00erent classes usually lie on disconnected $ARG2"
    ],
    "manifolds*****manifold": [
        "$ARG1 , and a small perturbation should not be able to jump from one class $ARG2"
    ],
    "tangent distance*****tangent prop": [
        "$ARG1 , $ARG2"
    ],
    "tangent prop*****manifold": [
        "$ARG1 , and $ARG2",
        "$ARG1 requires the user to manually specify functions that compute the tangent directions ( such as specifying that small translations of images remain in the same class $ARG2"
    ],
    "curse of dimensionality*****manifold": [
        "$ARG1 by assuming that the data lies near a low-dimensional $ARG2"
    ],
    "manifold*****manifold hypothesis": [
        "$ARG1 , as described in One of the early attempts to take advantage of the $ARG2"
    ],
    "manifold hypothesis*****tangent distance": [
        "$ARG1 is the $ARG2"
    ],
    "distance*****manifolds": [
        "$ARG1 but one that is derived from knowledge of the $ARG2",
        "$ARG1 between the $ARG2"
    ],
    "invariant*****factors of variation": [
        "$ARG1 to the local $ARG2",
        "$ARG1 to known $ARG2"
    ],
    "factors of variation*****manifold": [
        "$ARG1 that correspond to movement on the $ARG2",
        "$ARG1 correspond to movement along the $ARG2"
    ],
    "manifold*****distance": [
        "$ARG1 , it would make sense to use as nearest-neighbor $ARG2"
    ],
    "1*****distance": [
        "$ARG1 and x2 the $ARG2"
    ],
    "optimization problem*****tangent plane": [
        "$ARG1 , to \ufb01nd the nearest pair of points on M1 and M2 ) , a cheap alternative that makes sense locally is to approximate Mi by its $ARG2"
    ],
    "tangent plane*****measure": [
        "$ARG1 at xi and $ARG2"
    ],
    "distance*****tangent plane": [
        "$ARG1 between the two tangents , or between a $ARG2"
    ],
    "system*****dimension": [
        "$ARG1 ( in the $ARG2"
    ],
    "dimension*****manifolds": [
        "$ARG1 of the $ARG2"
    ],
    "algorithm*****tangent vector": [
        "$ARG1 requires one to specify the $ARG2",
        "$ARG1 , the $ARG2"
    ],
    "algorithm*****invariant": [
        "$ARG1 ( Simard et al. , 1992 ) ( \ufb01gure 7.9 ) trains a neural net classi\ufb01er with an extra penalty to make each output f ( x ) of the neural net locally $ARG2"
    ],
    "invariance*****manifold": [
        "$ARG1 is achieved by requiring \u2207x f ( x ) to be orthogonal to the known $ARG2"
    ],
    "tangent vector*****directional derivative": [
        "$ARG1 v ( i ) at x , or equivalently that the $ARG2"
    ],
    "directional derivative*****regularization": [
        "$ARG1 of f at x in the directions v ( i ) be small by adding a $ARG2"
    ],
    "deep learning*****hyperparameter": [
        "$ARG1 This regularizer can of course be scaled by an appropriate $ARG2"
    ],
    "hyperparameter*****neural network": [
        "$ARG1 , and , for most $ARG2",
        "$ARG1 values for $ARG2"
    ],
    "tangent vector*****translation": [
        "$ARG1 are derived a priori , usually from the formal knowledge of the e\ufb00ect of transformations such as $ARG2",
        "$ARG1 go beyond the classical invariants that arise out of the geometry of images ( such as $ARG2"
    ],
    "tangent prop*****supervised learning": [
        "$ARG1 has been used not just for $ARG2"
    ],
    "tangent prop*****dataset augmentation": [
        "$ARG1 is closely related to $ARG2",
        "$ARG1 and $ARG2"
    ],
    "weights*****sigmoid": [
        "$ARG1 , as $ARG2"
    ],
    "dataset augmentation*****recti\ufb01ed linear unit": [
        "$ARG1 works well with $ARG2"
    ],
    "double backprop*****adversarial training": [
        "$ARG1 ( Drucker and LeCun , 1992 ) and $ARG2",
        "$ARG1 regularizes the Jacobian to be small , while $ARG2",
        "$ARG1 and $ARG2"
    ],
    "adversarial training*****model": [
        "$ARG1 \ufb01nds inputs near the original inputs and trains the $ARG2",
        "$ARG1 both require that the $ARG2"
    ],
    "dataset augmentation*****model": [
        "$ARG1 using manually speci\ufb01ed transformations both require that the $ARG2"
    ],
    "invariant*****change": [
        "$ARG1 to certain speci\ufb01ed directions of $ARG2",
        "$ARG1 to all directions of $ARG2"
    ],
    "dataset augmentation*****tangent prop": [
        "$ARG1 is the non-in\ufb01nitesimal version of $ARG2"
    ],
    "tangent prop*****adversarial training": [
        "$ARG1 , $ARG2"
    ],
    "adversarial training*****double backprop": [
        "$ARG1 is the non-in\ufb01nitesimal version of $ARG2"
    ],
    "manifold tangent classi\ufb01er*****tangent vector": [
        "$ARG1 ( Rifai et al. , 2011c ) , eliminates the need to know the $ARG2"
    ],
    "illustration*****tangent prop": [
        "$ARG1 of the main idea of the $ARG2"
    ],
    "algorithm*****manifold tangent classi\ufb01er": [
        "$ARG1 ( Simard et al. , 1992 ) and $ARG2",
        "$ARG1 proposed with the $ARG2"
    ],
    "manifold tangent classi\ufb01er*****function": [
        "$ARG1 ( Rifai et al. , 2011c ) , which both regularize the classi\ufb01er output $ARG2"
    ],
    "manifold*****vector": [
        "$ARG1 ) and a $ARG2"
    ],
    "manifold*****change": [
        "$ARG1 , and not to $ARG2"
    ],
    "tangent prop*****manifold tangent classi\ufb01er": [
        "$ARG1 and the $ARG2"
    ],
    "manifold tangent classi\ufb01er*****change": [
        "$ARG1 regularize f ( x ) to not $ARG2"
    ],
    "manifold*****manifold tangent classi\ufb01er": [
        "$ARG1 ) while the $ARG2"
    ],
    "manifold tangent classi\ufb01er*****manifold": [
        "$ARG1 estimates the $ARG2"
    ],
    "manifold tangent classi\ufb01er*****technique": [
        "$ARG1 makes use of this $ARG2"
    ],
    "technique*****tangent vector": [
        "$ARG1 to avoid needing user-speci\ufb01ed $ARG2"
    ],
    "manifold tangent classi\ufb01er*****1": [
        "$ARG1 is therefore simple : ( $ARG2"
    ],
    "1*****manifold": [
        "$ARG1 ) use an autoencoder to learn the $ARG2"
    ],
    "manifold*****unsupervised learning": [
        "$ARG1 structure by $ARG2"
    ],
    "unsupervised learning*****tangent prop": [
        "$ARG1 , and ( 2 ) use these tangents to regularize a neural net classi\ufb01er as in $ARG2"
    ],
    "probability theory*****information": [
        "$ARG1 and $ARG2",
        "$ARG1 allows us to make uncertain statements and reason in the presence of uncertainty , $ARG2",
        "$ARG1 and $ARG2"
    ],
    "arti\ufb01cial intelligence*****probability theory": [
        "$ARG1 applications , we use $ARG2"
    ],
    "probability*****probability theory": [
        "$ARG1 tell us how AI systems should reason , so we design our algorithms to compute or approximate various expressions derived using $ARG2"
    ],
    "probability*****statistics": [
        "$ARG1 and $ARG2"
    ],
    "information*****structured probabilistic model": [
        "$ARG1 theory , you may wish to skip all of this chapter except for section 3.14 , which describes the graphs we use to describe $ARG2"
    ],
    "structured probabilistic model*****machine learning": [
        "$ARG1 for $ARG2",
        "$ARG1 merely as a language to describe which direct probabilistic relationships di\ufb00erent $ARG2"
    ],
    "information*****computer science": [
        "$ARG1 THEORY Many branches of $ARG2"
    ],
    "machine learning*****stochastic": [
        "$ARG1 must always deal with uncertain quantities , and sometimes may also need to deal with $ARG2"
    ],
    "stochastic*****system": [
        "$ARG1 when we can not observe all of the variables that drive the behavior of the $ARG2"
    ],
    "rule*****system": [
        "$ARG1 is deterministic and our modeling $ARG2"
    ],
    "system*****rule": [
        "$ARG1 has the \ufb01delity to accommodate a complex $ARG2"
    ],
    "reasoning*****probability theory": [
        "$ARG1 about uncertainty , it is not immediately obvious that $ARG2"
    ],
    "probability theory*****arti\ufb01cial intelligence": [
        "$ARG1 can provide all of the tools we want for $ARG2"
    ],
    "probability*****bayesian probability": [
        "$ARG1 , while the latter , related to qualitative levels of certainty , is known as $ARG2"
    ],
    "list*****reasoning": [
        "$ARG1 several properties that we expect common sense $ARG2"
    ],
    "set*****assumption": [
        "$ARG1 of formal rules for determining what propositions are implied to be true or false given the $ARG2"
    ],
    "probability theory*****set": [
        "$ARG1 provides a $ARG2"
    ],
    "discrete random variable*****number": [
        "$ARG1 is one that has a \ufb01nite or countably in\ufb01nite $ARG2"
    ],
    "probability distribution*****random variable": [
        "$ARG1 is a description of how likely a $ARG2",
        "$ARG1 P ( x ) and Q ( x ) over the same $ARG2",
        "$ARG1 is able to represent direct interactions between those two $ARG2"
    ],
    "probability distribution*****probability mass function": [
        "$ARG1 depends on whether the variables are discrete or Discrete Variables and $ARG2",
        "$ARG1 over discrete variables may be described using a $ARG2"
    ],
    "probability mass function*****probability distribution": [
        "$ARG1 A $ARG2"
    ],
    "random variable*****probability": [
        "$ARG1 with a di\ufb00erent $ARG2",
        "$ARG1 to the $ARG2"
    ],
    "function*****probability mass function": [
        "$ARG1 and the reader must infer which $ARG2"
    ],
    "probability mass function*****identity": [
        "$ARG1 to use based on the $ARG2"
    ],
    "identity*****random variable": [
        "$ARG1 of the $ARG2"
    ],
    "probability mass function*****random variable": [
        "$ARG1 maps from a state of a $ARG2",
        "$ARG1 on a $ARG2"
    ],
    "probability distribution*****joint probability distribution": [
        "$ARG1 over many variables is known as a $ARG2"
    ],
    "property*****probability": [
        "$ARG1 , we could obtain probabilities greater than one by computing the $ARG2"
    ],
    "example*****discrete random variable": [
        "$ARG1 , consider a single $ARG2",
        "$ARG1 , suppose we have $ARG2"
    ],
    "uniform distribution*****probability mass function": [
        "$ARG1 on x\u2014that is , make each of its states equally likely\u2014by setting its $ARG2"
    ],
    "information*****probability density function": [
        "$ARG1 THEORY Continuous Variables and $ARG2"
    ],
    "probability distribution*****probability density function": [
        "$ARG1 using a $ARG2"
    ],
    "probability density function*****function": [
        "$ARG1 , a $ARG2"
    ],
    "probability density function*****probability": [
        "$ARG1 p ( x ) does not give the $ARG2",
        "$ARG1 corresponding to a speci\ufb01c $ARG2"
    ],
    "function*****probability": [
        "$ARG1 to \ufb01nd the actual $ARG2"
    ],
    "set*****integral": [
        "$ARG1 S is given by the $ARG2",
        "$ARG1 S is given by the $ARG2"
    ],
    "integral*****set": [
        "$ARG1 of p ( x ) over that $ARG2",
        "$ARG1 of p ( x ) over the $ARG2"
    ],
    "random variable*****uniform distribution": [
        "$ARG1 , consider a $ARG2"
    ],
    "subset*****marginal probability": [
        "$ARG1 is known as the $ARG2"
    ],
    "information*****marginal probability": [
        "$ARG1 THEORY The name \u201c $ARG2"
    ],
    "marginal probability*****process": [
        "$ARG1 \u201d comes from the $ARG2"
    ],
    "conditional probability*****germany": [
        "$ARG1 that a person is from $ARG2"
    ],
    "germany*****change": [
        "$ARG1 given that they speak German is quite high , but if a randomly selected person is taught to speak German , their country of origin does not $ARG2"
    ],
    "rule*****joint probability distribution": [
        "$ARG1 of Conditional Probabilities Any $ARG2"
    ],
    "joint probability distribution*****random variable": [
        "$ARG1 over many $ARG2"
    ],
    "rule*****product rule of probability": [
        "$ARG1 or $ARG2"
    ],
    "independence*****conditional independence": [
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2"
    ],
    "conditional independence*****random variable": [
        "$ARG1 Two $ARG2"
    ],
    "random variable*****conditional probability distribution": [
        "$ARG1 z if the $ARG2"
    ],
    "expectation*****variance": [
        "$ARG1 , $ARG2",
        "$ARG1 or $ARG2"
    ],
    "variance*****covariance": [
        "$ARG1 and $ARG2"
    ],
    "covariance*****expectation": [
        "$ARG1 The $ARG2"
    ],
    "expectation*****expected value": [
        "$ARG1 or $ARG2"
    ],
    "expected value*****function": [
        "$ARG1 of some $ARG2"
    ],
    "probability distribution*****average": [
        "$ARG1 P ( x ) is the $ARG2"
    ],
    "information*****identity": [
        "$ARG1 THEORY When the $ARG2"
    ],
    "identity*****distribution": [
        "$ARG1 of the $ARG2"
    ],
    "random variable*****expectation": [
        "$ARG1 that the $ARG2",
        "$ARG1 the $ARG2"
    ],
    "variance*****measure": [
        "$ARG1 gives a $ARG2"
    ],
    "covariance*****other": [
        "$ARG1 gives some sense of how much two values are linearly related to each $ARG2"
    ],
    "absolute value*****covariance": [
        "$ARG1 of the $ARG2"
    ],
    "covariance*****change": [
        "$ARG1 mean that the values $ARG2"
    ],
    "covariance*****variable": [
        "$ARG1 is negative , then one $ARG2"
    ],
    "other*****correlation": [
        "$ARG1 measures such as $ARG2"
    ],
    "correlation*****variable": [
        "$ARG1 normalize the contribution of each $ARG2"
    ],
    "covariance*****dependence": [
        "$ARG1 and $ARG2",
        "$ARG1 , there must be no linear $ARG2"
    ],
    "independence*****property": [
        "$ARG1 is a distinct $ARG2"
    ],
    "property*****covariance": [
        "$ARG1 from $ARG2"
    ],
    "independence*****covariance": [
        "$ARG1 is a stronger requirement than zero $ARG2"
    ],
    "covariance*****independence": [
        "$ARG1 , because $ARG2"
    ],
    "number*****uniform distribution": [
        "$ARG1 x from a $ARG2"
    ],
    "uniform distribution*****1": [
        "$ARG1 over the interval [ \u22121 , $ARG2"
    ],
    "information*****probability": [
        "$ARG1 THEORY s. With $ARG2",
        "$ARG1 gained by observing an event of $ARG2"
    ],
    "covariance matrix*****vector": [
        "$ARG1 of a random $ARG2"
    ],
    "diagonal*****covariance": [
        "$ARG1 elements of the $ARG2"
    ],
    "probability distribution*****bernoulli distribution": [
        "$ARG1 are useful in many contexts in machine The $ARG2"
    ],
    "bernoulli distribution*****distribution": [
        "$ARG1 is a $ARG2"
    ],
    "1*****categorical distribution": [
        "$ARG1 \u2212 \u03c6 ) The multinoulli or $ARG2"
    ],
    "categorical distribution*****distribution": [
        "$ARG1 is a $ARG2"
    ],
    "1*****multinoulli distribution": [
        "$ARG1 The $ARG2"
    ],
    "multinoulli distribution*****special case": [
        "$ARG1 is a $ARG2"
    ],
    "special case*****multinomial distribution": [
        "$ARG1 of the $ARG2"
    ],
    "multinomial distribution*****distribution": [
        "$ARG1 is the $ARG2"
    ],
    "multinoulli distribution*****1": [
        "$ARG1 without clarifying that they refer only to the n = $ARG2",
        "$ARG1 are often used to refer to distributions over categories of objects , so we do not usually assume that state $ARG2"
    ],
    "multinoulli distribution*****distribution": [
        "$ARG1 are su\ufb03cient to describe any $ARG2"
    ],
    "distribution*****normal distribution": [
        "$ARG1 over real numbers is the $ARG2",
        "$ARG1 over the real numbers should take , the $ARG2"
    ],
    "normal distribution*****gaussian distribution": [
        "$ARG1 , also known as the $ARG2"
    ],
    "gaussian distribution*****\u03c3": [
        "$ARG1 : N ( x ; \u00b5 , $ARG2"
    ],
    "standard deviation*****distribution": [
        "$ARG1 of the $ARG2"
    ],
    "variance*****\u03c3": [
        "$ARG1 by $ARG2"
    ],
    "distribution*****precision": [
        "$ARG1 is to use a parameter \u03b2 \u2208 ( 0 , \u221e ) to control the $ARG2"
    ],
    "information*****normal distribution": [
        "$ARG1 THEORY p ( x ) Maximum at x = \u00b5 Figure 3.1 : The $ARG2"
    ],
    "model*****normal distribution": [
        "$ARG1 are truly close to being $ARG2"
    ],
    "central limit theorem*****random variable": [
        "$ARG1 shows that the sum of many independent $ARG2"
    ],
    "system*****probability distribution": [
        "$ARG1 can be decomposed into parts with more Second , out of all possible $ARG2"
    ],
    "probability distribution*****variance": [
        "$ARG1 with the same $ARG2"
    ],
    "variance*****normal distribution": [
        "$ARG1 , the $ARG2"
    ],
    "normal distribution*****model": [
        "$ARG1 as being the one that inserts the least amount of prior knowledge into a $ARG2"
    ],
    "normal distribution*****multivariate normal distribution": [
        "$ARG1 generalizes to R n , in which case it is known as the $ARG2"
    ],
    "covariance matrix*****diagonal matrix": [
        "$ARG1 to be a $ARG2"
    ],
    "covariance matrix*****scalar": [
        "$ARG1 is a $ARG2"
    ],
    "laplace distribution*****deep learning": [
        "$ARG1 In the context of $ARG2"
    ],
    "exponential distribution*****function": [
        "$ARG1 uses the indicator $ARG2"
    ],
    "probability*****laplace distribution": [
        "$ARG1 mass at an arbitrary point \u00b5 is the $ARG2"
    ],
    "laplace distribution*****distribution": [
        "$ARG1 Laplace ( x ; \u00b5 , \u03b3 ) = The Dirac $ARG2"
    ],
    "empirical distribution*****probability distribution": [
        "$ARG1 In some cases , we wish to specify that all of the mass in a $ARG2"
    ],
    "dirac delta function*****1": [
        "$ARG1 is de\ufb01ned such that it is zero-valued everywhere except 0 , yet integrates to $ARG2"
    ],
    "dirac delta function*****function": [
        "$ARG1 is not an ordinary $ARG2"
    ],
    "information*****generalized function": [
        "$ARG1 THEORY mathematical object called a $ARG2"
    ],
    "dirac delta function*****series": [
        "$ARG1 as being the limit point of a $ARG2"
    ],
    "series*****other": [
        "$ARG1 of functions that put less and less mass on all points $ARG2"
    ],
    "empirical distribution*****multinoulli distribution": [
        "$ARG1 can be conceptualized as a $ARG2"
    ],
    "multinoulli distribution*****probability": [
        "$ARG1 , with a $ARG2"
    ],
    "empirical distribution*****dataset": [
        "$ARG1 formed from a $ARG2"
    ],
    "empirical distribution*****probability": [
        "$ARG1 is that it is the $ARG2"
    ],
    "distribution*****identity": [
        "$ARG1 generates the sample is determined by sampling a component $ARG2"
    ],
    "identity*****multinoulli distribution": [
        "$ARG1 from a P ( c = i ) P ( x | c = i ) P ( x ) = where P ( c ) is the $ARG2"
    ],
    "example*****mixture distribution": [
        "$ARG1 of a $ARG2"
    ],
    "mixture distribution*****empirical distribution": [
        "$ARG1 : the $ARG2"
    ],
    "empirical distribution*****mixture distribution": [
        "$ARG1 over real-valued variables is a $ARG2"
    ],
    "mixture distribution*****example": [
        "$ARG1 with one Dirac component for each training $ARG2"
    ],
    "information*****mixture model": [
        "$ARG1 THEORY The $ARG2"
    ],
    "mixture model*****probability distribution": [
        "$ARG1 is one simple strategy for combining $ARG2"
    ],
    "mixture model*****concept": [
        "$ARG1 allows us to brie\ufb02y glimpse a $ARG2"
    ],
    "concept*****variable": [
        "$ARG1 that will be of paramount importance later\u2014the latent $ARG2"
    ],
    "variable*****random variable": [
        "$ARG1 is a $ARG2"
    ],
    "variable*****mixture model": [
        "$ARG1 c of the $ARG2"
    ],
    "mixture model*****example": [
        "$ARG1 provides an $ARG2"
    ],
    "mixture model*****gaussian mixture model": [
        "$ARG1 is the $ARG2"
    ],
    "example*****constraint": [
        "$ARG1 , the covariances could be shared across components via the $ARG2"
    ],
    "covariance matrix*****diagonal": [
        "$ARG1 for each component to be $ARG2"
    ],
    "diagonal*****isotropic": [
        "$ARG1 or $ARG2"
    ],
    "gaussian mixture*****prior probability": [
        "$ARG1 specify the $ARG2"
    ],
    "gaussian mixture model*****universal approximator": [
        "$ARG1 is a $ARG2"
    ],
    "universal approximator*****gaussian mixture model": [
        "$ARG1 of densities , in the sense that any smooth density can be approximated with any speci\ufb01c , non-zero amount of error by a $ARG2"
    ],
    "probability distribution*****deep learning": [
        "$ARG1 used in $ARG2"
    ],
    "logistic sigmoid*****\u03c3": [
        "$ARG1 : $ARG2"
    ],
    "1*****logistic sigmoid": [
        "$ARG1 + exp ( \u2212x ) The $ARG2"
    ],
    "information*****gaussian mixture model": [
        "$ARG1 THEORY Figure 3.2 : Samples from a $ARG2"
    ],
    "covariance matrix*****variance": [
        "$ARG1 , meaning it has the same amount of $ARG2",
        "$ARG1 , meaning it can control the $ARG2",
        "$ARG1 , allowing it to control the $ARG2"
    ],
    "graph*****sigmoid function": [
        "$ARG1 of the $ARG2"
    ],
    "sigmoid function*****function": [
        "$ARG1 saturates when its argument is very positive or very negative , meaning that the $ARG2"
    ],
    "function*****softplus": [
        "$ARG1 is the $ARG2"
    ],
    "graph*****softplus": [
        "$ARG1 of the $ARG2"
    ],
    "information*****\u03c3": [
        "$ARG1 THEORY $ARG2",
        "$ARG1 THEORY $ARG2"
    ],
    "\u03c3*****function": [
        "$ARG1 ( y ) dy \u03b6 ( x ) \u2212 \u03b6 ( \u2212x ) = x The $ARG2"
    ],
    "\u03c3*****statistics": [
        "$ARG1 \u22121 ( x ) is called the logit in $ARG2"
    ],
    "information*****rule": [
        "$ARG1 THEORY Bayes \u2019 $ARG2"
    ],
    "rule*****conditional probability": [
        "$ARG1 is straightforward to derive from the de\ufb01nition of $ARG2"
    ],
    "thomas bayes*****special case": [
        "$ARG1 , who \ufb01rst discovered a $ARG2"
    ],
    "random variable*****probability density function": [
        "$ARG1 and $ARG2"
    ],
    "probability density function*****probability theory": [
        "$ARG1 requires developing $ARG2"
    ],
    "probability theory*****mathematics": [
        "$ARG1 in terms of a branch of $ARG2"
    ],
    "mathematics*****measure theory": [
        "$ARG1 known as $ARG2"
    ],
    "precision*****example": [
        "$ARG1 of real numbers , for $ARG2",
        "$ARG1 equal to the percentage of people who have the disease ( 0.0001 % in our $ARG2"
    ],
    "measure theory*****set": [
        "$ARG1 is to provide a characterization of the $ARG2",
        "$ARG1 provides a rigorous way of describing that a $ARG2"
    ],
    "set*****measure zero": [
        "$ARG1 is said to have $ARG2",
        "$ARG1 of $ARG2",
        "$ARG1 of all the rational numbers has $ARG2"
    ],
    "example*****measure zero": [
        "$ARG1 , within R 2 , a line has $ARG2"
    ],
    "measure zero*****measure": [
        "$ARG1 , while a \ufb01lled polygon has positive $ARG2"
    ],
    "measure zero*****set": [
        "$ARG1 ( so the $ARG2"
    ],
    "measure theory*****almost everywhere": [
        "$ARG1 is $ARG2"
    ],
    "property*****almost everywhere": [
        "$ARG1 that holds $ARG2"
    ],
    "almost everywhere*****set": [
        "$ARG1 holds throughout all of space except for on a $ARG2"
    ],
    "set*****theorem": [
        "$ARG1 of The Banach-Tarski $ARG2"
    ],
    "theorem*****example": [
        "$ARG1 provides a fun $ARG2"
    ],
    "information*****measure zero": [
        "$ARG1 THEORY $ARG2"
    ],
    "probability theory*****almost everywhere": [
        "$ARG1 hold for all discrete values but only hold \u201c $ARG2"
    ],
    "almost everywhere*****random variable": [
        "$ARG1 \u201d for continuous Another technical detail of continuous variables relates to handling continuous $ARG2"
    ],
    "function*****recall": [
        "$ARG1 g. $ARG2"
    ],
    "recall*****probability": [
        "$ARG1 that the $ARG2"
    ],
    "problem*****scalar": [
        "$ARG1 , we return to the $ARG2"
    ],
    "derivative*****determinant": [
        "$ARG1 generalizes to the $ARG2"
    ],
    "determinant*****jacobian matrix": [
        "$ARG1 of the $ARG2"
    ],
    "information*****applied mathematics": [
        "$ARG1 theory is a branch of $ARG2"
    ],
    "applied mathematics*****information": [
        "$ARG1 that revolves around quantifying how much $ARG2"
    ],
    "information*****expected": [
        "$ARG1 theory tells how to design optimal codes and calculate the $ARG2"
    ],
    "expected*****probability distribution": [
        "$ARG1 length of messages sampled from speci\ufb01c $ARG2"
    ],
    "probability distribution*****similarity": [
        "$ARG1 or quantify $ARG2"
    ],
    "similarity*****probability distribution": [
        "$ARG1 between $ARG2"
    ],
    "content*****information": [
        "$ARG1 , and in the extreme case , events that are guaranteed to happen should have no $ARG2",
        "$ARG1 \u2022 Less likely events should have higher $ARG2"
    ],
    "information*****self-information": [
        "$ARG1 as \ufb01nding out that a tossed coin has come up as heads In order to satisfy all three of these properties , we de\ufb01ne the $ARG2"
    ],
    "probability distribution*****shannon entropy": [
        "$ARG1 using the $ARG2"
    ],
    "other*****shannon entropy": [
        "$ARG1 words , the $ARG2"
    ],
    "shannon entropy*****distribution": [
        "$ARG1 of a $ARG2"
    ],
    "distribution*****expected": [
        "$ARG1 is the $ARG2"
    ],
    "expected*****information": [
        "$ARG1 amount of $ARG2"
    ],
    "number*****base": [
        "$ARG1 of bits ( if the logarithm is $ARG2"
    ],
    "base*****average": [
        "$ARG1 2 , otherwise the units are di\ufb00erent ) needed on $ARG2"
    ],
    "shannon entropy*****di\ufb00erential entropy": [
        "$ARG1 is known as the $ARG2"
    ],
    "random variable*****measure": [
        "$ARG1 x , we can $ARG2"
    ],
    "measure*****divergence": [
        "$ARG1 how di\ufb00erent these two distributions are using the Kullback-Leibler ( KL ) $ARG2"
    ],
    "information*****base": [
        "$ARG1 ( measured in bits if we use the $ARG2"
    ],
    "base*****machine learning": [
        "$ARG1 2 logarithm , but in $ARG2"
    ],
    "divergence*****distribution": [
        "$ARG1 is 0 if and only if P and Q are the same $ARG2"
    ],
    "distribution*****almost everywhere": [
        "$ARG1 in the case of discrete variables , or equal \u201c $ARG2"
    ],
    "divergence*****distance": [
        "$ARG1 is non-negative and measures the di\ufb00erence between two distributions , it is often conceptualized as measuring some sort of $ARG2"
    ],
    "information*****shannon entropy": [
        "$ARG1 THEORY $ARG2"
    ],
    "divergence*****cross-entropy": [
        "$ARG1 is the $ARG2"
    ],
    "cross-entropy*****divergence": [
        "$ARG1 H ( P , Q ) = H ( P ) + DKL ( P \ue06bQ ) , which is similar to the KL $ARG2",
        "$ARG1 with respect to Q is equivalent to minimizing the KL $ARG2"
    ],
    "convention*****information": [
        "$ARG1 , in the context of $ARG2"
    ],
    "information*****divergence": [
        "$ARG1 THEORY q \u2217 = argminq DKL ( p\ue06bq ) q\u2217 = argmin qDKL ( q \ue06bp ) p ( x ) q\u2217 ( x ) p ( x ) q \u2217 ( x ) Figure 3.6 : The KL $ARG2"
    ],
    "multiple*****divergence": [
        "$ARG1 modes that are su\ufb03ciently widely separated , as in this \ufb01gure , the KL $ARG2"
    ],
    "probability*****divergence": [
        "$ARG1 region , then this direction of the KL $ARG2"
    ],
    "information*****joint probability distribution": [
        "$ARG1 THEORY describe the entire $ARG2"
    ],
    "graph*****structured probabilistic model": [
        "$ARG1 , we call it a $ARG2"
    ],
    "conditional probability distribution*****example": [
        "$ARG1 , as in the $ARG2"
    ],
    "contains*****random variable": [
        "$ARG1 one factor for every $ARG2"
    ],
    "information*****directed graphical model": [
        "$ARG1 THEORY Figure 3.7 : A $ARG2"
    ],
    "random variable*****graph": [
        "$ARG1 a , b , c , d and e. This $ARG2",
        "$ARG1 a , b , c , d and e. This $ARG2"
    ],
    "constant*****integral": [
        "$ARG1 Z , de\ufb01ned to be the sum or $ARG2"
    ],
    "integral*****1": [
        "$ARG1 over all states of the product of the \u03c6 functions , in order to obtain a normalized $ARG2"
    ],
    "property*****probability distribution": [
        "$ARG1 of a $ARG2"
    ],
    "probability distribution*****property": [
        "$ARG1 ; it is a $ARG2"
    ],
    "information*****undirected graphical model": [
        "$ARG1 THEORY Figure 3.8 : An $ARG2"
    ],
    "undirected graphical model*****random variable": [
        "$ARG1 over $ARG2"
    ],
    "probability distribution*****structured probabilistic model": [
        "$ARG1 may be described in both Throughout parts I and II of this book , we will use $ARG2"
    ],
    "structured probabilistic model*****probability theory": [
        "$ARG1 in much greater This chapter has reviewed the basic concepts of $ARG2"
    ],
    "probability theory*****deep learning": [
        "$ARG1 that are most relevant to $ARG2"
    ],
    "machine learning*****objective function": [
        "$ARG1 models , training algorithms , and $ARG2"
    ],
    "hyperparameters*****example": [
        "$ARG1 , or changing algorithms , based on speci\ufb01c \ufb01ndings from As a running $ARG2"
    ],
    "convolutional network*****number": [
        "$ARG1 recognizes the address $ARG2"
    ],
    "number*****database": [
        "$ARG1 in each photograph , allowing the Google Maps $ARG2"
    ],
    "bayes error*****rate": [
        "$ARG1 de\ufb01nes the minimum error $ARG2"
    ],
    "rate*****probability distribution": [
        "$ARG1 that you can hope to achieve , even if you have in\ufb01nite training data and can recover the true $ARG2"
    ],
    "information*****variable": [
        "$ARG1 about the output $ARG2"
    ],
    "variable*****system": [
        "$ARG1 , or because the $ARG2"
    ],
    "system*****stochastic": [
        "$ARG1 might be intrinsically $ARG2"
    ],
    "collection*****human": [
        "$ARG1 can require time , money , or $ARG2"
    ],
    "set*****rate": [
        "$ARG1 and you are not allowed to collect How can one determine a reasonable level of performance to expect ? Typically , in the academic setting , we have some estimate of the error $ARG2",
        "$ARG1 is higher than your target error $ARG2",
        "$ARG1 is higher than than your target error $ARG2"
    ],
    "rate*****system": [
        "$ARG1 , of a $ARG2"
    ],
    "example*****spam detection": [
        "$ARG1 , an e-mail $ARG2"
    ],
    "rate*****measure": [
        "$ARG1 of a spam classi\ufb01er , we may wish to $ARG2"
    ],
    "accuracy*****task": [
        "$ARG1 on the detection $ARG2"
    ],
    "problem*****measure": [
        "$ARG1 is to instead $ARG2"
    ],
    "model*****recall": [
        "$ARG1 that were correct , while $ARG2"
    ],
    "precision*****recall": [
        "$ARG1 , but zero $ARG2",
        "$ARG1 on the y-axis and $ARG2",
        "$ARG1 for $ARG2",
        "$ARG1 p and $ARG2"
    ],
    "recall*****precision": [
        "$ARG1 , but $ARG2"
    ],
    "precision and recall*****precision": [
        "$ARG1 , it is common to plot a PR curve , with $ARG2"
    ],
    "recall*****option": [
        "$ARG1 r into an Another $ARG2"
    ],
    "task*****number": [
        "$ARG1 is to transcribe the address $ARG2"
    ],
    "human*****transcription": [
        "$ARG1 being to obtain the correct $ARG2"
    ],
    "transcription*****human": [
        "$ARG1 , then the best course of action is to allow a $ARG2",
        "$ARG1 project was a great success , and allowed hundreds of millions of addresses to be transcribed both faster and at lower cost than would have been possible via $ARG2"
    ],
    "human*****process": [
        "$ARG1 operators must $ARG2"
    ],
    "coverage*****machine learning": [
        "$ARG1 is the fraction of examples for which the $ARG2"
    ],
    "coverage*****accuracy": [
        "$ARG1 for $ARG2",
        "$ARG1 thus became the main performance metric optimized during the project , with $ARG2"
    ],
    "accuracy*****process": [
        "$ARG1 by refusing to $ARG2"
    ],
    "example*****coverage": [
        "$ARG1 , but this reduces the $ARG2"
    ],
    "task*****transcription": [
        "$ARG1 , the goal for the project was to reach human-level $ARG2"
    ],
    "accuracy*****coverage": [
        "$ARG1 while maintaining 95 % $ARG2"
    ],
    "task*****accuracy": [
        "$ARG1 is 98 % $ARG2"
    ],
    "example*****measure": [
        "$ARG1 , $ARG2"
    ],
    "problem*****weights": [
        "$ARG1 has a chance of being solved by just choosing a few linear $ARG2"
    ],
    "machine translation*****deep learning": [
        "$ARG1 , and so on , then you are likely to do well by beginning with an appropriate $ARG2"
    ],
    "image*****convolutional network": [
        "$ARG1 ) , use a $ARG2"
    ],
    "leaky relu*****maxout": [
        "$ARG1 , PreLus and $ARG2"
    ],
    "optimization algorithm*****learning rate": [
        "$ARG1 is SGD with momentum with a decaying $ARG2"
    ],
    "optimization*****convolutional network": [
        "$ARG1 performance , especially for $ARG2"
    ],
    "contains*****regularization": [
        "$ARG1 tens of millions of examples or more , you should include some mild forms of $ARG2"
    ],
    "batch normalization*****generalization": [
        "$ARG1 also sometimes reduces $ARG2"
    ],
    "generalization*****dropout": [
        "$ARG1 error and allows $ARG2"
    ],
    "dropout*****statistics": [
        "$ARG1 to be omitted , due to the noise in the estimate of the $ARG2"
    ],
    "natural language processing*****unsupervised learning": [
        "$ARG1 , are known to bene\ufb01t tremendously from $ARG2"
    ],
    "learning*****word embedding": [
        "$ARG1 unsupervised $ARG2"
    ],
    "other*****computer vision": [
        "$ARG1 domains , such as $ARG2"
    ],
    "computer vision*****unsupervised learning": [
        "$ARG1 , current $ARG2"
    ],
    "unsupervised learning*****number": [
        "$ARG1 techniques do not bring a bene\ufb01t , except in the semi-supervised setting , when the $ARG2"
    ],
    "unsupervised learning*****system": [
        "$ARG1 later if you observe that your initial Determining Whether to Gather More Data After the \ufb01rst end-to-end $ARG2"
    ],
    "measure*****algorithm": [
        "$ARG1 the performance of the $ARG2"
    ],
    "example*****learning rate": [
        "$ARG1 by tuning the $ARG2"
    ],
    "optimization algorithm*****problem": [
        "$ARG1 do not work well , then the $ARG2"
    ],
    "problem*****quality": [
        "$ARG1 might be the $ARG2"
    ],
    "other*****expected": [
        "$ARG1 means , and the amount of data that is $ARG2"
    ],
    "example*****object recognition": [
        "$ARG1 , the development of large labeled datasets was one of the most important factors in solving $ARG2"
    ],
    "regularization*****dropout": [
        "$ARG1 strategies such as $ARG2"
    ],
    "number*****generalization": [
        "$ARG1 of examples will not have a noticeable impact on $ARG2"
    ],
    "hyperparameters*****algorithm": [
        "$ARG1 that control many aspects of the $ARG2"
    ],
    "hyperparameters*****memory": [
        "$ARG1 a\ufb00ect the time and $ARG2"
    ],
    "hyperparameters*****quality": [
        "$ARG1 a\ufb00ect the $ARG2"
    ],
    "hyperparameters*****generalization": [
        "$ARG1 , training error , $ARG2"
    ],
    "generalization*****memory": [
        "$ARG1 error and computational resources ( $ARG2",
        "$ARG1 error subject to some runtime and $ARG2"
    ],
    "e\ufb00ective capacity*****learning": [
        "$ARG1 of a $ARG2",
        "$ARG1 of the $ARG2"
    ],
    "search*****generalization": [
        "$ARG1 is usually to \ufb01nd the lowest $ARG2"
    ],
    "memory*****hyperparameters": [
        "$ARG1 impact of various $ARG2",
        "$ARG1 cost , or due to $ARG2"
    ],
    "search*****e\ufb00ective capacity": [
        "$ARG1 is to adjust the $ARG2"
    ],
    "function*****hyperparameters": [
        "$ARG1 of one of the $ARG2",
        "$ARG1 , without requiring hand-tuning of $ARG2"
    ],
    "hyperparameter*****generalization": [
        "$ARG1 value corresponds to low capacity , and $ARG2",
        "$ARG1 value corresponds to high capacity , and the $ARG2"
    ],
    "other*****hyperparameter": [
        "$ARG1 extreme , the $ARG2"
    ],
    "hyperparameters*****hyperparameter": [
        "$ARG1 , over\ufb01tting occurs when the value of the $ARG2",
        "$ARG1 , over\ufb01tting occurs when the value of the $ARG2"
    ],
    "weight decay*****e\ufb00ective capacity": [
        "$ARG1 coe\ufb03cient of zero corresponds to the greatest $ARG2"
    ],
    "number*****maxout": [
        "$ARG1 of linear pieces in a $ARG2"
    ],
    "algorithm*****preprocessing": [
        "$ARG1 , such as a $ARG2"
    ],
    "preprocessing*****standard deviation": [
        "$ARG1 step that normalizes the input features by subtracting their mean and dividing by their $ARG2"
    ],
    "other*****hyperparameters": [
        "$ARG1 words , some $ARG2"
    ],
    "learning rate*****hyperparameter": [
        "$ARG1 is perhaps the most important $ARG2"
    ],
    "hyperparameter*****learning rate": [
        "$ARG1 , tune the $ARG2"
    ],
    "other*****e\ufb00ective capacity": [
        "$ARG1 hyperparameters\u2014the $ARG2"
    ],
    "learning rate*****optimization problem": [
        "$ARG1 is correct for the $ARG2"
    ],
    "optimization problem*****learning rate": [
        "$ARG1 , not when the $ARG2"
    ],
    "learning rate*****gradient descent": [
        "$ARG1 is too large , $ARG2"
    ],
    "other*****learning rate": [
        "$ARG1 than the $ARG2"
    ],
    "regularization*****optimization algorithm": [
        "$ARG1 and you are con\ufb01dent that your $ARG2"
    ],
    "regularization*****learning rate": [
        "$ARG1 e\ufb00ects arising out of having a too large or too small $ARG2"
    ],
    "learning rate*****optimization": [
        "$ARG1 , since poor $ARG2"
    ],
    "set*****reasoning": [
        "$ARG1 by $ARG2"
    ],
    "reasoning*****model": [
        "$ARG1 about whether they increase or decrease $ARG2"
    ],
    "operation*****model": [
        "$ARG1 on the $ARG2"
    ],
    "e\ufb00ective capacity*****optimization": [
        "$ARG1 due to $ARG2"
    ],
    "optimization*****kernel": [
        "$ARG1 failure Increasing the $ARG2"
    ],
    "convolution*****weight decay": [
        "$ARG1 keeps the Decreasing the $ARG2"
    ],
    "dimension*****model": [
        "$ARG1 , reducing $ARG2"
    ],
    "hyperparameter optimization*****learning": [
        "$ARG1 Algorithms The ideal $ARG2",
        "$ARG1 algorithms that wrap a $ARG2"
    ],
    "learning*****logistic regression": [
        "$ARG1 algorithms such as $ARG2"
    ],
    "logistic regression*****hyperparameters": [
        "$ARG1 and SVMs stems in part from their ability to perform well with only one or two tuned $ARG2"
    ],
    "number*****hyperparameters": [
        "$ARG1 of tuned $ARG2",
        "$ARG1 of non-in\ufb02uential $ARG2",
        "$ARG1 of $ARG2"
    ],
    "hyperparameters*****optimization": [
        "$ARG1 , we realize that an $ARG2"
    ],
    "optimization*****hyperparameters": [
        "$ARG1 is taking place : we are trying to \ufb01nd a value of the $ARG2"
    ],
    "hyperparameters*****objective function": [
        "$ARG1 that optimizes an $ARG2"
    ],
    "objective function*****memory": [
        "$ARG1 , such as validation error , sometimes under constraints ( such as a budget for training time , $ARG2"
    ],
    "hyperparameter optimization*****hyperparameters": [
        "$ARG1 algorithms often have their own $ARG2"
    ],
    "hyperparameters*****grid search": [
        "$ARG1 , the common practice is to perform $ARG2"
    ],
    "model*****hyperparameter": [
        "$ARG1 for every joint speci\ufb01cation of $ARG2"
    ],
    "hyperparameter*****cartesian product": [
        "$ARG1 values in the $ARG2"
    ],
    "cartesian product*****set": [
        "$ARG1 of the $ARG2"
    ],
    "grid search*****random search": [
        "$ARG1 and $ARG2",
        "$ARG1 and $ARG2",
        "$ARG1 , one may often want to run repeated versions of $ARG2",
        "$ARG1 will unnecessarily repeat two equivalent experiments while $ARG2"
    ],
    "illustration*****hyperparameters": [
        "$ARG1 purposes we display two $ARG2"
    ],
    "grid search*****set": [
        "$ARG1 , we provide a $ARG2"
    ],
    "random search*****probability distribution": [
        "$ARG1 , we provide a $ARG2"
    ],
    "probability distribution*****hyperparameter": [
        "$ARG1 over joint $ARG2"
    ],
    "hyperparameters*****other": [
        "$ARG1 are independent from each $ARG2"
    ],
    "distribution*****hyperparameter": [
        "$ARG1 over a single $ARG2",
        "$ARG1 for each $ARG2"
    ],
    "hyperparameter*****distribution": [
        "$ARG1 include uniform and log-uniform ( to sample from a log-uniform $ARG2"
    ],
    "distribution*****uniform distribution": [
        "$ARG1 , take the exp of a sample from a $ARG2"
    ],
    "random search*****set": [
        "$ARG1 evaluate the validation $ARG2",
        "$ARG1 reduces the validation $ARG2"
    ],
    "illustration*****hyperparameter": [
        "$ARG1 , only the $ARG2",
        "$ARG1 of a grid of $ARG2"
    ],
    "grid search*****number": [
        "$ARG1 wastes an amount of computation that is exponential in the $ARG2",
        "$ARG1 is that its computational cost grows exponentially with the $ARG2",
        "$ARG1 , in terms of the $ARG2"
    ],
    "hyperparameters*****random search": [
        "$ARG1 , while $ARG2",
        "$ARG1 : $ARG2",
        "$ARG1 would have the same values for these two runs , whereas with $ARG2"
    ],
    "random search*****hyperparameter": [
        "$ARG1 tests a unique value of every in\ufb02uential $ARG2"
    ],
    "search*****hyperparameters": [
        "$ARG1 over be chosen ? In the case of numerical ( ordered ) $ARG2",
        "$ARG1 for good $ARG2"
    ],
    "hyperparameters*****element": [
        "$ARG1 , the smallest and largest $ARG2"
    ],
    "element*****list": [
        "$ARG1 of each $ARG2"
    ],
    "grid search*****learning rate": [
        "$ARG1 involves picking values approximately on a logarithmic scale , e.g. , a $ARG2"
    ],
    "example*****grid search": [
        "$ARG1 , suppose that we ran a $ARG2"
    ],
    "grid search*****hyperparameter": [
        "$ARG1 over a $ARG2",
        "$ARG1 , when two values of a $ARG2"
    ],
    "1*****search": [
        "$ARG1 , then we underestimated the range in which the best \u03b1 lies and we should shift the grid and run another $ARG2"
    ],
    "search*****example": [
        "$ARG1 with \u03b1 in , for $ARG2"
    ],
    "problem*****grid search": [
        "$ARG1 with $ARG2"
    ],
    "search*****grid search": [
        "$ARG1 ) Unfortunately , due to the exponential cost of $ARG2"
    ],
    "grid search*****search": [
        "$ARG1 , even parallelization may not provide a satisfactory size of $ARG2"
    ],
    "grid search*****hyperparameters": [
        "$ARG1 that is as simple to program , more convenient to use , and converges much faster to good values of the $ARG2",
        "$ARG1 , one should not discretize or bin the values of the $ARG2",
        "$ARG1 , when there are several $ARG2"
    ],
    "hyperparameter*****hyperparameters": [
        "$ARG1 , e.g. , a Bernoulli or multinoulli for binary or discrete $ARG2"
    ],
    "hyperparameters*****uniform distribution": [
        "$ARG1 , or a $ARG2"
    ],
    "uniform distribution*****hyperparameters": [
        "$ARG1 on a log-scale for positive real-valued $ARG2"
    ],
    "random search*****grid search": [
        "$ARG1 can be exponentially more e\ufb03cient than a $ARG2",
        "$ARG1 \ufb01nds good solutions faster than $ARG2"
    ],
    "hyperparameters*****measure": [
        "$ARG1 that do not strongly a\ufb00ect the performance $ARG2"
    ],
    "set*****grid search": [
        "$ARG1 error much faster than $ARG2",
        "$ARG1 error , $ARG2"
    ],
    "random search*****search": [
        "$ARG1 , to re\ufb01ne the $ARG2"
    ],
    "hyperparameter*****other": [
        "$ARG1 ( given values of the $ARG2"
    ],
    "grid search*****other": [
        "$ARG1 , the $ARG2"
    ],
    "random search*****other": [
        "$ARG1 will still give two independent explorations of the $ARG2"
    ],
    "hyperparameters*****optimization problem": [
        "$ARG1 can be cast as an $ARG2"
    ],
    "gradient*****measure": [
        "$ARG1 of some di\ufb00erentiable error $ARG2"
    ],
    "hyperparameters*****gradient": [
        "$ARG1 , we can simply follow this $ARG2"
    ],
    "hyperparameter*****optimization": [
        "$ARG1 guesses by performing $ARG2"
    ],
    "search*****regression model": [
        "$ARG1 use a Bayesian $ARG2"
    ],
    "regression model*****expected value": [
        "$ARG1 to estimate both the $ARG2"
    ],
    "expected value*****set": [
        "$ARG1 of the validation $ARG2"
    ],
    "hyperparameter*****expectation": [
        "$ARG1 and the uncertainty around this $ARG2"
    ],
    "optimization*****exploration": [
        "$ARG1 thus involves a tradeo\ufb00 between $ARG2"
    ],
    "exploration*****hyperparameters": [
        "$ARG1 ( proposing $ARG2"
    ],
    "exploitation*****hyperparameters": [
        "$ARG1 ( proposing $ARG2"
    ],
    "hyperparameter optimization*****spearmint": [
        "$ARG1 include $ARG2"
    ],
    "bayesian hyperparameter optimization*****deep learning": [
        "$ARG1 as an established tool for achieving better $ARG2"
    ],
    "bayesian hyperparameter optimization*****human": [
        "$ARG1 sometimes performs comparably to $ARG2"
    ],
    "human*****other": [
        "$ARG1 experts , sometimes better , but fails catastrophically on $ARG2"
    ],
    "hyperparameter optimization*****deep learning": [
        "$ARG1 is an important \ufb01eld of research that , while often driven primarily by the needs of $ARG2"
    ],
    "deep learning*****potential": [
        "$ARG1 , holds the $ARG2"
    ],
    "potential*****machine learning": [
        "$ARG1 to bene\ufb01t not only the entire \ufb01eld of $ARG2"
    ],
    "machine learning*****hyperparameter optimization": [
        "$ARG1 but the discipline of engineering in One drawback common to most $ARG2"
    ],
    "hyperparameter optimization*****random search": [
        "$ARG1 algorithms with more sophistication than $ARG2"
    ],
    "random search*****information": [
        "$ARG1 is that they require for a training experiment to run to completion before they are able to extract any $ARG2"
    ],
    "information*****search": [
        "$ARG1 can be gleaned early in an experiment , than manual $ARG2"
    ],
    "set*****multiple": [
        "$ARG1 of $ARG2"
    ],
    "system*****algorithm": [
        "$ARG1 performs poorly , it is usually di\ufb03cult to tell whether the poor performance is intrinsic to the $ARG2"
    ],
    "neural network*****classi\ufb01cation": [
        "$ARG1 on a new $ARG2"
    ],
    "expected*****machine learning": [
        "$ARG1 behavior or sub-optimal A further di\ufb03culty is that most $ARG2"
    ],
    "machine learning*****multiple": [
        "$ARG1 models have $ARG2"
    ],
    "rule*****learning rate": [
        "$ARG1 for each parameter separately , and we made an error in the update for the biases : where \u03b1 is the $ARG2"
    ],
    "model*****view": [
        "$ARG1 to detect objects in images , $ARG2"
    ],
    "model*****speech": [
        "$ARG1 of $ARG2"
    ],
    "output layer*****probability": [
        "$ARG1 assign a $ARG2"
    ],
    "system*****problem": [
        "$ARG1 originally had a $ARG2"
    ],
    "number*****system": [
        "$ARG1 detection $ARG2",
        "$ARG1 detection $ARG2",
        "$ARG1 detection $ARG2"
    ],
    "transcription*****probability": [
        "$ARG1 network then assigned very low $ARG2"
    ],
    "system*****transcription": [
        "$ARG1 , even though the $ARG2"
    ],
    "transcription*****process": [
        "$ARG1 network needed to be able to $ARG2"
    ],
    "process*****reasoning": [
        "$ARG1 greater variation in the position and scale of the address $ARG2"
    ],
    "example*****output layer": [
        "$ARG1 can be \ufb01t just by setting the biases of the $ARG2"
    ],
    "optimization*****set": [
        "$ARG1 on the training $ARG2"
    ],
    "derivative*****accuracy": [
        "$ARG1 by using a small , \ufb01nite \ue00f : f \ue030 ( x ) \u2248 f ( x + \ue00f ) \u2212 f ( x ) We can improve the $ARG2"
    ],
    "gradient*****statistics": [
        "$ARG1 : It is often useful to visualize $ARG2"
    ],
    "statistics*****neural network": [
        "$ARG1 of $ARG2"
    ],
    "neural network*****epoch": [
        "$ARG1 activations and gradients , collected over a large amount of training iterations ( maybe one $ARG2"
    ],
    "example*****average": [
        "$ARG1 , for recti\ufb01ers , how often are they o\ufb00 ? Are there units that are always o\ufb00 ? For tanh units , the $ARG2"
    ],
    "average*****absolute value": [
        "$ARG1 of the $ARG2"
    ],
    "magnitude*****minibatch": [
        "$ARG1 of parameter updates over a $ARG2"
    ],
    "gradient*****subset": [
        "$ARG1 with respect to some $ARG2"
    ],
    "subset*****algorithm": [
        "$ARG1 of variables will be zero after each step of the $ARG2"
    ],
    "number*****street": [
        "$ARG1 Recognition To provide an end-to-end description of how to apply our design methodology in practice , we present a brief account of the $ARG2"
    ],
    "system*****street": [
        "$ARG1 , such as the $ARG2"
    ],
    "view*****database": [
        "$ARG1 cars , the $ARG2"
    ],
    "process*****collection": [
        "$ARG1 began with data $ARG2"
    ],
    "accuracy*****street": [
        "$ARG1 , the $ARG2"
    ],
    "system*****coverage": [
        "$ARG1 sacri\ufb01ces $ARG2",
        "$ARG1 was motivated by a theoretical understanding of the $ARG2",
        "$ARG1 \u2019 s $ARG2"
    ],
    "convolutional network*****coverage": [
        "$ARG1 improved , it became possible to reduce the con\ufb01dence threshold below which the network refuses to transcribe the input , eventually exceeding the goal of 95 % $ARG2"
    ],
    "convolutional network*****recti\ufb01ed linear unit": [
        "$ARG1 with $ARG2"
    ],
    "transcription*****model": [
        "$ARG1 project began with such a $ARG2"
    ],
    "output layer*****model": [
        "$ARG1 of the $ARG2"
    ],
    "softmax*****task": [
        "$ARG1 units were trained exactly the same as if the $ARG2"
    ],
    "classi\ufb01cation*****softmax": [
        "$ARG1 , with each $ARG2"
    ],
    "change*****street": [
        "$ARG1 to the $ARG2"
    ],
    "output layer*****cost function": [
        "$ARG1 and $ARG2"
    ],
    "set*****problem": [
        "$ARG1 performance in order to determine whether the $ARG2",
        "$ARG1 error were so similar , this suggested that the $ARG2"
    ],
    "problem*****accuracy": [
        "$ARG1 could have been resolved by spending weeks improving the $ARG2"
    ],
    "change*****transcription": [
        "$ARG1 added ten percentage points to the $ARG2"
    ]
}